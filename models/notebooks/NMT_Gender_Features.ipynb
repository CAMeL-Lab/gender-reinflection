{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.calima_star.database import CalimaStarDB\n",
    "from camel_tools.calima_star.analyzer import CalimaStarAnalyzer\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample:\n",
    "    \"\"\"Simple object to encapsulate each data example\"\"\"\n",
    "    def __init__(self, src, trg, \n",
    "                 src_g, trg_g):    \n",
    "        self.src = src\n",
    "        self.trg = trg\n",
    "        self.src_g = src_g\n",
    "        self.trg_g = trg_g\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_str())\n",
    "    \n",
    "    def to_json_str(self):\n",
    "        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)\n",
    "    \n",
    "    def to_dict(self):\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataset:\n",
    "    \"\"\"Encapsulates the raw examples in InputExample objects\"\"\"\n",
    "    def __init__(self, data_dir):\n",
    "        self.train_examples = self.get_train_examples(data_dir)\n",
    "        self.dev_examples = self.get_dev_examples(data_dir)\n",
    "        self.test_examples = self.get_dev_examples(data_dir)\n",
    "        \n",
    "    def create_examples(self, src_path, trg_path):\n",
    "        \n",
    "        src_txt = self.get_txt_examples(src_path)\n",
    "        src_gender_labels = self.get_labels(src_path + '.label')\n",
    "        trg_txt = self.get_txt_examples(trg_path)\n",
    "        trg_gender_labels = self.get_labels(trg_path + '.label')\n",
    "        \n",
    "        examples = []\n",
    "        \n",
    "        for i in range(len(src_txt)):\n",
    "            src = src_txt[i].strip()\n",
    "            trg = trg_txt[i].strip()\n",
    "            src_g = src_gender_labels[i].strip()\n",
    "            trg_g = trg_gender_labels[i].strip()\n",
    "            input_example = InputExample(src, trg, src_g, trg_g)\n",
    "            examples.append(input_example)\n",
    "        \n",
    "        return examples\n",
    "    \n",
    "    def get_labels(self, data_dir):\n",
    "        with open(data_dir) as f:\n",
    "            return f.readlines()\n",
    "        \n",
    "    def get_txt_examples(self, data_dir):\n",
    "        with open(data_dir, encoding='utf8') as f:\n",
    "            return f.readlines()\n",
    "    \n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Reads the train examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-train.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-train.ar.M'))\n",
    "    \n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Reads the dev examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-dev.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-dev.ar.M'))\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Reads the test examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-test.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-test.ar.M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Base vocabulary class\"\"\"\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = dict()\n",
    "        \n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        return self.idx_to_token[index]\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self.token_to_idx}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "    \n",
    "class SeqVocabulary(Vocabulary):\n",
    "    \"\"\"Sequence vocabulary class\"\"\"\n",
    "    def __init__(self, token_to_idx=None, unk_token='<unk>',\n",
    "                 pad_token='<pad>', sos_token='<s>',\n",
    "                 eos_token='</s>'):\n",
    "        \n",
    "        super(SeqVocabulary, self).__init__(token_to_idx)\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        \n",
    "        self.pad_idx = self.add_token(self.pad_token)\n",
    "        self.unk_idx = self.add_token(self.unk_token)\n",
    "        self.sos_idx = self.add_token(self.sos_token)\n",
    "        self.eos_idx = self.add_token(self.eos_token)\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        contents = super(SeqVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self.unk_token,\n",
    "                         'pad_token': self.pad_token,\n",
    "                         'sos_token': self.sos_token, \n",
    "                         'eos_token': self.eos_token})\n",
    "        return contents\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx.get(token, self.unk_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphFeaturizer:\n",
    "    \"\"\"Morphological Featurizer Class\"\"\"\n",
    "    def __init__(self, analyzer_db_path):\n",
    "        self.db = CalimaStarDB(analyzer_db_path)\n",
    "        self.analyzer = CalimaStarAnalyzer(db, cache_size=46000)\n",
    "        self.disambiguator = MLEDisambiguator(analyzer)\n",
    "        self.w_to_features = {}\n",
    "    \n",
    "    def featurize(self, sentence):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - sentence (str): a sentence in Arabic\n",
    "        Returns:\n",
    "            - a dictionary of word to vector mapping for each word in the sentence.\n",
    "              Each vector will be a one-hot representing the following features:\n",
    "              [lex+m lex+f spvar+m spvar+f]\n",
    "        \"\"\"\n",
    "        # using the MLEDisambiguator to get the analyses\n",
    "        disambiguations = self.disambiguator.disambiguate(sentence.split(' '), top=0)\n",
    "        # disambiguations is a list of DisambiguatedWord objects\n",
    "        # each DisambiguatedWord object is a tuple of: (word, scored_analyses)\n",
    "        # scored_analyses is a list of ScoredAnalysis objects\n",
    "        # each ScoredAnalysis object is a tuple of: (score, analysis)\n",
    "    \n",
    "        for disambig in disambiguations:\n",
    "            word, scored_analyses = disambig\n",
    "            if word not in self.w_to_features:\n",
    "                self.w_to_features[word] = list()\n",
    "                if scored_analyses:\n",
    "                    for scored_analysis in scored_analyses:\n",
    "                        # each analysis will have a vector\n",
    "                        score, analysis = scored_analysis\n",
    "                        features = np.zeros(4, dtype=int)\n",
    "\n",
    "                        # getting the source and gender features\n",
    "                        src = analysis['source']\n",
    "                        gen = analysis['gen']\n",
    "\n",
    "                        if src == 'lex' and gen == 'm':\n",
    "                            features[0] = 1\n",
    "                        elif src == 'lex' and gen == 'f':\n",
    "                            features[1] = 1\n",
    "                        elif src == 'spvar' and gen == 'm':\n",
    "                            features[2] = 1\n",
    "                        elif src == 'spvar' and gen == 'f':\n",
    "                            features[3] = 1\n",
    "\n",
    "                        self.w_to_features[word].append(features)\n",
    "\n",
    "                    # squashing all the vectors into one\n",
    "                    self.w_to_features[word] = np.array(self.w_to_features[word])\n",
    "                    self.w_to_features[word] = np.array(self.w_to_features[word].sum(axis=0) != 0, dtype=int).tolist()\n",
    "                else:\n",
    "                    self.w_to_features[word] = features = np.zeros(4, dtype=int).tolist()\n",
    "\n",
    "    def featurize_sentences(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            self.featurize(sentence)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'morph_features': self.w_to_features}\n",
    "    \n",
    "    def from_serializable(self, contents):\n",
    "        self.w_to_features = contents['morph_features']\n",
    "        \n",
    "    def save_morph_features(self, path):\n",
    "        with open(path, mode='w', encoding='utf8') as f:\n",
    "            return json.dump(self.to_serializable(), f, ensure_ascii=False)\n",
    "    \n",
    "    def load_morph_features(self, path):\n",
    "        with open(path) as f:\n",
    "            return self.from_serializable(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    \"\"\"Vectorizer Class\"\"\"\n",
    "    def __init__(self, src_vocab_char, trg_vocab_char, src_vocab_word, trg_vocab_word):\n",
    "        \"\"\"src_vocab_char and trg_vocab_char \n",
    "        are on the char level. src_vocab_word and \n",
    "        trg_vocab_word are on the word level\"\"\"\n",
    "        self.src_vocab_char = src_vocab_char\n",
    "        self.trg_vocab_char = trg_vocab_char\n",
    "        self.src_vocab_word = src_vocab_word\n",
    "        self.trg_vocab_word = trg_vocab_word\n",
    "        \n",
    "    @classmethod\n",
    "    def create_vectorizer(cls, data_examples):\n",
    "        \"\"\"Class method which builds the vectorizer\n",
    "        vocab\"\"\"\n",
    "        \n",
    "        src_vocab_char = SeqVocabulary()\n",
    "        trg_vocab_char = SeqVocabulary()\n",
    "        src_vocab_word = SeqVocabulary()\n",
    "        trg_vocab_word = SeqVocabulary()\n",
    "        \n",
    "        for ex in data_examples:\n",
    "            src = ex.src\n",
    "            trg = ex.trg\n",
    "            \n",
    "            # splitting by a regex to maintain the space\n",
    "            src = re.split(r'(\\s+)', src)\n",
    "            trg = re.split(r'(\\s+)', trg)\n",
    "    \n",
    "            for word in src:\n",
    "                src_vocab_word.add_token(word)\n",
    "                src_vocab_char.add_many(list(word))\n",
    "                \n",
    "            for word in trg:\n",
    "                trg_vocab_word.add_token(word)\n",
    "                trg_vocab_char.add_many(list(word))\n",
    "        \n",
    "        return cls(src_vocab_char, trg_vocab_char, src_vocab_word, trg_vocab_word)\n",
    "    \n",
    "    def get_src_indices(self, seq):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - seq (str): The src sequence\n",
    "        \n",
    "        Returns:\n",
    "          - char_level_indices (list): <s> + List of chars to index mapping + </s>\n",
    "          - word_level_indices (list): <s> + List of words to index mapping + </s>\n",
    "        \"\"\"\n",
    "        char_level_indices = [self.src_vocab_char.sos_idx]\n",
    "        word_level_indices = [self.src_vocab_word.sos_idx]\n",
    "        seq = re.split(r'(\\s+)', seq)\n",
    "        for word in seq:\n",
    "            for c in word:\n",
    "                char_level_indices.append(self.src_vocab_char.lookup_token(c))\n",
    "                word_level_indices.append(self.src_vocab_word.lookup_token(word))\n",
    "        \n",
    "        word_level_indices.append(self.src_vocab_word.eos_idx)\n",
    "        char_level_indices.append(self.src_vocab_char.eos_idx)\n",
    "        \n",
    "        assert len(word_level_indices) == len(char_level_indices)\n",
    "        return char_level_indices, word_level_indices\n",
    "    \n",
    "    def get_trg_indices(self, seq):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - seq (str): The trg sequence\n",
    "        \n",
    "        Returns:\n",
    "          - trg_x_indices (list): <s> + List of tokens to index mapping\n",
    "          - trg_y_indices (list): List of tokens to index mapping + </s>\n",
    "        \"\"\"\n",
    "        indices = [self.trg_vocab_char.lookup_token(t) for t in seq]\n",
    "        \n",
    "        trg_x_indices = [self.trg_vocab_char.sos_idx] + indices\n",
    "        trg_y_indices = indices + [self.trg_vocab_char.eos_idx]\n",
    "        return trg_x_indices, trg_y_indices\n",
    "    \n",
    "    \n",
    "    def vectorize(self, src, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - src (str): The src sequence\n",
    "          - src (str): The trg sequence\n",
    "        Returns:\n",
    "          - vectorized_src \n",
    "          - vectorized_trg_x \n",
    "          - vectorized_trg_y\n",
    "        \"\"\"\n",
    "        src = src\n",
    "        trg = trg\n",
    "        \n",
    "        vectorized_src_char, vectorized_src_word = self.get_src_indices(src)\n",
    "        vectorized_trg_x, vectorized_trg_y = self.get_trg_indices(trg)\n",
    "        \n",
    "        return {'src_char': torch.tensor(vectorized_src_char, dtype=torch.long),\n",
    "                'src_word': torch.tensor(vectorized_src_word, dtype=torch.long),\n",
    "                'trg_x': torch.tensor(vectorized_trg_x, dtype=torch.long),\n",
    "                'trg_y': torch.tensor(vectorized_trg_y, dtype=torch.long)\n",
    "               }\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'src_vocab_char': self.src_vocab_char.to_serializable(),\n",
    "                'trg_vocab_char': self.trg_vocab_char.to_serializable(),\n",
    "                'src_vocab_word': self.src_vocab_word.to_serializable(),\n",
    "                'trg_vocab_word': self.trg_vocab_word.to_serializable()\n",
    "               }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        src_vocab_char = SeqVocabulary.from_serializable(contents['src_vocab_char'])\n",
    "        trg_vocab_char = SeqVocabulary.from_serializable(contents['trg_vocab_char'])\n",
    "        src_vocab_word = SeqVocabulary.from_serializable(contents['src_vocab_word'])\n",
    "        trg_vocab_word = SeqVocabulary.from_serializable(contents['trg_vocab_word'])\n",
    "        return cls(src_vocab_char, trg_vocab_char, src_vocab_word, trg_vocab_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MT_Dataset(Dataset):\n",
    "    \"\"\"MT Dataset as a PyTorch dataset\"\"\"\n",
    "    def __init__(self, raw_dataset, vectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.train_examples = raw_dataset.train_examples\n",
    "        self.dev_examples = raw_dataset.dev_examples\n",
    "        self.test_examples = raw_dataset.test_examples\n",
    "        self.lookup_split = {'train': self.train_examples,\n",
    "                             'dev': self.dev_examples,\n",
    "                             'test': self.test_examples}\n",
    "        self.set_split('train')\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "    \n",
    "    @classmethod\n",
    "    def load_data_and_create_vectorizer(cls, data_dir):\n",
    "        raw_dataset = RawDataset(data_dir)\n",
    "        # Note: we always create the vectorized based on the train examples\n",
    "        vectorizer = Vectorizer.create_vectorizer(raw_dataset.train_examples)\n",
    "        return cls(raw_dataset, vectorizer)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_data_and_load_vectorizer(cls, data_dir, vec_path):\n",
    "        raw_dataset = RawDataset(data_dir)\n",
    "        vectorizer = cls.load_vectorizer(vec_path)\n",
    "        return cls(raw_dataset, vectorizer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer(vec_path):\n",
    "        with open(vec_path) as f:\n",
    "            return Vectorizer.from_serializable(json.load(f))\n",
    "    \n",
    "    def save_vectorizer(self, vec_path):\n",
    "        with open(vec_path, 'w') as f:\n",
    "            return json.dump(self.vectorizer.to_serializable(), f)\n",
    "        \n",
    "    def set_split(self, split):\n",
    "        self.split = split\n",
    "        self.split_examples = self.lookup_split[self.split]\n",
    "        return self.split_examples\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        example = self.split_examples[index]\n",
    "        src, trg = example.src, example.trg\n",
    "        vectorized = self.vectorizer.vectorize(src, trg)\n",
    "        return vectorized\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.split_examples)\n",
    "    \n",
    "    \n",
    "class Collator:\n",
    "    def __init__(self, src_pad_idx, trg_pad_idx):\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # Sorting the batch by src seqs length in descending order\n",
    "        sorted_batch = sorted(batch, key=lambda x: x['src_char'].shape[0], reverse=True)\n",
    "        \n",
    "        src_char_seqs = [x['src_char'] for x in sorted_batch]\n",
    "        src_word_seqs = [x['src_word'] for x in sorted_batch]\n",
    "        assert len(src_word_seqs) == len(src_char_seqs)\n",
    "        trg_x_seqs = [x['trg_x'] for x in sorted_batch]\n",
    "        trg_y_seqs = [x['trg_y'] for x in sorted_batch]\n",
    "        lengths = [len(seq) for seq in src_char_seqs]\n",
    "        \n",
    "        padded_src_char_seqs = pad_sequence(src_char_seqs, batch_first=True, padding_value=self.src_pad_idx)\n",
    "        padded_src_word_seqs = pad_sequence(src_word_seqs, batch_first=True, padding_value=self.src_pad_idx)\n",
    "        padded_trg_x_seqs = pad_sequence(trg_x_seqs, batch_first=True, padding_value=self.trg_pad_idx)\n",
    "        padded_trg_y_seqs = pad_sequence(trg_y_seqs, batch_first=True, padding_value=self.trg_pad_idx)\n",
    "        lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "        \n",
    "        return {'src_char': padded_src_char_seqs,\n",
    "                'src_word': padded_src_word_seqs,\n",
    "                'trg_x': padded_trg_x_seqs,\n",
    "                'trg_y': padded_trg_y_seqs,\n",
    "                'src_lengths': lengths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_morph_embeddings(morph_featurizer, morph_feature_path, word_vocab):\n",
    "    \"\"\"Creating a morphological features embedding matrix\"\"\"\n",
    "    # Loading the morph features\n",
    "    morph_featurizer.load_morph_features(morph_feature_path)\n",
    "    morph_features = morph_featurizer.w_to_features\n",
    "    \n",
    "    # Note: morph_features will have all the words in word_vocab\n",
    "    # except: <s>, pad, unk, </s>, ' '\n",
    "    \n",
    "    # Creating a zero embedding matrix of shape: (len(src_word_vocab), 4)\n",
    "    morph_embedding_matrix = torch.zeros((len(word_vocab), 4))\n",
    "    for word in word_vocab.token_to_idx:\n",
    "        if word in morph_features:\n",
    "            index = word_vocab.lookup_token(word)\n",
    "            morph_embedding_matrix[index] = torch.tensor(morph_features[word], dtype=torch.float64)\n",
    "    return morph_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder bi-GRU\"\"\"\n",
    "    def __init__(self, input_dim, embed_dim,\n",
    "                 hidd_dim, morph_embedding,\n",
    "                 char_padding_idx=0, word_padding_idx=0):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.char_embedding_layer = nn.Embedding(input_dim, embed_dim, padding_idx=char_padding_idx)\n",
    "        self.morph_embedding_layer = nn.Embedding.from_pretrained(morph_embedding, padding_idx=word_padding_idx)\n",
    "        self.rnn = nn.GRU(embed_dim + 4, hidd_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, char_src_seqs, word_src_seqs, src_seqs_lengths):\n",
    "    \n",
    "        embedded_char_seqs = self.char_embedding_layer(char_src_seqs)\n",
    "        # embedded_char_seqs shape: [batch_size, max_src_seq_len, embed_dim]\n",
    "        \n",
    "        embedded_word_seqs = self.morph_embedding_layer(word_src_seqs)\n",
    "        # embedded_char_seqs shape: [batch_size, max_src_seq_len, 4]\n",
    "\n",
    "        embedded_seqs = torch.cat((embedded_char_seqs, embedded_word_seqs), dim=2)\n",
    "        # embedded_seqs shape: [batch_size, max_src_seq_len, embed_dim + 4]\n",
    "        \n",
    "        # packing the embedded_seqs\n",
    "        packed_embedded_seqs = pack_padded_sequence(embedded_seqs, src_seqs_lengths, batch_first=True)\n",
    "        \n",
    "        output, hidd = self.rnn(packed_embedded_seqs)\n",
    "        # hidd shape: [num_layers * num_dirs, batch_size, hidd_dim]\n",
    "        \n",
    "        # changing hidd shape to: [batch_size, num_layers * num_dirs, hidd_dim]\n",
    "        hidd = hidd.permute(1, 0 ,2)\n",
    "        \n",
    "        # changing hidd shape to: [batch_size, num_layers * num_dirs * hidd_dim]\n",
    "        hidd = hidd.contiguous().view(hidd.shape[0], -1)\n",
    "        \n",
    "        # unpacking the output\n",
    "        output, lengths = pad_packed_sequence(output, batch_first=True)\n",
    "        # output shape: [batch_size, src_seqs_length, num_dirs * hidd_dim]\n",
    "        return output, hidd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_featurizer = MorphFeaturizer('/home/ba63/databases/calima-msa/calima-msa.0.2.2.utf8.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_featurizer.load_morph_features('/home/ba63/databases/morph_features.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MT_Dataset.load_data_and_create_vectorizer('/home/ba63/gender-bias/data/christine_2019/'\\\n",
    "                                                     'Arabic-parallel-gender-corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = make_morph_embeddings(morph_featurizer, morph_feature_path='/home/ba63/databases/morph_features.json',\n",
    "                             word_vocab=vectorizer.src_vocab_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_INPUT_DIM = len(vectorizer.src_vocab_char)\n",
    "embed_dim = 10\n",
    "hidd_dim = 64\n",
    "CHAR_PADDING_IDX = vectorizer.src_vocab_char.pad_idx\n",
    "WORD_PADDING_IDX = vectorizer.src_vocab_word.pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=ENCODER_INPUT_DIM, \n",
    "                  embed_dim=embed_dim,\n",
    "                  hidd_dim=hidd_dim,\n",
    "                  morph_embedding=matrix,\n",
    "                  char_padding_idx=0,\n",
    "                  word_padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = Collator(src_pad_idx=vectorizer.src_vocab_char.pad_idx, \n",
    "                    trg_pad_idx=vectorizer.trg_vocab_char.pad_idx)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader:\n",
    "    src_char = batch['src_char']\n",
    "    src_word = batch['src_word']\n",
    "    lengths = batch['src_lengths']\n",
    "    output, hidd = encoder(src_char, src_word, lengths)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder bi-GRU\"\"\"\n",
    "    def __init__(self, input_dim, embed_dim,\n",
    "                 hidd_dim, padding_idx=0):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(input_dim, embed_dim, padding_idx=padding_idx)\n",
    "        self.rnn = nn.GRU(embed_dim, hidd_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, src_seqs, src_seqs_lengths):\n",
    "    \n",
    "        embedded_seqs = self.embedding_layer(src_seqs)\n",
    "        # embedded_seqs shape: [batch_size, max_src_seq_len, embed_dim]\n",
    "        \n",
    "        # packing the embedded_seqs\n",
    "        packed_embedded_seqs = pack_padded_sequence(embedded_seqs, src_seqs_lengths, batch_first=True)\n",
    "        \n",
    "        output, hidd = self.rnn(packed_embedded_seqs)\n",
    "        # hidd shape: [num_layers * num_dirs, batch_size, hidd_dim]\n",
    "        \n",
    "        # changing hidd shape to: [batch_size, num_layers * num_dirs, hidd_dim]\n",
    "        hidd = hidd.permute(1, 0 ,2)\n",
    "        \n",
    "        # changing hidd shape to: [batch_size, num_layers * num_dirs * hidd_dim]\n",
    "        hidd = hidd.contiguous().view(hidd.shape[0], -1)\n",
    "        \n",
    "        # unpacking the output\n",
    "        output, lengths = pad_packed_sequence(output, batch_first=True)\n",
    "        # output shape: [batch_size, src_seqs_length, num_dirs * hidd_dim]\n",
    "        return output, hidd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MT_Dataset.load_data_and_create_vectorizer('/home/ba63/gender-bias/data/christine_2019/'\\\n",
    "                                                     'Arabic-parallel-gender-corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': tensor([ 2,  4,  5,  6,  7,  8,  9, 10,  4, 11, 10, 12,  5, 13, 14, 10, 15, 10,\n",
       "         16, 14, 11, 12, 17, 14, 18, 10, 19, 11,  8,  9, 10, 15,  3]),\n",
       " 'trg_x': tensor([ 2,  4,  5,  6,  7,  8,  9, 10,  4, 11, 10, 12,  5, 13, 14, 10, 15, 10,\n",
       "         16, 14, 11, 12, 17, 14, 18, 10, 19, 11,  8,  9, 10, 15]),\n",
       " 'trg_y': tensor([ 4,  5,  6,  7,  8,  9, 10,  4, 11, 10, 12,  5, 13, 14, 10, 15, 10, 16,\n",
       "         14, 11, 12, 17, 14, 18, 10, 19, 11,  8,  9, 10, 15,  3])}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'my name is bashar'\n",
    "char_vocab = Vocabulary()\n",
    "word_vocab = Vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in sentence.split(' '):\n",
    "    word_vocab.add_token(w)\n",
    "    char_vocab.add_many(list(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m': 0,\n",
       " 'y': 1,\n",
       " 'n': 2,\n",
       " 'a': 3,\n",
       " 'e': 4,\n",
       " 'i': 5,\n",
       " 's': 6,\n",
       " 'b': 7,\n",
       " 'h': 8,\n",
       " 'r': 9}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vocab.token_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_char_seq = torch.tensor([[char_vocab.lookup_token(t) for t in sentence]])\n",
    "vectorized_word_seq = torch.tensor([[word_vocab.lookup_token(t) for t in sentence.split(' ')]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = nn.Embedding(len(word_vocab), 10)\n",
    "char_embedding = nn.Embedding(len(char_vocab), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_word = word_embedding(vectorized_word_seq)\n",
    "embedded_char = char_embedding(vectorized_char_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 10])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_word = embedded_word.view(embedded_word.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17, 10])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_char.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [torch.tensor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running a check on the model before training.\n",
      "Sentences:\n",
      "everybody eat the food . I kept looking out the window , trying to find the one I was waiting for .\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'function' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-65ade1cb7e70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running a check on the model before training.\\nSentences:\\n{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_to_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_to_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-126-65ade1cb7e70>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running a check on the model before training.\\nSentences:\\n{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_to_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_to_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-2c08a9e05244>\u001b[0m in \u001b[0;36msequence_to_idx\u001b[0;34m(sequence, ix)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msequence_to_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-124-2c08a9e05244>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msequence_to_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'function' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
