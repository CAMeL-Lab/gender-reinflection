{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.calima_star.database import CalimaStarDB\n",
    "from camel_tools.calima_star.analyzer import CalimaStarAnalyzer\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample:\n",
    "    \"\"\"Simple object to encapsulate each data example\"\"\"\n",
    "    def __init__(self, src, trg, \n",
    "                 src_g, trg_g):    \n",
    "        self.src = src\n",
    "        self.trg = trg\n",
    "        self.src_g = src_g\n",
    "        self.trg_g = trg_g\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_str())\n",
    "    \n",
    "    def to_json_str(self):\n",
    "        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)\n",
    "    \n",
    "    def to_dict(self):\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataset:\n",
    "    \"\"\"Encapsulates the raw examples in InputExample objects\"\"\"\n",
    "    def __init__(self, data_dir):\n",
    "        self.train_examples = self.get_train_examples(data_dir)\n",
    "        self.dev_examples = self.get_dev_examples(data_dir)\n",
    "        self.test_examples = self.get_dev_examples(data_dir)\n",
    "        \n",
    "    def create_examples(self, src_path, trg_path):\n",
    "        \n",
    "        src_txt = self.get_txt_examples(src_path)\n",
    "        src_gender_labels = self.get_labels(src_path + '.label')\n",
    "        trg_txt = self.get_txt_examples(trg_path)\n",
    "        trg_gender_labels = self.get_labels(trg_path + '.label')\n",
    "        \n",
    "        examples = []\n",
    "        \n",
    "        for i in range(len(src_txt)):\n",
    "            src = src_txt[i].strip()\n",
    "            trg = trg_txt[i].strip()\n",
    "            src_g = src_gender_labels[i].strip()\n",
    "            trg_g = trg_gender_labels[i].strip()\n",
    "            input_example = InputExample(src, trg, src_g, trg_g)\n",
    "            examples.append(input_example)\n",
    "        \n",
    "        return examples\n",
    "    \n",
    "    def get_labels(self, data_dir):\n",
    "        with open(data_dir) as f:\n",
    "            return f.readlines()\n",
    "        \n",
    "    def get_txt_examples(self, data_dir):\n",
    "        with open(data_dir, encoding='utf8') as f:\n",
    "            return f.readlines()\n",
    "    \n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Reads the train examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-train.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-train.ar.M'))\n",
    "    \n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Reads the dev examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-dev.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-dev.ar.M'))\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Reads the test examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-test.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-test.ar.M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Base vocabulary class\"\"\"\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = dict()\n",
    "        \n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        return self.idx_to_token[index]\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self.token_to_idx}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "    \n",
    "class SeqVocabulary(Vocabulary):\n",
    "    \"\"\"Sequence vocabulary class\"\"\"\n",
    "    def __init__(self, token_to_idx=None, unk_token='<unk>',\n",
    "                 pad_token='<pad>', sos_token='<s>',\n",
    "                 eos_token='</s>'):\n",
    "        \n",
    "        super(SeqVocabulary, self).__init__(token_to_idx)\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        \n",
    "        self.pad_idx = self.add_token(self.pad_token)\n",
    "        self.unk_idx = self.add_token(self.unk_token)\n",
    "        self.sos_idx = self.add_token(self.sos_token)\n",
    "        self.eos_idx = self.add_token(self.eos_token)\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        contents = super(SeqVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self.unk_token,\n",
    "                         'pad_token': self.pad_token,\n",
    "                         'sos_token': self.sos_token, \n",
    "                         'eos_token': self.eos_token})\n",
    "        return contents\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx.get(token, self.unk_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphFeaturizer:\n",
    "    \"\"\"Morphological Featurizer Class\"\"\"\n",
    "    def __init__(self, analyzer_db_path):\n",
    "        self.db = CalimaStarDB(analyzer_db_path)\n",
    "        self.analyzer = CalimaStarAnalyzer(self.db, cache_size=46000)\n",
    "        self.disambiguator = MLEDisambiguator(self.analyzer)\n",
    "        self.w_to_features = {}\n",
    "    \n",
    "    def featurize(self, sentence):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - sentence (str): a sentence in Arabic\n",
    "        Returns:\n",
    "            - a dictionary of word to vector mapping for each word in the sentence.\n",
    "              Each vector will be a one-hot representing the following features:\n",
    "              [lex+m lex+f spvar+m spvar+f]\n",
    "        \"\"\"\n",
    "        # using the MLEDisambiguator to get the analyses\n",
    "        disambiguations = self.disambiguator.disambiguate(sentence.split(' '), top=0)\n",
    "        # disambiguations is a list of DisambiguatedWord objects\n",
    "        # each DisambiguatedWord object is a tuple of: (word, scored_analyses)\n",
    "        # scored_analyses is a list of ScoredAnalysis objects\n",
    "        # each ScoredAnalysis object is a tuple of: (score, analysis)\n",
    "    \n",
    "        for disambig in disambiguations:\n",
    "            word, scored_analyses = disambig\n",
    "            if word not in self.w_to_features:\n",
    "                self.w_to_features[word] = list()\n",
    "                if scored_analyses:\n",
    "                    for scored_analysis in scored_analyses:\n",
    "                        # each analysis will have a vector\n",
    "                        score, analysis = scored_analysis\n",
    "                        features = np.zeros(4)\n",
    "\n",
    "                        # getting the source and gender features\n",
    "                        src = analysis['source']\n",
    "                        gen = analysis['gen']\n",
    "\n",
    "                        if src == 'lex' and gen == 'm':\n",
    "                            features[0] = 1\n",
    "                        elif src == 'lex' and gen == 'f':\n",
    "                            features[1] = 1\n",
    "                        elif src == 'spvar' and gen == 'm':\n",
    "                            features[2] = 1\n",
    "                        elif src == 'spvar' and gen == 'f':\n",
    "                            features[3] = 1\n",
    "\n",
    "                        self.w_to_features[word].append(features)\n",
    "\n",
    "                    # squashing all the vectors into one\n",
    "                    self.w_to_features[word] = np.array(self.w_to_features[word])\n",
    "                    self.w_to_features[word] = self.w_to_features[word].sum(axis=0)\n",
    "                    # replacing all the elements > 0 with 1\n",
    "                    self.w_to_features[word][self.w_to_features[word] > 0] = 1\n",
    "                    # replacing all the 0 elements with 1e-6\n",
    "                    self.w_to_features[word][self.w_to_features[word] == 0] = 1e-6\n",
    "                    self.w_to_features[word] = self.w_to_features[word].tolist()\n",
    "                else:\n",
    "                    self.w_to_features[word] = np.full((4), 1e-6).tolist()\n",
    "\n",
    "    def featurize_sentences(self, sentences):\n",
    "        \"\"\"Featurizes a list of sentences\"\"\"\n",
    "        for sentence in sentences:\n",
    "            self.featurize(sentence)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'morph_features': self.w_to_features}\n",
    "    \n",
    "    def from_serializable(self, contents):\n",
    "        self.w_to_features = contents['morph_features']\n",
    "        \n",
    "    def save_morph_features(self, path):\n",
    "        with open(path, mode='w', encoding='utf8') as f:\n",
    "            return json.dump(self.to_serializable(), f, ensure_ascii=False)\n",
    "    \n",
    "    def load_morph_features(self, path):\n",
    "        with open(path) as f:\n",
    "            return self.from_serializable(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    \"\"\"Vectorizer Class\"\"\"\n",
    "    def __init__(self, src_vocab_char, trg_vocab_char, src_vocab_word, trg_vocab_word):\n",
    "        \"\"\"src_vocab_char and trg_vocab_char \n",
    "        are on the char level. src_vocab_word and \n",
    "        trg_vocab_word are on the word level\"\"\"\n",
    "        self.src_vocab_char = src_vocab_char\n",
    "        self.trg_vocab_char = trg_vocab_char\n",
    "        self.src_vocab_word = src_vocab_word\n",
    "        self.trg_vocab_word = trg_vocab_word\n",
    "        \n",
    "    @classmethod\n",
    "    def create_vectorizer(cls, data_examples):\n",
    "        \"\"\"Class method which builds the vectorizer\n",
    "        vocab\"\"\"\n",
    "        \n",
    "        src_vocab_char = SeqVocabulary()\n",
    "        trg_vocab_char = SeqVocabulary()\n",
    "        src_vocab_word = SeqVocabulary()\n",
    "        trg_vocab_word = SeqVocabulary()\n",
    "        \n",
    "        for ex in data_examples:\n",
    "            src = ex.src\n",
    "            trg = ex.trg\n",
    "            \n",
    "            # splitting by a regex to maintain the space\n",
    "            src = re.split(r'(\\s+)', src)\n",
    "            trg = re.split(r'(\\s+)', trg)\n",
    "    \n",
    "            for word in src:\n",
    "                src_vocab_word.add_token(word)\n",
    "                src_vocab_char.add_many(list(word))\n",
    "                \n",
    "            for word in trg:\n",
    "                trg_vocab_word.add_token(word)\n",
    "                trg_vocab_char.add_many(list(word))\n",
    "        \n",
    "        return cls(src_vocab_char, trg_vocab_char, src_vocab_word, trg_vocab_word)\n",
    "    \n",
    "    def get_src_indices(self, seq):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - seq (str): The src sequence\n",
    "        \n",
    "        Returns:\n",
    "          - char_level_indices (list): <s> + List of chars to index mapping + </s>\n",
    "          - word_level_indices (list): <s> + List of words to index mapping + </s>\n",
    "        \"\"\"\n",
    "        char_level_indices = [self.src_vocab_char.sos_idx]\n",
    "        word_level_indices = [self.src_vocab_word.sos_idx]\n",
    "        seq = re.split(r'(\\s+)', seq)\n",
    "        for word in seq:\n",
    "            for c in word:\n",
    "                char_level_indices.append(self.src_vocab_char.lookup_token(c))\n",
    "                word_level_indices.append(self.src_vocab_word.lookup_token(word))\n",
    "        \n",
    "        word_level_indices.append(self.src_vocab_word.eos_idx)\n",
    "        char_level_indices.append(self.src_vocab_char.eos_idx)\n",
    "        \n",
    "        assert len(word_level_indices) == len(char_level_indices)\n",
    "        return char_level_indices, word_level_indices\n",
    "    \n",
    "    def get_trg_indices(self, seq):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - seq (str): The trg sequence\n",
    "        \n",
    "        Returns:\n",
    "          - trg_x_indices (list): <s> + List of chars to index mapping\n",
    "          - trg_y_indices (list): List of chars to index mapping + </s>\n",
    "        \"\"\"\n",
    "        indices = [self.trg_vocab_char.lookup_token(t) for t in seq]\n",
    "        \n",
    "        trg_x_indices = [self.trg_vocab_char.sos_idx] + indices\n",
    "        trg_y_indices = indices + [self.trg_vocab_char.eos_idx]\n",
    "        return trg_x_indices, trg_y_indices\n",
    "    \n",
    "    \n",
    "    def vectorize(self, src, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - src (str): The src sequence\n",
    "          - src (str): The trg sequence\n",
    "        Returns:\n",
    "          - vectorized_src_char\n",
    "          - vectorized_src_word\n",
    "          - vectorized_trg_x \n",
    "          - vectorized_trg_y\n",
    "        \"\"\"\n",
    "        src = src\n",
    "        trg = trg\n",
    "        \n",
    "        vectorized_src_char, vectorized_src_word = self.get_src_indices(src)\n",
    "        vectorized_trg_x, vectorized_trg_y = self.get_trg_indices(trg)\n",
    "        \n",
    "        return {'src_char': torch.tensor(vectorized_src_char, dtype=torch.long),\n",
    "                'src_word': torch.tensor(vectorized_src_word, dtype=torch.long),\n",
    "                'trg_x': torch.tensor(vectorized_trg_x, dtype=torch.long),\n",
    "                'trg_y': torch.tensor(vectorized_trg_y, dtype=torch.long)\n",
    "               }\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'src_vocab_char': self.src_vocab_char.to_serializable(),\n",
    "                'trg_vocab_char': self.trg_vocab_char.to_serializable(),\n",
    "                'src_vocab_word': self.src_vocab_word.to_serializable(),\n",
    "                'trg_vocab_word': self.trg_vocab_word.to_serializable()\n",
    "               }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        src_vocab_char = SeqVocabulary.from_serializable(contents['src_vocab_char'])\n",
    "        trg_vocab_char = SeqVocabulary.from_serializable(contents['trg_vocab_char'])\n",
    "        src_vocab_word = SeqVocabulary.from_serializable(contents['src_vocab_word'])\n",
    "        trg_vocab_word = SeqVocabulary.from_serializable(contents['trg_vocab_word'])\n",
    "        return cls(src_vocab_char, trg_vocab_char, src_vocab_word, trg_vocab_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MT_Dataset(Dataset):\n",
    "    \"\"\"MT Dataset as a PyTorch dataset\"\"\"\n",
    "    def __init__(self, raw_dataset, vectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.train_examples = raw_dataset.train_examples\n",
    "        self.dev_examples = raw_dataset.dev_examples\n",
    "        self.test_examples = raw_dataset.test_examples\n",
    "        self.lookup_split = {'train': self.train_examples,\n",
    "                             'dev': self.dev_examples,\n",
    "                             'test': self.test_examples}\n",
    "        self.set_split('train')\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "    \n",
    "    @classmethod\n",
    "    def load_data_and_create_vectorizer(cls, data_dir):\n",
    "        raw_dataset = RawDataset(data_dir)\n",
    "        # Note: we always create the vectorizer based on the train examples\n",
    "        vectorizer = Vectorizer.create_vectorizer(raw_dataset.train_examples)\n",
    "        return cls(raw_dataset, vectorizer)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_data_and_load_vectorizer(cls, data_dir, vec_path):\n",
    "        raw_dataset = RawDataset(data_dir)\n",
    "        vectorizer = cls.load_vectorizer(vec_path)\n",
    "        return cls(raw_dataset, vectorizer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer(vec_path):\n",
    "        with open(vec_path) as f:\n",
    "            return Vectorizer.from_serializable(json.load(f))\n",
    "    \n",
    "    def save_vectorizer(self, vec_path):\n",
    "        with open(vec_path, 'w') as f:\n",
    "            return json.dump(self.vectorizer.to_serializable(), f)\n",
    "        \n",
    "    def set_split(self, split):\n",
    "        self.split = split\n",
    "        self.split_examples = self.lookup_split[self.split]\n",
    "        return self.split_examples\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        example = self.split_examples[index]\n",
    "        src, trg = example.src, example.trg\n",
    "        vectorized = self.vectorizer.vectorize(src, trg)\n",
    "        return vectorized\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.split_examples)\n",
    "    \n",
    "    \n",
    "class Collator:\n",
    "    def __init__(self, src_pad_idx, trg_pad_idx):\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # Sorting the batch by src seqs length in descending order\n",
    "        sorted_batch = sorted(batch, key=lambda x: x['src_char'].shape[0], reverse=True)\n",
    "        \n",
    "        src_char_seqs = [x['src_char'] for x in sorted_batch]\n",
    "        src_word_seqs = [x['src_word'] for x in sorted_batch]\n",
    "        assert len(src_word_seqs) == len(src_char_seqs)\n",
    "        trg_x_seqs = [x['trg_x'] for x in sorted_batch]\n",
    "        trg_y_seqs = [x['trg_y'] for x in sorted_batch]\n",
    "        lengths = [len(seq) for seq in src_char_seqs]\n",
    "        \n",
    "        padded_src_char_seqs = pad_sequence(src_char_seqs, batch_first=True, padding_value=self.src_pad_idx)\n",
    "        padded_src_word_seqs = pad_sequence(src_word_seqs, batch_first=True, padding_value=self.src_pad_idx)\n",
    "        padded_trg_x_seqs = pad_sequence(trg_x_seqs, batch_first=True, padding_value=self.trg_pad_idx)\n",
    "        padded_trg_y_seqs = pad_sequence(trg_y_seqs, batch_first=True, padding_value=self.trg_pad_idx)\n",
    "        lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "        \n",
    "        return {'src_char': padded_src_char_seqs,\n",
    "                'src_word': padded_src_word_seqs,\n",
    "                'trg_x': padded_trg_x_seqs,\n",
    "                'trg_y': padded_trg_y_seqs,\n",
    "                'src_lengths': lengths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_morph_embeddings(morph_featurizer, word_vocab):\n",
    "    \"\"\"Creating a morphological features embedding matrix\"\"\"\n",
    "    morph_features = morph_featurizer.w_to_features\n",
    "    \n",
    "    # Note: morph_features will have all the words in word_vocab\n",
    "    # except: <s>, pad, unk, </s>, ' '\n",
    "    \n",
    "    # Creating a zero embedding matrix of shape: (len(word_vocab), 4)\n",
    "    morph_embedding_matrix = torch.ones((len(word_vocab), 4)) * 1e-6\n",
    "    for word in word_vocab.token_to_idx:\n",
    "        if word in morph_features:\n",
    "            index = word_vocab.lookup_token(word)\n",
    "            morph_embedding_matrix[index] = torch.tensor(morph_features[word], dtype=torch.float64)\n",
    "    return morph_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder bi-GRU\"\"\"\n",
    "    def __init__(self, input_dim, embed_dim,\n",
    "                 hidd_dim, morph_embedding,\n",
    "                 char_padding_idx=0, word_padding_idx=0):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.char_embedding_layer = nn.Embedding(input_dim, embed_dim, padding_idx=char_padding_idx)\n",
    "        self.morph_embedding_layer = nn.Embedding.from_pretrained(morph_embedding,\n",
    "                                                                  padding_idx=word_padding_idx)\n",
    "        self.rnn = nn.GRU(embed_dim + 4, hidd_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, char_src_seqs, word_src_seqs, src_seqs_lengths):\n",
    "    \n",
    "        embedded_char_seqs = self.char_embedding_layer(char_src_seqs)\n",
    "        # embedded_char_seqs shape: [batch_size, max_src_seq_len, embed_dim]\n",
    "        \n",
    "        embedded_word_seqs = self.morph_embedding_layer(word_src_seqs)\n",
    "        # embedded_char_seqs shape: [batch_size, max_src_seq_len, 4]\n",
    "\n",
    "        embedded_seqs = torch.cat((embedded_char_seqs, embedded_word_seqs), dim=2)\n",
    "        # embedded_seqs shape: [batch_size, max_src_seq_len, embed_dim + 4]\n",
    "        \n",
    "        # packing the embedded_seqs\n",
    "        packed_embedded_seqs = pack_padded_sequence(embedded_seqs, src_seqs_lengths, batch_first=True)\n",
    "        \n",
    "        output, hidd = self.rnn(packed_embedded_seqs)\n",
    "        # hidd shape: [num_layers * num_dirs, batch_size, hidd_dim]\n",
    "        \n",
    "        # changing hidd shape to: [batch_size, num_layers * num_dirs, hidd_dim]\n",
    "        hidd = hidd.permute(1, 0 ,2)\n",
    "        \n",
    "        # changing hidd shape to: [batch_size, num_layers * num_dirs * hidd_dim]\n",
    "        hidd = hidd.contiguous().view(hidd.shape[0], -1)\n",
    "        \n",
    "        # unpacking the output\n",
    "        output, lengths = pad_packed_sequence(output, batch_first=True)\n",
    "        # output shape: [batch_size, src_seqs_length, num_dirs * hidd_dim]\n",
    "        return output, hidd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder GRU\n",
    "       \n",
    "       Things to note:\n",
    "           - The input to the decoder rnn at each time step is the \n",
    "             concatenation of the embedded token and the context vector\n",
    "           - The context vector will have a size of batch_size, hidd_dim\n",
    "           - Note that the decoder hidd_dim == the encoder hidd_dim * 2\n",
    "           - The prediction layer input is the concatenation of \n",
    "             the context vector and the h_t of the decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, embed_dim,\n",
    "                 hidd_dim, output_dim,\n",
    "                 attention,\n",
    "                 padding_idx=0):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidd_dim = hidd_dim\n",
    "        self.attention = attention\n",
    "        self.embedding_layer = nn.Embedding(input_dim, embed_dim, padding_idx=padding_idx)\n",
    "        # the input to the rnn is the context_vector + embedded token --> embed_dim + hidd_dim\n",
    "        self.rnn = nn.GRUCell((embed_dim + hidd_dim), hidd_dim)\n",
    "        # the input to the classifier is h_t + context_vector --> hidd_dim * 2\n",
    "        self.classification_layer = nn.Linear(hidd_dim * 2, output_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, trg_seqs, encoder_outputs, decoder_h_t, context_vectors, attention_mask):\n",
    "        # trg_seqs shape: [batch_size]\n",
    "        batch_size = trg_seqs.shape[0]\n",
    "\n",
    "        # Step 1: embedding the target seqs\n",
    "        embedded_seqs = self.embedding_layer(trg_seqs)\n",
    "        # embedded_seqs shape: [batch_size, embed_dim]\n",
    "        \n",
    "        # concatenating the embedded trg sequence with the context_vectors\n",
    "        rnn_input = torch.cat((embedded_seqs, context_vectors), dim=1)\n",
    "        # rnn_input shape: [batch_size, embed_dim + hidd_dim]\n",
    "        \n",
    "        # Step 2: feeding the input to the rnn and updating the decoder_h_t\n",
    "        decoder_h_t = self.rnn(rnn_input, decoder_h_t)\n",
    "        # decoder_h_t shape: [batch_size, hidd_dim]\n",
    "        \n",
    "        # Step 3: updating the context vectors through attention\n",
    "        context_vectors, atten_scores = self.attention(key_vectors=encoder_outputs, \n",
    "                                                       query_vector=decoder_h_t,\n",
    "                                                       mask=attention_mask)\n",
    "        \n",
    "        # concatenating decoder_h_t with the context_vectors to create a \n",
    "        # prediction vector\n",
    "        predictions_vector = torch.cat((decoder_h_t, context_vectors), dim=1)\n",
    "        # predictions_vector: [batch_size, hidd_dim * 2]\n",
    "        \n",
    "        # Step 4: feeding the prediction vector to the fc layer\n",
    "        # to a make a prediction\n",
    "        prediction = self.classification_layer(predictions_vector)\n",
    "        # prediction shape: [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, decoder_h_t, atten_scores, context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"Attention mechanism as a MLP \n",
    "    as used by Bahdanau et. al 2015\"\"\"\n",
    "\n",
    "    def __init__(self, encoder_hidd_dim, decoder_hidd_dim):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.atten = nn.Linear((encoder_hidd_dim * 2) + decoder_hidd_dim, decoder_hidd_dim)\n",
    "        self.v = nn.Linear(decoder_hidd_dim, 1)\n",
    "        \n",
    "    def forward(self, key_vectors, query_vector, mask):\n",
    "        \"\"\"key_vectors: encoder hidden states.\n",
    "           query_vector: decoder hidden state at time t\n",
    "           mask: the mask vector of zeros and ones\n",
    "        \"\"\"\n",
    "        \n",
    "        #key_vectors shape: [batch_size, src_seq_length, encoder_hidd_dim * 2]\n",
    "        #query_vector shape: [batch_size, decoder_hidd_dim]\n",
    "        #Note: encoder_hidd_dim * 2 == decoder_hidd_dim\n",
    "        \n",
    "        batch_size, src_seq_length, encoder_hidd_dim = key_vectors.shape\n",
    "        \n",
    "        #changing the shape of query_vector to [batch_size, src_seq_length, decoder_hidd_dim]\n",
    "        #we will repeat the query_vector src_seq_length times at dim 1\n",
    "        query_vector = query_vector.unsqueeze(1).repeat(1, src_seq_length, 1)\n",
    "        \n",
    "        # Step 1: Compute the attention scores through a MLP\n",
    "        \n",
    "        # concatenating the key_vectors and the query_vector\n",
    "        atten_input = torch.cat((key_vectors, query_vector), dim=2)\n",
    "        # atten_input shape: [batch_size, src_seq_length, (encoder_hidd_dim * 2) + decoder_hidd_dim]\n",
    "        \n",
    "        atten_scores = self.atten(atten_input)\n",
    "        # atten_scores shape: [batch_size, src_seq_length, decoder_hidd_dim]\n",
    "\n",
    "        atten_scores = torch.tanh(atten_scores)\n",
    "    \n",
    "        # mapping atten_scores from decoder_hidd_dim to 1\n",
    "        atten_scores = self.v(atten_scores)\n",
    "    \n",
    "        # atten_scores shape: [batch_size, src_seq_length, 1]\n",
    "        atten_scores = atten_scores.squeeze(dim=2)\n",
    "        # atten_scores shape: [batch_size, src_seq_length]\n",
    "        \n",
    "        # masking the atten_scores\n",
    "        atten_scores = atten_scores.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # Step 2: normalizing atten_scores through a softmax to get probs\n",
    "        atten_scores = F.softmax(atten_scores, dim=1)\n",
    "        \n",
    "        # Step 3: computing the new context vector\n",
    "        context_vectors = torch.matmul(key_vectors.permute(0, 2, 1), atten_scores.unsqueeze(2)).squeeze(dim=2)\n",
    "        \n",
    "        # context_vectors shape: [batch_size, encoder_hidd_dim * 2]\n",
    "        \n",
    "        return context_vectors, atten_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Seq2Seq model\"\"\"\n",
    "    def __init__(self, encoder_input_dim, encoder_embed_dim,\n",
    "                 encoder_hidd_dim, decoder_input_dim, \n",
    "                 decoder_embed_dim, decoder_output_dim,\n",
    "                 morph_embedding_matrix=None, src_char_padding_idx=0, \n",
    "                 src_word_padding_idx=0, trg_padding_idx=0,\n",
    "                 trg_sos_idx=2):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_dim=encoder_input_dim,\n",
    "                               embed_dim=encoder_embed_dim,\n",
    "                               hidd_dim=encoder_hidd_dim,\n",
    "                               morph_embedding=morph_embedding_matrix,\n",
    "                               char_padding_idx=src_char_padding_idx,\n",
    "                               word_padding_idx=src_word_padding_idx)\n",
    "        \n",
    "        self.decoder_hidd_dim = encoder_hidd_dim * 2\n",
    "        \n",
    "        self.attention = AdditiveAttention(encoder_hidd_dim=encoder_hidd_dim,\n",
    "                                           decoder_hidd_dim=self.decoder_hidd_dim)\n",
    "        \n",
    "        self.decoder = Decoder(input_dim=decoder_input_dim,\n",
    "                               embed_dim=decoder_embed_dim,\n",
    "                               hidd_dim=self.decoder_hidd_dim,\n",
    "                               output_dim=decoder_input_dim,\n",
    "                               attention=self.attention,\n",
    "                               padding_idx=trg_padding_idx)\n",
    "        \n",
    "        self.src_padding_idx = src_char_padding_idx\n",
    "        self.trg_sos_idx = trg_sos_idx\n",
    "        self.sampling_temperature = 3\n",
    "        \n",
    "    def create_mask(self, src_seqs, src_padding_idx):\n",
    "        mask = (src_seqs != src_padding_idx)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, char_src_seqs, word_src_seqs, src_seqs_lengths, trg_seqs, teacher_forcing_prob=0.3):\n",
    "        # trg_seqs shape: [batch_size, trg_seqs_length]\n",
    "        # reshaping to: [trg_seqs_length, batch_size]\n",
    "        trg_seqs = trg_seqs.permute(1, 0)\n",
    "        trg_seqs_length, batch_size = trg_seqs.shape\n",
    "        \n",
    "        # passing the src to the encoder\n",
    "        encoder_outputs, encoder_hidd = self.encoder(char_src_seqs, word_src_seqs, src_seqs_lengths)\n",
    "        \n",
    "        # creating attention masks\n",
    "        attention_mask = self.create_mask(char_src_seqs, self.src_padding_idx)\n",
    "\n",
    "        predictions = []\n",
    "        decoder_attention_scores = []\n",
    "        \n",
    "        # initializing the trg_seqs to <s> token\n",
    "        y_t = torch.ones(batch_size, dtype=torch.long) * self.trg_sos_idx\n",
    "        \n",
    "        # intializing the context_vectors to zero\n",
    "        context_vectors = torch.zeros(batch_size, self.decoder_hidd_dim)\n",
    "        \n",
    "        # initializing the hidden state of the decoder to the encoder hidden state\n",
    "        decoder_h_t = encoder_hidd\n",
    "        \n",
    "        # moving y_t and context_vectors to the right device\n",
    "        y_t = y_t.to(encoder_hidd.device)\n",
    "        context_vectors = context_vectors.to(encoder_hidd.device)\n",
    "        \n",
    "        for i in range(trg_seqs_length):\n",
    "            teacher_forcing = np.random.random() < teacher_forcing_prob\n",
    "            # if teacher_forcing, use ground truth target tokens\n",
    "            # as an input to the decoder\n",
    "            if teacher_forcing:\n",
    "                y_t = trg_seqs[i]\n",
    "            \n",
    "            # do a single decoder step\n",
    "            prediction, decoder_h_t, atten_scores, context_vectors = self.decoder(y_t, \n",
    "                                                                                  encoder_outputs, \n",
    "                                                                                  decoder_h_t, \n",
    "                                                                                  context_vectors, \n",
    "                                                                                  attention_mask=attention_mask)\n",
    "            \n",
    "#             # updating the context_vectors \n",
    "#             context_vectors, atten_scores = self.attention(key_vectors=encoder_outputs, query_vector=decoder_h_t,\n",
    "#                                                            mask=attention_mask)\n",
    "            # If not teacher force, use the maximum \n",
    "            # prediction as an input to the decoder in \n",
    "            # the next time step\n",
    "            if not teacher_forcing:\n",
    "                # we multiply the predictions with a sampling_temperature\n",
    "                # to make the propablities peakier, so we can be confident about the\n",
    "                # maximum prediction\n",
    "                pred_output_probs = F.softmax(prediction * self.sampling_temperature, dim=1)\n",
    "                y_t = torch.argmax(pred_output_probs, dim=1)\n",
    "\n",
    "            predictions.append(prediction)\n",
    "            decoder_attention_scores.append(atten_scores)\n",
    "        \n",
    "        \n",
    "        predictions = torch.stack(predictions)\n",
    "        # predictions shape: [trg_seq_len, batch_size, output_dim]\n",
    "        predictions = predictions.permute(1, 0, 2)\n",
    "        # predictions shape: [batch_size, trg_seq_len, output_dim]\n",
    "    \n",
    "    \n",
    "        decoder_attention_scores = torch.stack(decoder_attention_scores)\n",
    "        # attention_scores_total shape: [trg_seq_len, batch_size, src_seq_len]\n",
    "        decoder_attention_scores = decoder_attention_scores.permute(1, 0, 2)\n",
    "        # attention_scores_total shape: [batch_size, trg_seq_len, src_seq_len]\n",
    "        \n",
    "        return predictions, decoder_attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, cuda):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(data_dir='/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus',\n",
    "                          vectorizer_path='/home/ba63/gender-bias/models/saved_models/char_level_vectorizer.json',\n",
    "                          morph_features_path='/home/ba63/databases/morph_features.json',\n",
    "                          analyzer_db_path='/home/ba63/databases/calima-msa/calima-msa.0.2.2.utf8.db',\n",
    "                          use_morph_embeddings=True,\n",
    "                          reload_files=False,\n",
    "                          cache_files=True,\n",
    "                          num_epochs=50,\n",
    "                          embedding_dim=32,\n",
    "                          hidd_dim=64,\n",
    "                          learning_rate=5e-4,\n",
    "                          use_cuda=True,\n",
    "                          batch_size=64,\n",
    "                          seed=21,\n",
    "                          model_path='/home/ba63/gender-bias/models/saved_models/char_level_model_small_morph_1e-10.pt'\n",
    "                          )\n",
    "\n",
    "device = torch.device('cuda' if args.use_cuda else 'cpu')\n",
    "set_seed(args.seed, args.use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_featurizer = MorphFeaturizer(args.analyzer_db_path)\n",
    "if args.reload_files:\n",
    "    dataset = MT_Dataset.load_data_and_load_vectorizer(args.data_dir, args.vectorizer_path)\n",
    "    morph_featurizer.load_morph_features(args.morph_features_path)\n",
    "else:\n",
    "    dataset = MT_Dataset.load_data_and_create_vectorizer(args.data_dir)\n",
    "    # TODO: Find a better way to integrate the morph analyzer\n",
    "    src_train_sentences = [t.src for t in dataset.train_examples]\n",
    "    morph_featurizer.featurize_sentences(src_train_sentences)\n",
    "\n",
    "if args.cache_files:\n",
    "    dataset.save_vectorizer(args.vectorizer_path)\n",
    "    morph_featurizer.save_morph_features(args.morph_features_path)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "if args.use_morph_embeddings:\n",
    "    morph_embeddings_matrix = make_morph_embeddings(morph_featurizer, vectorizer.src_vocab_word)\n",
    "    \n",
    "ENCODER_INPUT_DIM = len(vectorizer.src_vocab_char)\n",
    "DECODER_INPUT_DIM = len(vectorizer.trg_vocab_char)\n",
    "DECODER_OUTPUT_DIM = len(vectorizer.trg_vocab_char)\n",
    "SRC_CHAR_PAD_INDEX = vectorizer.src_vocab_char.pad_idx\n",
    "SRC_WORD_PAD_INDEX = vectorizer.src_vocab_word.pad_idx\n",
    "TRG_PAD_INDEX = vectorizer.trg_vocab_char.pad_idx\n",
    "TRG_SOS_INDEX = vectorizer.trg_vocab_char.sos_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (char_embedding_layer): Embedding(71, 32, padding_idx=0)\n",
       "    (morph_embedding_layer): Embedding(13894, 4, padding_idx=0)\n",
       "    (rnn): GRU(36, 64, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (attention): AdditiveAttention(\n",
       "    (atten): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (v): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): AdditiveAttention(\n",
       "      (atten): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (v): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "    (embedding_layer): Embedding(71, 32, padding_idx=0)\n",
       "    (rnn): GRUCell(160, 128)\n",
       "    (classification_layer): Linear(in_features=256, out_features=71, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2Seq(encoder_input_dim=ENCODER_INPUT_DIM,\n",
    "                encoder_embed_dim=args.embedding_dim,\n",
    "                encoder_hidd_dim=args.hidd_dim,\n",
    "                decoder_input_dim=DECODER_INPUT_DIM,\n",
    "                decoder_embed_dim=args.embedding_dim,\n",
    "                decoder_output_dim=DECODER_OUTPUT_DIM,\n",
    "                morph_embedding_matrix=morph_embeddings_matrix,\n",
    "                src_char_padding_idx=SRC_CHAR_PAD_INDEX,\n",
    "                src_word_padding_idx=SRC_WORD_PAD_INDEX,\n",
    "                trg_padding_idx=TRG_PAD_INDEX,\n",
    "                trg_sos_idx=TRG_SOS_INDEX)\n",
    "                    \n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_INDEX)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                 patience=2, factor=0.5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device='cpu', teacher_forcing_prob=0.3):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        src_char = batch['src_char']\n",
    "        src_word = batch['src_word']\n",
    "        trg_x = batch['trg_x']\n",
    "        trg_y = batch['trg_y']\n",
    "        src_lengths = batch['src_lengths']\n",
    "        \n",
    "        \n",
    "        preds, attention_scores = model(src_char, src_word, src_lengths, trg_x,\n",
    "                                        teacher_forcing_prob=teacher_forcing_prob)\n",
    "        \n",
    "        # CrossEntropyLoss accepts matrices always! \n",
    "        # the preds must be of size (N, C) where C is the number \n",
    "        # of classes and N is the number of samples. \n",
    "        # The ground truth must be a Vector of size C!\n",
    "        preds = preds.contiguous().view(-1, preds.shape[-1])\n",
    "        trg_y = trg_y.view(-1)\n",
    "\n",
    "        loss = criterion(preds, trg_y)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device='cpu', teacher_forcing_prob=0):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            src_char = batch['src_char']\n",
    "            src_word = batch['src_word']\n",
    "            trg_x = batch['trg_x']\n",
    "            trg_y = batch['trg_y']\n",
    "            src_lengths = batch['src_lengths']\n",
    "            \n",
    "            # we turn off teacher_forcing during evaluation\n",
    "            preds, attention_scores = model(src_char, src_word, src_lengths, trg_x, \n",
    "                                            teacher_forcing_prob=teacher_forcing_prob)\n",
    "            # CrossEntropyLoss accepts matrices always! \n",
    "            # the preds must be of size (N, C) where C is the number \n",
    "            # of classes and N is the number of samples. \n",
    "            # The ground truth must be a Vector of size C!\n",
    "            preds = preds.contiguous().view(-1, preds.shape[-1])\n",
    "            trg_y = trg_y.view(-1)\n",
    "            \n",
    "            loss = criterion(preds, trg_y)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Epoch: 1\n",
      "\tTrain Loss: 3.2312   |   Dev Loss: 3.0746\n",
      "Epoch: 2\n",
      "\tTrain Loss: 2.9252   |   Dev Loss: 2.8195\n",
      "Epoch: 3\n",
      "\tTrain Loss: 2.4998   |   Dev Loss: 2.3328\n",
      "Epoch: 4\n",
      "\tTrain Loss: 1.9682   |   Dev Loss: 2.0000\n",
      "Epoch: 5\n",
      "\tTrain Loss: 1.5458   |   Dev Loss: 1.6660\n",
      "Epoch: 6\n",
      "\tTrain Loss: 1.2882   |   Dev Loss: 1.4443\n",
      "Epoch: 7\n",
      "\tTrain Loss: 1.0250   |   Dev Loss: 1.0527\n",
      "Epoch: 8\n",
      "\tTrain Loss: 0.8651   |   Dev Loss: 0.9396\n",
      "Epoch: 9\n",
      "\tTrain Loss: 0.7352   |   Dev Loss: 0.8209\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.6508   |   Dev Loss: 0.8397\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.5419   |   Dev Loss: 1.1992\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.5658   |   Dev Loss: 1.4264\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.4104   |   Dev Loss: 0.8325\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.3749   |   Dev Loss: 0.5127\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.3574   |   Dev Loss: 0.6118\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.3405   |   Dev Loss: 0.4653\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.3449   |   Dev Loss: 0.4721\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.3394   |   Dev Loss: 0.4406\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.3037   |   Dev Loss: 0.5499\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.2740   |   Dev Loss: 0.8580\n",
      "Epoch: 21\n",
      "\tTrain Loss: 0.3324   |   Dev Loss: 0.4405\n",
      "Epoch: 22\n",
      "\tTrain Loss: 0.2603   |   Dev Loss: 0.4631\n",
      "Epoch: 23\n",
      "\tTrain Loss: 0.2741   |   Dev Loss: 0.4754\n",
      "Epoch: 24\n",
      "\tTrain Loss: 0.2785   |   Dev Loss: 0.4322\n",
      "Epoch: 25\n",
      "\tTrain Loss: 0.2726   |   Dev Loss: 0.4554\n",
      "Epoch: 26\n",
      "\tTrain Loss: 0.2374   |   Dev Loss: 0.3607\n",
      "Epoch: 27\n",
      "\tTrain Loss: 0.2427   |   Dev Loss: 0.7448\n",
      "Epoch: 28\n",
      "\tTrain Loss: 0.2254   |   Dev Loss: 1.4516\n",
      "Epoch: 29\n",
      "\tTrain Loss: 0.2896   |   Dev Loss: 0.4295\n",
      "Epoch: 30\n",
      "\tTrain Loss: 0.1780   |   Dev Loss: 0.3528\n",
      "Epoch: 31\n",
      "\tTrain Loss: 0.2062   |   Dev Loss: 0.3707\n",
      "Epoch: 32\n",
      "\tTrain Loss: 0.1625   |   Dev Loss: 0.5520\n",
      "Epoch: 33\n",
      "\tTrain Loss: 0.1587   |   Dev Loss: 0.3477\n",
      "Epoch: 34\n",
      "\tTrain Loss: 0.1518   |   Dev Loss: 0.3272\n",
      "Epoch: 35\n",
      "\tTrain Loss: 0.1728   |   Dev Loss: 0.5404\n",
      "Epoch: 36\n",
      "\tTrain Loss: 0.1599   |   Dev Loss: 0.4773\n",
      "Epoch: 37\n",
      "\tTrain Loss: 0.1527   |   Dev Loss: 0.3516\n",
      "Epoch: 38\n",
      "\tTrain Loss: 0.1415   |   Dev Loss: 0.3516\n",
      "Epoch: 39\n",
      "\tTrain Loss: 0.1294   |   Dev Loss: 0.3760\n",
      "Epoch: 40\n",
      "\tTrain Loss: 0.1359   |   Dev Loss: 0.3937\n",
      "Epoch: 41\n",
      "\tTrain Loss: 0.1293   |   Dev Loss: 0.4475\n",
      "Epoch: 42\n",
      "\tTrain Loss: 0.1201   |   Dev Loss: 0.3800\n",
      "Epoch: 43\n",
      "\tTrain Loss: 0.1206   |   Dev Loss: 0.4502\n",
      "Epoch: 44\n",
      "\tTrain Loss: 0.1186   |   Dev Loss: 0.3622\n",
      "Epoch: 45\n",
      "\tTrain Loss: 0.1161   |   Dev Loss: 0.3893\n",
      "Epoch: 46\n",
      "\tTrain Loss: 0.1124   |   Dev Loss: 0.3723\n",
      "Epoch: 47\n",
      "\tTrain Loss: 0.1121   |   Dev Loss: 0.3683\n",
      "Epoch: 48\n",
      "\tTrain Loss: 0.1123   |   Dev Loss: 0.3654\n",
      "Epoch: 49\n",
      "\tTrain Loss: 0.1127   |   Dev Loss: 0.3579\n",
      "Epoch: 50\n",
      "\tTrain Loss: 0.1106   |   Dev Loss: 0.3744\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "collator = Collator(SRC_CHAR_PAD_INDEX, TRG_PAD_INDEX)\n",
    "best_loss = 1e10\n",
    "set_seed(args.seed, args.use_cuda)\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "print(f'Using {device}')\n",
    "for epoch in range(args.num_epochs):\n",
    "#     teacher_forcing_prob = (epoch + 5) / args.num_epochs\n",
    "    teacher_forcing_prob = 0.3\n",
    "    dataset.set_split('train')\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=args.batch_size, collate_fn=collator, drop_last=True)\n",
    "    train_loss = train(model, dataloader, optimizer, criterion, device, teacher_forcing_prob=teacher_forcing_prob)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    dataset.set_split('dev')\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=args.batch_size, collate_fn=collator, drop_last=True)\n",
    "    dev_loss = evaluate(model, dataloader, criterion, device, teacher_forcing_prob=0)\n",
    "    dev_losses.append(dev_loss)\n",
    "    \n",
    "    #save best model\n",
    "    if dev_loss < best_loss:\n",
    "        best_loss = dev_loss\n",
    "        torch.save(model.state_dict(), args.model_path)\n",
    "    \n",
    "    # calling the scheduler\n",
    "    scheduler.step(dev_loss)\n",
    "\n",
    "    print(f'Epoch: {(epoch + 1)}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f}   |   Dev Loss: {dev_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, 1 + args.num_epochs), np.asarray(train_losses), 'b-', color='blue', label='Training')\n",
    "plt.plot(range(1, 1 + args.num_epochs), np.asarray(dev_losses), 'b-', color='orange', label='Evaluation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMT_Batch_Sampler:\n",
    "    def __init__(self, model, src_vocab_char, src_vocab_word, trg_vocab):\n",
    "        self.model = model\n",
    "        self.src_vocab_char = src_vocab_char\n",
    "        self.src_vocab_word = src_vocab_word\n",
    "        self.trg_vocab = trg_vocab\n",
    "    \n",
    "    def update_batch(self, batch):\n",
    "        self.sample_batch = batch\n",
    "        \n",
    "        src_char = batch['src_char']\n",
    "        src_word = batch['src_word']\n",
    "        trg_x = batch['trg_x']\n",
    "        trg_y = batch['trg_y']\n",
    "        src_lengths = batch['src_lengths']\n",
    "\n",
    "        preds, attention_scores = self.model(src_char, src_word, src_lengths, trg_x, teacher_forcing_prob=0)\n",
    "        # preds shape: [batch_size, trg_seq_len, output_dim]\n",
    "        \n",
    "        self.sample_batch['preds'] = preds\n",
    "        self.sample_batch['attention_scores'] = attention_scores\n",
    "        return self.sample_batch\n",
    "    \n",
    "    def get_pred_sentence(self, index):\n",
    "        preds = self.sample_batch['preds']\n",
    "       \n",
    "        max_preds = torch.argmax(preds, dim=2)\n",
    "        # max_preds shape: [batch_size, trg_seq_len]\n",
    "        max_pred_sentence = max_preds[index].cpu().detach().numpy()\n",
    "        return self.get_str_sentence(max_pred_sentence, self.trg_vocab)\n",
    "    \n",
    "    def get_trg_sentence(self, index):\n",
    "        trg_sentence = self.sample_batch['trg_y'][index].cpu().detach().numpy()\n",
    "        return self.get_str_sentence(trg_sentence, self.trg_vocab)\n",
    "    \n",
    "    def get_src_sentence(self, index):\n",
    "        src_sentence = self.sample_batch['src_char'][index].cpu().detach().numpy()\n",
    "        return self.get_str_sentence(src_sentence, self.src_vocab_char)\n",
    "    \n",
    "    def get_str_sentence(self, vectorized_sentence, vocab):\n",
    "        sentence = []\n",
    "        for i in vectorized_sentence:\n",
    "            if i == vocab.sos_idx:\n",
    "                continue\n",
    "            elif i == vocab.eos_idx:\n",
    "                break\n",
    "            else:\n",
    "                sentence.append(vocab.lookup_index(i))\n",
    "        return ''.join(sentence)\n",
    "    \n",
    "    def translate_sentence(self, sentence, max_len=120):\n",
    "\n",
    "        # vectorizing the src sentence on the char level and word level\n",
    "        sentence = re.split(r'(\\s+)', sentence)\n",
    "        vectorized_src_sentence_char = [self.src_vocab_char.sos_idx]\n",
    "        vectorized_src_sentence_word = [self.src_vocab_word.sos_idx]\n",
    "        for word in sentence:\n",
    "            for c in word:\n",
    "                vectorized_src_sentence_char.append(self.src_vocab_char.lookup_token(c))\n",
    "                vectorized_src_sentence_word.append(self.src_vocab_word.lookup_token(word))\n",
    "        \n",
    "        vectorized_src_sentence_word.append(self.src_vocab_word.eos_idx)\n",
    "        vectorized_src_sentence_char.append(self.src_vocab_char.eos_idx)\n",
    "        \n",
    "        # getting sentence length\n",
    "        src_sentence_length = [len(vectorized_src_sentence_char)]\n",
    "        \n",
    "        # converting the lists to tensors\n",
    "        vectorized_src_sentence_char = torch.tensor([vectorized_src_sentence_char], dtype=torch.long)\n",
    "        vectorized_src_sentence_word = torch.tensor([vectorized_src_sentence_word], dtype=torch.long)\n",
    "        src_sentence_length = torch.tensor(src_sentence_length, dtype=torch.long)\n",
    "        \n",
    "        # passing the src sentence to the encoder\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs, encoder_h_t= self.model.encoder(vectorized_src_sentence_char,\n",
    "                                                             vectorized_src_sentence_word,\n",
    "                                                             src_sentence_length)\n",
    "        \n",
    "        # creating attention mask\n",
    "        attention_mask = self.model.create_mask(vectorized_src_sentence_char, self.src_vocab_char.pad_idx)\n",
    "        \n",
    "        # initilizating the first decoder_h_t to encoder_h_t\n",
    "        decoder_h_t = encoder_h_t\n",
    "        \n",
    "        # initializing the context vectors to 0\n",
    "        context_vectors = torch.zeros(1, self.model.decoder.hidd_dim)\n",
    "        \n",
    "        # intializing the trg sequences to the <s> token\n",
    "        trg_seqs = [self.trg_vocab.sos_idx]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(max_len):\n",
    "                y_t = torch.tensor([trg_seqs[-1]], dtype=torch.long)\n",
    "\n",
    "                # do a single decoder step\n",
    "                prediction, decoder_h_t, atten_scores, context_vectors = self.model.decoder(y_t, \n",
    "                                                                                          encoder_outputs, \n",
    "                                                                                          decoder_h_t, \n",
    "                                                                                          context_vectors, \n",
    "                                                                                          attention_mask=attention_mask)\n",
    "\n",
    "                # getting the most probable prediciton\n",
    "                max_pred = torch.argmax(prediction, dim=1).item()\n",
    "\n",
    "                # if we reach </s> token, stop decoding\n",
    "                if max_pred == self.trg_vocab.eos_idx:\n",
    "                    break\n",
    "\n",
    "                trg_seqs.append(max_pred)\n",
    "\n",
    "        str_sentence = self.get_str_sentence(trg_seqs, self.trg_vocab)\n",
    "        return str_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed, args.use_cuda)\n",
    "dataset.set_split('train')\n",
    "model.load_state_dict(torch.load(args.model_path))\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "collator = Collator(SRC_CHAR_PAD_INDEX, TRG_PAD_INDEX)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collator)\n",
    "sampler = NMT_Batch_Sampler(model, vectorizer.src_vocab_char, vectorizer.src_vocab_word, vectorizer.trg_vocab_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/'\\\n",
    "    'edits_annotations/char_level_model_small_morph_new.train_preds', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_inf = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/'\\\n",
    "    'edits_annotations/char_level_model_small_morph_new.train_preds.inf', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/'\\\n",
    "    'edits_annotations/char_level_model_small_morph_new.train_log.inf', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    updated_batch = sampler.update_batch(batch)\n",
    "\n",
    "#     print(updated_batch['trg_x'])\n",
    "#     print(updated_batch['trg_y'])\n",
    "#     print(updated_batch['src'])\n",
    "    src = sampler.get_src_sentence(0)\n",
    "    trg = sampler.get_trg_sentence(0)\n",
    "    pred = sampler.get_pred_sentence(0)\n",
    "    translated = sampler.translate_sentence(src)\n",
    "    \n",
    "#     print(src)\n",
    "#     print(trg)\n",
    "#     print(pred)\n",
    "#     print(translated)\n",
    "#     print(len(translated))\n",
    "    \n",
    "    train_log.write(f'src: ' + src)\n",
    "    train_log.write('\\n')\n",
    "    train_log.write(f'trg: ' + trg)\n",
    "    train_log.write('\\n')\n",
    "    train_log.write(f'pred: ' + pred)\n",
    "    train_log.write('\\n')\n",
    "    train_log.write(f'trans: ' + translated)\n",
    "    train_log.write('\\n\\n')\n",
    "    train_preds.write(pred)\n",
    "    train_preds.write('\\n')\n",
    "    train_preds_inf.write(translated)\n",
    "    train_preds_inf.write('\\n')\n",
    "#     attention_scores = updated_batch['attention_scores'][0].cpu().detach().numpy()\n",
    "#     fig = plt.figure(figsize=(10,10))\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     cax = ax.matshow(attention_scores, cmap='bone')\n",
    "#     fig.colorbar(cax)\n",
    "\n",
    "#     # Set up axes\n",
    "#     ax.set_xticklabels(['','<s>'] + src.split(' ') +\n",
    "#                    ['</s>'], rotation=90)\n",
    "#     ax.set_yticklabels([''] + pred.split(' ') + ['</s>'])\n",
    "\n",
    "#     # Show label at every tick\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#     ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "train_log.close()\n",
    "train_preds.close()\n",
    "train_preds_inf.close()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed, args.use_cuda)\n",
    "dataset.set_split('dev')\n",
    "model.load_state_dict(torch.load(args.model_path))\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "collator = Collator(SRC_CHAR_PAD_INDEX, TRG_PAD_INDEX)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collator)\n",
    "sampler = NMT_Batch_Sampler(model, vectorizer.src_vocab_char, vectorizer.src_vocab_word, vectorizer.trg_vocab_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_preds = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/'\\\n",
    "    'edits_annotations/char_level_model_small_morph_new.dev_preds', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_preds_inf = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/'\\\n",
    "    'edits_annotations/char_level_model_small_morph_new.dev_preds.inf', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_log = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/'\\\n",
    "    'edits_annotations/char_level_model_small_morph_new.dev_log', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    updated_batch = sampler.update_batch(batch)\n",
    "\n",
    "#     print(updated_batch['trg_x'])\n",
    "#     print(updated_batch['trg_y'])\n",
    "#     print(updated_batch['src'])\n",
    "    src = sampler.get_src_sentence(0)\n",
    "    trg = sampler.get_trg_sentence(0)\n",
    "    pred = sampler.get_pred_sentence(0)\n",
    "    translated = sampler.translate_sentence(src)\n",
    "    \n",
    "#     print(translated)\n",
    "#     print(src)\n",
    "#     print(trg)\n",
    "#     print(pred)\n",
    "    \n",
    "    dev_log.write(f'src: ' + src)\n",
    "    dev_log.write('\\n')\n",
    "    dev_log.write(f'trg: ' + trg)\n",
    "    dev_log.write('\\n')\n",
    "    dev_log.write(f'pred: ' + pred)\n",
    "    dev_log.write('\\n')\n",
    "    dev_log.write(f'trans: ' + translated)\n",
    "    dev_log.write('\\n\\n')\n",
    "    dev_preds.write(pred)\n",
    "    dev_preds.write('\\n')\n",
    "    dev_preds_inf.write(translated)\n",
    "    dev_preds_inf.write('\\n')\n",
    "#     attention_scores = updated_batch['attention_scores'][0].cpu().detach().numpy()\n",
    "#     fig = plt.figure(figsize=(10,10))\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     cax = ax.matshow(attention_scores, cmap='bone')\n",
    "#     fig.colorbar(cax)\n",
    "\n",
    "#     # Set up axes\n",
    "#     ax.set_xticklabels(['','<s>'] + src.split(' ') +\n",
    "#                    ['</s>'], rotation=90)\n",
    "#     ax.set_yticklabels([''] + pred.split(' ') + ['</s>'])\n",
    "\n",
    "#     # Show label at every tick\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#     ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "#     break\n",
    "\n",
    "dev_log.close()\n",
    "dev_preds.close()\n",
    "dev_preds_inf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
