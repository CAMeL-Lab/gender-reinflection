{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample:\n",
    "    \"\"\"Simple object to encapsulate each data example\"\"\"\n",
    "    def __init__(self, src, trg, \n",
    "                 src_g, trg_g):    \n",
    "        self.src = src\n",
    "        self.trg = trg\n",
    "        self.src_g = src_g\n",
    "        self.trg_g = trg_g\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_str())\n",
    "    \n",
    "    def to_json_str(self):\n",
    "        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)\n",
    "    \n",
    "    def to_dict(self):\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataset:\n",
    "    \"\"\"Encapsulates the raw examples in InputExample objects\"\"\"\n",
    "    def __init__(self, data_dir):\n",
    "        self.train_examples = self.get_train_examples(data_dir)\n",
    "        self.dev_examples = self.get_dev_examples(data_dir)\n",
    "        self.test_examples = self.get_dev_examples(data_dir)\n",
    "        \n",
    "    def create_examples(self, src_path, trg_path):\n",
    "        \n",
    "        src_txt = self.get_txt_examples(src_path)\n",
    "        src_gender_labels = self.get_labels(src_path + '.label')\n",
    "        trg_txt = self.get_txt_examples(trg_path)\n",
    "        trg_gender_labels = self.get_labels(trg_path + '.label')\n",
    "        \n",
    "        examples = []\n",
    "        \n",
    "        for i in range(len(src_txt)):\n",
    "            src = src_txt[i].strip()\n",
    "            trg = trg_txt[i].strip()\n",
    "            src_g = src_gender_labels[i].strip()\n",
    "            trg_g = trg_gender_labels[i].strip()\n",
    "            input_example = InputExample(src, trg, src_g, trg_g)\n",
    "            examples.append(input_example)\n",
    "        \n",
    "        return examples\n",
    "    \n",
    "    def get_labels(self, data_dir):\n",
    "        with open(data_dir) as f:\n",
    "            return f.readlines()\n",
    "        \n",
    "    def get_txt_examples(self, data_dir):\n",
    "        with open(data_dir, encoding='utf8') as f:\n",
    "            return f.readlines()\n",
    "    \n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Reads the train examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-train.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-train.ar.M'))\n",
    "    \n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Reads the dev examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-dev.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-dev.ar.M'))\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Reads the test examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-test.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-test.ar.M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = RawDataset('/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/').train_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8566"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Base vocabulary class\"\"\"\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = dict()\n",
    "        \n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        return self.idx_to_token[index]\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self.token_to_idx}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "    \n",
    "class SeqVocabulary(Vocabulary):\n",
    "    \"\"\"Sequence vocabulary class\"\"\"\n",
    "    def __init__(self, token_to_idx=None, unk_token='<unk>',\n",
    "                 pad_token='<pad>', sos_token='<s>',\n",
    "                 eos_token='</s>'):\n",
    "        \n",
    "        super(SeqVocabulary, self).__init__(token_to_idx)\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        \n",
    "        self.pad_idx = self.add_token(self.pad_token)\n",
    "        self.unk_idx = self.add_token(self.unk_token)\n",
    "        self.sos_idx = self.add_token(self.sos_token)\n",
    "        self.eos_idx = self.add_token(self.eos_token)\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        contents = super(SeqVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self.unk_token,\n",
    "                         'pad_token': self.pad_token,\n",
    "                         'sos_token': self.sos_token, \n",
    "                         'eos_token': self.eos_token})\n",
    "        return contents\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx.get(token, self.unk_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    \"\"\"Vectorizer Class\"\"\"\n",
    "    def __init__(self, src_vocab, trg_vocab):\n",
    "        \"\"\"src_vocab and trg_vocab are on the char\n",
    "        level\"\"\"\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "    \n",
    "    @classmethod\n",
    "    def create_vectorizer(cls, data_examples):\n",
    "        \"\"\"Class method which builds the vectorizer\n",
    "        vocab\"\"\"\n",
    "        \n",
    "        src_vocab = SeqVocabulary()\n",
    "        trg_vocab = SeqVocabulary()\n",
    "        \n",
    "        for ex in data_examples:\n",
    "            src = ex.src\n",
    "            trg = ex.trg\n",
    "            \n",
    "            for t in src:\n",
    "                src_vocab.add_token(t)\n",
    "                \n",
    "            for t in trg:\n",
    "                trg_vocab.add_token(t)\n",
    "        \n",
    "        return cls(src_vocab, trg_vocab)\n",
    "    \n",
    "    def get_src_indices(self, seq):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - seq (str): The src sequence\n",
    "        \n",
    "        Returns:\n",
    "          - indices (list): <s> + List of tokens to index mapping + </s>\n",
    "        \"\"\"\n",
    "        indices = [self.src_vocab.sos_idx] \n",
    "        indices.extend([self.src_vocab.lookup_token(t) for t in seq])\n",
    "        indices.append(self.src_vocab.eos_idx)\n",
    "        return indices\n",
    "    \n",
    "    def get_trg_indices(self, seq):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - seq (str): The trg sequence\n",
    "        \n",
    "        Returns:\n",
    "          - trg_x_indices (list): <s> + List of tokens to index mapping\n",
    "          - trg_y_indices (list): List of tokens to index mapping + </s>\n",
    "        \"\"\"\n",
    "        indices = [self.trg_vocab.lookup_token(t) for t in seq]\n",
    "        \n",
    "        trg_x_indices = [self.trg_vocab.sos_idx] + indices\n",
    "        trg_y_indices = indices + [self.trg_vocab.eos_idx]\n",
    "        return trg_x_indices, trg_y_indices\n",
    "        \n",
    "    def vectorize(self, src, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - src (str): The src sequence\n",
    "          - src (str): The trg sequence\n",
    "        Returns:\n",
    "          - vectorized_src \n",
    "          - vectorized_trg_x \n",
    "          - vectorized_trg_y\n",
    "        \"\"\"\n",
    "        src = src\n",
    "        trg = trg\n",
    "        \n",
    "        vectorized_src = self.get_src_indices(src)\n",
    "        vectorized_trg_x, vectorized_trg_y = self.get_trg_indices(trg)\n",
    "        \n",
    "        return {'src': torch.tensor(vectorized_src, dtype=torch.long),\n",
    "                'trg_x': torch.tensor(vectorized_trg_x, dtype=torch.long),\n",
    "                'trg_y': torch.tensor(vectorized_trg_y, dtype=torch.long)\n",
    "               }\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'src_vocab': self.src_vocab.to_serializable(),\n",
    "                'trg_vocab': self.trg_vocab.to_serializable()\n",
    "               }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        src_vocab = SeqVocabulary.from_serializable(contents['src_vocab'])\n",
    "        trg_vocab = SeqVocabulary.from_serializable(contents['trg_vocab'])\n",
    "        return cls(src_vocab, trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MT_Dataset(Dataset):\n",
    "    \"\"\"MT Dataset as a PyTorch dataset\"\"\"\n",
    "    def __init__(self, raw_dataset, vectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.train_examples = raw_dataset.train_examples\n",
    "        self.dev_examples = raw_dataset.dev_examples\n",
    "        self.test_examples = raw_dataset.test_examples\n",
    "        self.lookup_split = {'train': self.train_examples,\n",
    "                             'dev': self.dev_examples,\n",
    "                             'test': self.test_examples}\n",
    "        self.set_split('train')\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "    \n",
    "    @classmethod\n",
    "    def load_data_and_create_vectorizer(cls, data_dir):\n",
    "        raw_dataset = RawDataset(data_dir)\n",
    "        # Note: we always create the vectorized based on the train examples\n",
    "        vectorizer = Vectorizer.create_vectorizer(raw_dataset.train_examples)\n",
    "        return cls(raw_dataset, vectorizer)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_data_and_load_vectorizer(cls, data_dir, vec_path):\n",
    "        raw_dataset = RawDataset(data_dir)\n",
    "        vectorizer = cls.load_vectorizer(vec_path)\n",
    "        return cls(raw_dataset, vectorizer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer(vec_path):\n",
    "        with open(vec_path) as f:\n",
    "            return Vectorizer.from_serializable(json.load(f))\n",
    "    \n",
    "    def save_vectorizer(self, vec_path):\n",
    "        with open(vec_path, 'w') as f:\n",
    "            return json.dump(self.vectorizer.to_serializable(), f)\n",
    "        \n",
    "    def set_split(self, split):\n",
    "        self.split = split\n",
    "        self.split_examples = self.lookup_split[self.split]\n",
    "        return self.split_examples\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        example = self.split_examples[index]\n",
    "        src, trg = example.src, example.trg\n",
    "        vectorized = self.vectorizer.vectorize(src, trg)\n",
    "        return vectorized\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.split_examples)\n",
    "    \n",
    "    \n",
    "class Collator:\n",
    "    def __init__(self, src_pad_idx, trg_pad_idx):\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # Sorting the batch by src seqs length in descending order\n",
    "        sorted_batch = sorted(batch, key=lambda x: x['src'].shape[0], reverse=True)\n",
    "        \n",
    "        src_seqs = [x['src'] for x in sorted_batch]\n",
    "        trg_x_seqs = [x['trg_x'] for x in sorted_batch]\n",
    "        trg_y_seqs = [x['trg_y'] for x in sorted_batch]\n",
    "        lengths = [len(seq) for seq in src_seqs]\n",
    "        \n",
    "        padded_src_seqs = pad_sequence(src_seqs, batch_first=True, padding_value=self.src_pad_idx)\n",
    "        padded_trg_x_seqs = pad_sequence(trg_x_seqs, batch_first=True, padding_value=self.trg_pad_idx)\n",
    "        padded_trg_y_seqs = pad_sequence(trg_y_seqs, batch_first=True, padding_value=self.trg_pad_idx)\n",
    "        lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "        \n",
    "        return {'src': padded_src_seqs,\n",
    "                'trg_x': padded_trg_x_seqs,\n",
    "                'trg_y': padded_trg_y_seqs,\n",
    "                'src_lengths': lengths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder bi-GRU\"\"\"\n",
    "    def __init__(self, input_dim, embed_dim,\n",
    "                 hidd_dim, padding_idx=0):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(input_dim, embed_dim, padding_idx=padding_idx)\n",
    "        self.rnn = nn.GRU(embed_dim, hidd_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, src_seqs, src_seqs_lengths):\n",
    "    \n",
    "        embedded_seqs = self.embedding_layer(src_seqs)\n",
    "        # packing the embedded_seqs\n",
    "        packed_embedded_seqs = pack_padded_sequence(embedded_seqs, src_seqs_lengths, batch_first=True)\n",
    "        \n",
    "        output, hidd = self.rnn(packed_embedded_seqs)\n",
    "        # hidd shape: [num_layers * num_dirs, batch_size, hidd_dim]\n",
    "        \n",
    "        # changing hidd shape to: [batch_size, num_layers * num_dirs, hidd_dim]\n",
    "        hidd = hidd.permute(1, 0 ,2)\n",
    "        \n",
    "        # changing hidd shape to: [batch_size, num_layers * num_dirs * hidd_dim]\n",
    "        hidd = hidd.contiguous().view(hidd.shape[0], -1)\n",
    "        \n",
    "        # unpacking the output\n",
    "        output, lengths = pad_packed_sequence(output, batch_first=True)\n",
    "        # output shape: [batch_size, src_seqs_length, num_dirs * hidd_dim]\n",
    "        return output, hidd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder GRU\n",
    "       \n",
    "       Things to note:\n",
    "           - The input to the decoder rnn at each time step is the \n",
    "             concatenation of the embedded token and the context vector\n",
    "           - The context vector will have a size of batch_size, hidd_dim\n",
    "           - Note that the decoder hidd_dim == the encoder hidd_dim * 2\n",
    "           - The prediction layer input is the concatenation of \n",
    "             the context vector and the h_t of the decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, embed_dim,\n",
    "                 hidd_dim, output_dim,\n",
    "                 attention,\n",
    "                 padding_idx=0):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidd_dim = hidd_dim\n",
    "        self.attention = attention\n",
    "        self.embedding_layer = nn.Embedding(input_dim, embed_dim, padding_idx=padding_idx)\n",
    "        # the input to the rnn is the context_vector + embedded token --> embed_dim + hidd_dim\n",
    "        self.rnn = nn.GRUCell((embed_dim + hidd_dim), hidd_dim)\n",
    "        # the input to the classifier is h_t + context_vector --> hidd_dim * 2\n",
    "        self.classification_layer = nn.Linear(hidd_dim * 2, output_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, trg_seqs, encoder_outputs, decoder_h_t, context_vectors, attention_mask):\n",
    "        # trg_seqs shape: [batch_size]\n",
    "        batch_size = trg_seqs.shape[0]\n",
    "\n",
    "        # Step 1: embedding the target seqs\n",
    "        embedded_seqs = self.embedding_layer(trg_seqs)\n",
    "        # embedded_seqs shape: [batch_size, embed_dim]\n",
    "        \n",
    "        # concatenating the embedded trg sequence with the context_vectors\n",
    "        rnn_input = torch.cat((embedded_seqs, context_vectors), dim=1)\n",
    "        # rnn_input shape: [batch_size, embed_dim + hidd_dim]\n",
    "        \n",
    "        # Step 2: feeding the input to the rnn and updating the decoder_h_t\n",
    "        decoder_h_t = self.rnn(rnn_input, decoder_h_t)\n",
    "        # decoder_h_t shape: [batch_size, hidd_dim]\n",
    "        \n",
    "        # Step 3: updating the context vectors through attention\n",
    "        context_vectors, atten_scores = self.attention(key_vectors=encoder_outputs, \n",
    "                                                       query_vector=decoder_h_t,\n",
    "                                                       mask=attention_mask)\n",
    "        \n",
    "        # concatenating decoder_h_t with the context_vectors to create a \n",
    "        # prediction vector\n",
    "        predictions_vector = torch.cat((decoder_h_t, context_vectors), dim=1)\n",
    "        # predictions_vector: [batch_size, hidd_dim * 2]\n",
    "        \n",
    "        # Step 4: feeding the prediction vector to the fc layer\n",
    "        # to a make a prediction\n",
    "        prediction = self.classification_layer(predictions_vector)\n",
    "        # prediction shape: [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, decoder_h_t, atten_scores, context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Decoder(nn.Module):\n",
    "#     \"\"\"Decoder GRU\n",
    "       \n",
    "#        Things to note:\n",
    "#            - The input to the decoder rnn at each time step is the \n",
    "#              concatenation of the embedded token and the context vector\n",
    "#            - The context vector will have a size of batch_size, hidd_dim\n",
    "#            - Note that the decoder hidd_dim == the encoder hidd_dim * 2\n",
    "#            - The prediction layer input is the concatenation of \n",
    "#              the context vector and the h_t of the decoder\n",
    "#     \"\"\"\n",
    "#     def __init__(self, input_dim, embed_dim,\n",
    "#                  hidd_dim, output_dim,\n",
    "#                  padding_idx=0):\n",
    "        \n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.hidd_dim = hidd_dim\n",
    "#         self.embedding_layer = nn.Embedding(input_dim, embed_dim, padding_idx=padding_idx)\n",
    "#         # the input to the rnn is the context_vector + embedded token --> embed_dim + hidd_dim\n",
    "#         self.rnn = nn.GRU((embed_dim + hidd_dim), hidd_dim, batch_first=True)\n",
    "#         # the input to the classifier is h_t + context_vector --> hidd_dim * 2\n",
    "#         self.classification_layer = nn.Linear(hidd_dim * 2, output_dim)\n",
    "        \n",
    "    \n",
    "#     def forward(self, trg_seqs, encoder_outputs, decoder_h_t, context_vectors):\n",
    "#         # trg_seqs shape: [batch_size]\n",
    "#         batch_size = trg_seqs.shape[0]\n",
    "\n",
    "#         # Step 1: embedding the target seqs\n",
    "#         embedded_seqs = self.embedding_layer(trg_seqs)\n",
    "#         # embedded_seqs shape: [batch_size, embed_dim]\n",
    "        \n",
    "#         # concatenating the embedded trg sequence with the context_vectors\n",
    "#         rnn_input = torch.cat((embedded_seqs, context_vectors), dim=1)\n",
    "#         # rnn_input shape: [batch_size, embed_dim + hidd_dim]\n",
    "        \n",
    "#         # the GRU expects an input of dimension [batch_size, sequence_len, embed_dim + hidd_dim]\n",
    "#         # since we have a single token, the sequence_len will be 1\n",
    "#         rnn_input = rnn_input.unsqueeze(1)\n",
    "#         # rnn_input shape: [batch_size, 1, embed_dim + hidd_dim]\n",
    "        \n",
    "#         # the GRU also expects a hidden state of dimension \n",
    "#         # [num_layers * num_dirs, batch_size, hidd_dim]\n",
    "#         # since we have a single layer and a forward GRU,\n",
    "#         # num_layers * num_dirs will be 1\n",
    "#         decoder_h_t = decoder_h_t.unsqueeze(0)\n",
    "#         # decoder_h_t shape: [1, batch_size, hidd_dim]\n",
    "        \n",
    "#         # Step 2: feeding the input to the rnn and updating the decoder_h_t\n",
    "#         output, decoder_h_t = self.rnn(rnn_input, decoder_h_t)\n",
    "#         # output shape: [batch, 1, hidd_dim]\n",
    "#         # decoder_h_t shape: [1, batch_size, hidd_dim]\n",
    "        \n",
    "#         decoder_h_t = decoder_h_t.squeeze(0)\n",
    "#         # decoder_h_t shape: [batch_size, hidd_dim]\n",
    "        \n",
    "#         # concatenating decoder_h_t with the context_vectors to create a \n",
    "#         # prediction vector\n",
    "#         predictions_vector = torch.cat((decoder_h_t, context_vectors), dim=1)\n",
    "#         # predictions_vector: [batch_size, hidd_dim * 2]\n",
    "        \n",
    "#         # Step 3: feeding the prediction vector to the fc layer\n",
    "#         # to a make a prediction\n",
    "#         prediction = self.classification_layer(predictions_vector)\n",
    "#         # prediction shape: [batch_size, output_dim]\n",
    "        \n",
    "#         return prediction, decoder_h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"Attention mechanism as a MLP \n",
    "    as used by Bahdanau et. al 2015\"\"\"\n",
    "\n",
    "    def __init__(self, encoder_hidd_dim, decoder_hidd_dim):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.atten = nn.Linear((encoder_hidd_dim * 2) + decoder_hidd_dim, decoder_hidd_dim)\n",
    "        self.v = nn.Linear(decoder_hidd_dim, 1)\n",
    "        \n",
    "    def forward(self, key_vectors, query_vector, mask):\n",
    "        \"\"\"key_vectors: encoder hidden states.\n",
    "           query_vector: decoder hidden state at time t\n",
    "           mask: the mask vector of zeros and ones\n",
    "        \"\"\"\n",
    "        \n",
    "        #key_vectors shape: [batch_size, src_seq_length, encoder_hidd_dim * 2]\n",
    "        #query_vector shape: [batch_size, decoder_hidd_dim]\n",
    "        #Note: encoder_hidd_dim * 2 == decoder_hidd_dim\n",
    "        \n",
    "        batch_size, src_seq_length, encoder_hidd_dim = key_vectors.shape\n",
    "        \n",
    "        #changing the shape of query_vector to [batch_size, src_seq_length, decoder_hidd_dim]\n",
    "        #we will repeat the query_vector src_seq_length times at dim 1\n",
    "        query_vector = query_vector.unsqueeze(1).repeat(1, src_seq_length, 1)\n",
    "        \n",
    "        # Step 1: Compute the attention scores through a MLP\n",
    "        \n",
    "        # concatenating the key_vectors and the query_vector\n",
    "        atten_input = torch.cat((key_vectors, query_vector), dim=2)\n",
    "        # atten_input shape: [batch_size, src_seq_length, (encoder_hidd_dim * 2) + decoder_hidd_dim]\n",
    "        \n",
    "        atten_scores = self.atten(atten_input)\n",
    "        # atten_scores shape: [batch_size, src_seq_length, decoder_hidd_dim]\n",
    "\n",
    "        atten_scores = torch.tanh(atten_scores)\n",
    "    \n",
    "        # mapping atten_scores from decoder_hidd_dim to 1\n",
    "        atten_scores = self.v(atten_scores)\n",
    "    \n",
    "        # atten_scores shape: [batch_size, src_seq_length, 1]\n",
    "        atten_scores = atten_scores.squeeze(dim=2)\n",
    "        # atten_scores shape: [batch_size, src_seq_length]\n",
    "        \n",
    "        # masking the atten_scores\n",
    "        atten_scores = atten_scores.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # Step 2: normalizing atten_scores through a softmax to get probs\n",
    "        atten_scores = F.softmax(atten_scores, dim=1)\n",
    "        \n",
    "        # Step 3: computing the new context vector\n",
    "        context_vectors = torch.matmul(key_vectors.permute(0, 2, 1), atten_scores.unsqueeze(2)).squeeze(dim=2)\n",
    "        \n",
    "        # context_vectors shape: [batch_size, encoder_hidd_dim * 2]\n",
    "        \n",
    "        return context_vectors, atten_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Seq2Seq model\"\"\"\n",
    "    def __init__(self, encoder_input_dim, encoder_embed_dim,\n",
    "                 encoder_hidd_dim, decoder_input_dim, \n",
    "                 decoder_embed_dim, decoder_output_dim, \n",
    "                 src_padding_idx=0, trg_padding_idx=0, trg_sos_idx=2):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(input_dim=encoder_input_dim,\n",
    "                               embed_dim=encoder_embed_dim,\n",
    "                               hidd_dim=encoder_hidd_dim,\n",
    "                               padding_idx=src_padding_idx)\n",
    "        \n",
    "        self.decoder_hidd_dim = encoder_hidd_dim * 2\n",
    "        \n",
    "        self.attention = AdditiveAttention(encoder_hidd_dim=encoder_hidd_dim,\n",
    "                                           decoder_hidd_dim=self.decoder_hidd_dim)\n",
    "        \n",
    "        self.decoder = Decoder(input_dim=decoder_input_dim,\n",
    "                               embed_dim=decoder_embed_dim,\n",
    "                               hidd_dim=self.decoder_hidd_dim,\n",
    "                               output_dim=decoder_input_dim,\n",
    "                               attention=self.attention,\n",
    "                               padding_idx=trg_padding_idx)\n",
    "        \n",
    "        self.src_padding_idx = src_padding_idx\n",
    "        self.trg_sos_idx = trg_sos_idx\n",
    "        self.sampling_temperature = 3\n",
    "        \n",
    "    def create_mask(self, src_seqs, src_padding_idx):\n",
    "        mask = (src_seqs != src_padding_idx)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src_seqs, src_seqs_lengths, trg_seqs, teacher_forcing_prob=0.3):\n",
    "        # trg_seqs shape: [batch_size, trg_seqs_length]\n",
    "        # reshaping to: [trg_seqs_length, batch_size]\n",
    "        trg_seqs = trg_seqs.permute(1, 0)\n",
    "        trg_seqs_length, batch_size = trg_seqs.shape\n",
    "        \n",
    "        # passing the src to the encoder\n",
    "        encoder_outputs, encoder_hidd = self.encoder(src_seqs, src_seqs_lengths)\n",
    "        \n",
    "        # creating attention masks\n",
    "        attention_mask = self.create_mask(src_seqs, self.src_padding_idx)\n",
    "\n",
    "        predictions = []\n",
    "        decoder_attention_scores = []\n",
    "        \n",
    "        # initializing the trg_seqs to <s> token\n",
    "        y_t = torch.ones(batch_size, dtype=torch.long) * self.trg_sos_idx\n",
    "        \n",
    "        # intializing the context_vectors to zero\n",
    "        context_vectors = torch.zeros(batch_size, self.decoder_hidd_dim)\n",
    "        \n",
    "        # initializing the hidden state of the decoder to the encoder hidden state\n",
    "        decoder_h_t = encoder_hidd\n",
    "        \n",
    "        # moving y_t and context_vectors to the right device\n",
    "        y_t = y_t.to(encoder_hidd.device)\n",
    "        context_vectors = context_vectors.to(encoder_hidd.device)\n",
    "        \n",
    "        for i in range(trg_seqs_length):\n",
    "            teacher_forcing = np.random.random() < teacher_forcing_prob\n",
    "            # if teacher_forcing, use ground truth target tokens\n",
    "            # as an input to the decoder\n",
    "            if teacher_forcing:\n",
    "                y_t = trg_seqs[i]\n",
    "            \n",
    "            # do a single decoder step\n",
    "            prediction, decoder_h_t, atten_scores, context_vectors = self.decoder(y_t, \n",
    "                                                                                  encoder_outputs, \n",
    "                                                                                  decoder_h_t, \n",
    "                                                                                  context_vectors, \n",
    "                                                                                  attention_mask=attention_mask)\n",
    "            \n",
    "#             # updating the context_vectors \n",
    "#             context_vectors, atten_scores = self.attention(key_vectors=encoder_outputs, query_vector=decoder_h_t,\n",
    "#                                                            mask=attention_mask)\n",
    "            # If not teacher force, use the maximum \n",
    "            # prediction as an input to the decoder in \n",
    "            # the next time step\n",
    "            if not teacher_forcing:\n",
    "                # we multiply the predictions with a sampling_temperature\n",
    "                # to make the propablities peakier, so we can be confident about the\n",
    "                # maximum prediction\n",
    "                pred_output_probs = F.softmax(prediction * self.sampling_temperature, dim=1)\n",
    "                y_t = torch.argmax(pred_output_probs, dim=1)\n",
    "\n",
    "            predictions.append(prediction)\n",
    "            decoder_attention_scores.append(atten_scores)\n",
    "        \n",
    "        \n",
    "        predictions = torch.stack(predictions)\n",
    "        # predictions shape: [trg_seq_len, batch_size, output_dim]\n",
    "        predictions = predictions.permute(1, 0, 2)\n",
    "        # predictions shape: [batch_size, trg_seq_len, output_dim]\n",
    "    \n",
    "    \n",
    "        decoder_attention_scores = torch.stack(decoder_attention_scores)\n",
    "        # attention_scores_total shape: [trg_seq_len, batch_size, src_seq_len]\n",
    "        decoder_attention_scores = decoder_attention_scores.permute(1, 0, 2)\n",
    "        # attention_scores_total shape: [batch_size, trg_seq_len, src_seq_len]\n",
    "        \n",
    "        return predictions, decoder_attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, cuda):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(data_dir='/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus',\n",
    "                          vectorizer_path='/home/ba63/gender-bias/models/saved_models/char_level_vectorizer.json',\n",
    "                          reload_files=False,\n",
    "                          cache_files=False,\n",
    "                          num_epochs=50,\n",
    "                          embedding_dim=32,\n",
    "                          hidd_dim=64,\n",
    "                          learning_rate=5e-4,\n",
    "                          use_cuda=True,\n",
    "                          batch_size=64,\n",
    "                          seed=21,\n",
    "                          model_path='/home/ba63/gender-bias/models/saved_models/char_level_model_small.pt'\n",
    "                          )\n",
    "\n",
    "device = torch.device('cuda' if args.use_cuda else 'cpu')\n",
    "set_seed(args.seed, args.use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.reload_files:\n",
    "    dataset = MT_Dataset.load_data_and_load_vectorizer(args.data_dir, args.vectorizer_path)\n",
    "else:\n",
    "    dataset = MT_Dataset.load_data_and_create_vectorizer(args.data_dir)\n",
    "\n",
    "if args.cache_files:\n",
    "    dataset.save_vectorizer(args.vectorizer_path)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "ENCODER_INPUT_DIM = len(vectorizer.src_vocab)\n",
    "DECODER_INPUT_DIM = len(vectorizer.trg_vocab)\n",
    "DECODER_OUTPUT_DIM = len(vectorizer.trg_vocab)\n",
    "SRC_PAD_INDEX = vectorizer.src_vocab.pad_idx\n",
    "TRG_PAD_INDEX = vectorizer.trg_vocab.pad_idx\n",
    "TRG_SOS_INDEX = vectorizer.trg_vocab.sos_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding_layer): Embedding(71, 32, padding_idx=0)\n",
       "    (rnn): GRU(32, 64, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (attention): AdditiveAttention(\n",
       "    (atten): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (v): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): AdditiveAttention(\n",
       "      (atten): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (v): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "    (embedding_layer): Embedding(71, 32, padding_idx=0)\n",
       "    (rnn): GRUCell(160, 128)\n",
       "    (classification_layer): Linear(in_features=256, out_features=71, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2Seq(encoder_input_dim=ENCODER_INPUT_DIM,\n",
    "                encoder_embed_dim=args.embedding_dim,\n",
    "                encoder_hidd_dim=args.hidd_dim,\n",
    "                decoder_input_dim=DECODER_INPUT_DIM,\n",
    "                decoder_embed_dim=args.embedding_dim,\n",
    "                decoder_output_dim=DECODER_OUTPUT_DIM,\n",
    "                src_padding_idx=SRC_PAD_INDEX,\n",
    "                trg_padding_idx=TRG_PAD_INDEX,\n",
    "                trg_sos_idx=TRG_SOS_INDEX)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_INDEX)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                 patience=2, factor=0.5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device='cpu', teacher_forcing_prob=0.3):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        src = batch['src']\n",
    "        trg_x = batch['trg_x']\n",
    "        trg_y = batch['trg_y']\n",
    "        src_lengths = batch['src_lengths']\n",
    "\n",
    "        preds, attention_scores = model(src, src_lengths, trg_x, teacher_forcing_prob=teacher_forcing_prob)\n",
    "        \n",
    "        # CrossEntropyLoss accepts matrices always! \n",
    "        # the preds must be of size (N, C) where C is the number \n",
    "        # of classes and N is the number of samples. \n",
    "        # The ground truth must be a Vector of size C!\n",
    "        preds = preds.contiguous().view(-1, preds.shape[-1])\n",
    "        trg_y = trg_y.view(-1)\n",
    "\n",
    "        loss = criterion(preds, trg_y)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device='cpu', teacher_forcing_prob=0):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            src = batch['src']\n",
    "            trg_x = batch['trg_x']\n",
    "            trg_y = batch['trg_y']\n",
    "            src_lengths = batch['src_lengths']\n",
    "            \n",
    "            # we turn off teacher_forcing during evaluation\n",
    "            preds, attention_scores = model(src, src_lengths, trg_x, teacher_forcing_prob=teacher_forcing_prob)\n",
    "            # CrossEntropyLoss accepts matrices always! \n",
    "            # the preds must be of size (N, C) where C is the number \n",
    "            # of classes and N is the number of samples. \n",
    "            # The ground truth must be a Vector of size C!\n",
    "            preds = preds.contiguous().view(-1, preds.shape[-1])\n",
    "            trg_y = trg_y.view(-1)\n",
    "            \n",
    "            loss = criterion(preds, trg_y)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Epoch: 1\n",
      "\tTrain Loss: 3.2390   |   Dev Loss: 3.0741\n",
      "Epoch: 2\n",
      "\tTrain Loss: 2.9307   |   Dev Loss: 2.8376\n",
      "Epoch: 3\n",
      "\tTrain Loss: 2.5539   |   Dev Loss: 2.3341\n",
      "Epoch: 4\n",
      "\tTrain Loss: 2.0333   |   Dev Loss: 1.9834\n",
      "Epoch: 5\n",
      "\tTrain Loss: 1.6280   |   Dev Loss: 1.5242\n",
      "Epoch: 6\n",
      "\tTrain Loss: 1.3296   |   Dev Loss: 1.2734\n",
      "Epoch: 7\n",
      "\tTrain Loss: 1.1222   |   Dev Loss: 1.2329\n",
      "Epoch: 8\n",
      "\tTrain Loss: 0.9375   |   Dev Loss: 1.0303\n",
      "Epoch: 9\n",
      "\tTrain Loss: 0.8201   |   Dev Loss: 0.9730\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.7143   |   Dev Loss: 0.8144\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.6164   |   Dev Loss: 0.8157\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.5417   |   Dev Loss: 0.7214\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.5120   |   Dev Loss: 0.6907\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.4274   |   Dev Loss: 0.6471\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.3993   |   Dev Loss: 0.6370\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.4010   |   Dev Loss: 0.4761\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.3591   |   Dev Loss: 0.5433\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.3067   |   Dev Loss: 0.6691\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.3270   |   Dev Loss: 0.4290\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.3006   |   Dev Loss: 0.7042\n",
      "Epoch: 21\n",
      "\tTrain Loss: 0.2619   |   Dev Loss: 0.3569\n",
      "Epoch: 22\n",
      "\tTrain Loss: 0.2599   |   Dev Loss: 0.6252\n",
      "Epoch: 23\n",
      "\tTrain Loss: 0.3201   |   Dev Loss: 0.6594\n",
      "Epoch: 24\n",
      "\tTrain Loss: 0.2757   |   Dev Loss: 0.6770\n",
      "Epoch: 25\n",
      "\tTrain Loss: 0.1764   |   Dev Loss: 0.3769\n",
      "Epoch: 26\n",
      "\tTrain Loss: 0.1737   |   Dev Loss: 0.4600\n",
      "Epoch: 27\n",
      "\tTrain Loss: 0.1618   |   Dev Loss: 0.5116\n",
      "Epoch: 28\n",
      "\tTrain Loss: 0.1408   |   Dev Loss: 0.4053\n",
      "Epoch: 29\n",
      "\tTrain Loss: 0.1392   |   Dev Loss: 0.4055\n",
      "Epoch: 30\n",
      "\tTrain Loss: 0.1347   |   Dev Loss: 0.4400\n",
      "Epoch: 31\n",
      "\tTrain Loss: 0.1233   |   Dev Loss: 0.4354\n",
      "Epoch: 32\n",
      "\tTrain Loss: 0.1244   |   Dev Loss: 0.5018\n",
      "Epoch: 33\n",
      "\tTrain Loss: 0.1309   |   Dev Loss: 0.3452\n",
      "Epoch: 34\n",
      "\tTrain Loss: 0.1187   |   Dev Loss: 0.3869\n",
      "Epoch: 35\n",
      "\tTrain Loss: 0.1171   |   Dev Loss: 0.3949\n",
      "Epoch: 36\n",
      "\tTrain Loss: 0.1269   |   Dev Loss: 0.3297\n",
      "Epoch: 37\n",
      "\tTrain Loss: 0.1285   |   Dev Loss: 0.4143\n",
      "Epoch: 38\n",
      "\tTrain Loss: 0.1144   |   Dev Loss: 0.3569\n",
      "Epoch: 39\n",
      "\tTrain Loss: 0.1093   |   Dev Loss: 0.4017\n",
      "Epoch: 40\n",
      "\tTrain Loss: 0.1079   |   Dev Loss: 0.4279\n",
      "Epoch: 41\n",
      "\tTrain Loss: 0.1087   |   Dev Loss: 0.3904\n",
      "Epoch: 42\n",
      "\tTrain Loss: 0.1039   |   Dev Loss: 0.3668\n",
      "Epoch: 43\n",
      "\tTrain Loss: 0.1010   |   Dev Loss: 0.3754\n",
      "Epoch: 44\n",
      "\tTrain Loss: 0.0973   |   Dev Loss: 0.3244\n",
      "Epoch: 45\n",
      "\tTrain Loss: 0.0990   |   Dev Loss: 0.4049\n",
      "Epoch: 46\n",
      "\tTrain Loss: 0.0946   |   Dev Loss: 0.4921\n",
      "Epoch: 47\n",
      "\tTrain Loss: 0.0986   |   Dev Loss: 0.3999\n",
      "Epoch: 48\n",
      "\tTrain Loss: 0.0979   |   Dev Loss: 0.3595\n",
      "Epoch: 49\n",
      "\tTrain Loss: 0.0945   |   Dev Loss: 0.3678\n",
      "Epoch: 50\n",
      "\tTrain Loss: 0.0964   |   Dev Loss: 0.3653\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "collator = Collator(SRC_PAD_INDEX, TRG_PAD_INDEX)\n",
    "best_loss = 1e10\n",
    "set_seed(args.seed, args.use_cuda)\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "print(f'Using {device}')\n",
    "for epoch in range(args.num_epochs):\n",
    "#     teacher_forcing_prob = (epoch + 5) / args.num_epochs\n",
    "    teacher_forcing_prob = 0.3\n",
    "    dataset.set_split('train')\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=args.batch_size, collate_fn=collator, drop_last=True)\n",
    "    train_loss = train(model, dataloader, optimizer, criterion, device, teacher_forcing_prob=teacher_forcing_prob)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    dataset.set_split('dev')\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=args.batch_size, collate_fn=collator, drop_last=True)\n",
    "    dev_loss = evaluate(model, dataloader, criterion, device, teacher_forcing_prob=0)\n",
    "    dev_losses.append(dev_loss)\n",
    "    \n",
    "    #save best model\n",
    "    if dev_loss < best_loss:\n",
    "        best_loss = dev_loss\n",
    "        torch.save(model.state_dict(), args.model_path)\n",
    "    \n",
    "    # calling the scheduler\n",
    "    scheduler.step(dev_loss)\n",
    "\n",
    "    print(f'Epoch: {(epoch + 1)}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f}   |   Dev Loss: {dev_loss:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, 1 + args.num_epochs), np.asarray(train_losses), 'b-', color='blue', label='Training')\n",
    "plt.plot(range(1, 1 + args.num_epochs), np.asarray(dev_losses), 'b-', color='orange', label='Evaluation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMT_Batch_Sampler:\n",
    "    def __init__(self, model, src_vocab, trg_vocab):\n",
    "        self.model = model\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "    \n",
    "    def update_batch(self, batch):\n",
    "        self.sample_batch = batch\n",
    "        \n",
    "        src = batch['src']\n",
    "        trg_x = batch['trg_x']\n",
    "        trg_y = batch['trg_y']\n",
    "        src_lengths = batch['src_lengths']\n",
    "        preds, attention_scores = self.model(src, src_lengths, trg_x, teacher_forcing_prob=0)\n",
    "        # preds shape: [batch_size, trg_seq_len, output_dim]\n",
    "        \n",
    "        self.sample_batch['preds'] = preds\n",
    "        self.sample_batch['attention_scores'] = attention_scores\n",
    "        return self.sample_batch\n",
    "    \n",
    "    def get_pred_sentence(self, index):\n",
    "        preds = self.sample_batch['preds']\n",
    "       \n",
    "        max_preds = torch.argmax(preds, dim=2)\n",
    "        # max_preds shape: [batch_size, trg_seq_len]\n",
    "        max_pred_sentence = max_preds[index].cpu().detach().numpy()\n",
    "        return self.get_str_sentence(max_pred_sentence, self.trg_vocab)\n",
    "    \n",
    "    def get_trg_sentence(self, index):\n",
    "        trg_sentence = self.sample_batch['trg_y'][index].cpu().detach().numpy()\n",
    "        return self.get_str_sentence(trg_sentence, self.trg_vocab)\n",
    "    \n",
    "    def get_src_sentence(self, index):\n",
    "        src_sentence = self.sample_batch['src'][index].cpu().detach().numpy()\n",
    "        return self.get_str_sentence(src_sentence, self.src_vocab)\n",
    "    \n",
    "    def get_str_sentence(self, vectorized_sentence, vocab):\n",
    "        sentence = []\n",
    "        for i in vectorized_sentence:\n",
    "            if i == vocab.sos_idx:\n",
    "                continue\n",
    "            elif i == vocab.eos_idx:\n",
    "                break\n",
    "            else:\n",
    "                sentence.append(vocab.lookup_index(i))\n",
    "        return ''.join(sentence)\n",
    "    \n",
    "    def translate_sentence(self, sentence, max_len=120):\n",
    "        # vectorizing the src sentence\n",
    "        vectorized_src_sentence = [self.src_vocab.sos_idx]\n",
    "        vectorized_src_sentence.extend([self.src_vocab.lookup_token(t) for t in sentence])\n",
    "        vectorized_src_sentence.append(self.src_vocab.eos_idx)\n",
    "        \n",
    "        # getting sentence length\n",
    "        src_sentence_length = [len(vectorized_src_sentence)]\n",
    "        \n",
    "        # converting the lists to tensors\n",
    "        vectorized_src_sentence = torch.tensor([vectorized_src_sentence], dtype=torch.long)\n",
    "        src_sentence_length = torch.tensor(src_sentence_length, dtype=torch.long)\n",
    "        \n",
    "        # passing the src sentence to the encoder\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs, encoder_h_t= self.model.encoder(vectorized_src_sentence, src_sentence_length)\n",
    "        \n",
    "        # creating attention mask\n",
    "        attention_mask = self.model.create_mask(vectorized_src_sentence, self.src_vocab.pad_idx)\n",
    "        \n",
    "        # initilizating the first decoder_h_t to encoder_h_t\n",
    "        decoder_h_t = encoder_h_t\n",
    "        \n",
    "        # initializing the context vectors to 0\n",
    "        context_vectors = torch.zeros(1, self.model.decoder.hidd_dim)\n",
    "        \n",
    "        # intializing the trg sequences to the <s> token\n",
    "        trg_seqs = [self.trg_vocab.sos_idx]\n",
    "        \n",
    "#         print(trg_seq)\n",
    "        with torch.no_grad():\n",
    "            for i in range(max_len):\n",
    "                y_t = torch.tensor([trg_seqs[-1]], dtype=torch.long)\n",
    "\n",
    "                # do a single decoder step\n",
    "                prediction, decoder_h_t, atten_scores, context_vectors = self.model.decoder(y_t, \n",
    "                                                                                          encoder_outputs, \n",
    "                                                                                          decoder_h_t, \n",
    "                                                                                          context_vectors, \n",
    "                                                                                          attention_mask=attention_mask)\n",
    "\n",
    "                # getting the most probable prediciton\n",
    "                max_pred = torch.argmax(prediction, dim=1).item()\n",
    "\n",
    "                # if we reach </s> token, stop decoding\n",
    "                if max_pred == self.trg_vocab.eos_idx:\n",
    "                    break\n",
    "\n",
    "                trg_seqs.append(max_pred)\n",
    "\n",
    "        str_sentence = self.get_str_sentence(trg_seqs, self.trg_vocab)\n",
    "        return str_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed, args.use_cuda)\n",
    "dataset.set_split('train')\n",
    "model.load_state_dict(torch.load(args.model_path))\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "collator = Collator(SRC_PAD_INDEX, TRG_PAD_INDEX)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collator)\n",
    "sampler = NMT_Batch_Sampler(model, vectorizer.src_vocab, vectorizer.trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/'\\\n",
    "    'edits_annotations/char_level_model_small.train_preds', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_inf = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/'\\\n",
    "    'edits_annotations/char_level_model_small.train_preds.inf', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/'\\\n",
    "    'edits_annotations/char_level_model_small.train_log.inf', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    updated_batch = sampler.update_batch(batch)\n",
    "\n",
    "#     print(updated_batch['trg_x'])\n",
    "#     print(updated_batch['trg_y'])\n",
    "#     print(updated_batch['src'])\n",
    "    src = sampler.get_src_sentence(0)\n",
    "    trg = sampler.get_trg_sentence(0)\n",
    "    pred = sampler.get_pred_sentence(0)\n",
    "    translated = sampler.translate_sentence(src)\n",
    "    \n",
    "#     print(src)\n",
    "#     print(trg)\n",
    "#     print(pred)\n",
    "#     print(translated)\n",
    "#     print(len(translated))\n",
    "    \n",
    "    train_log.write(f'src: ' + src)\n",
    "    train_log.write('\\n')\n",
    "    train_log.write(f'trg: ' + trg)\n",
    "    train_log.write('\\n')\n",
    "    train_log.write(f'pred: ' + pred)\n",
    "    train_log.write('\\n')\n",
    "    train_log.write(f'trans: ' + translated)\n",
    "    train_log.write('\\n\\n')\n",
    "    train_preds.write(pred)\n",
    "    train_preds.write('\\n')\n",
    "    train_preds_inf.write(translated)\n",
    "    train_preds_inf.write('\\n')\n",
    "#     attention_scores = updated_batch['attention_scores'][0].cpu().detach().numpy()\n",
    "#     fig = plt.figure(figsize=(10,10))\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     cax = ax.matshow(attention_scores, cmap='bone')\n",
    "#     fig.colorbar(cax)\n",
    "\n",
    "#     # Set up axes\n",
    "#     ax.set_xticklabels(['','<s>'] + src.split(' ') +\n",
    "#                    ['</s>'], rotation=90)\n",
    "#     ax.set_yticklabels([''] + pred.split(' ') + ['</s>'])\n",
    "\n",
    "#     # Show label at every tick\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#     ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "train_log.close()\n",
    "train_preds.close()\n",
    "train_preds_inf.close()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed, args.use_cuda)\n",
    "dataset.set_split('dev')\n",
    "model.load_state_dict(torch.load(args.model_path))\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "collator = Collator(SRC_PAD_INDEX, TRG_PAD_INDEX)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collator)\n",
    "sampler = NMT_Batch_Sampler(model, vectorizer.src_vocab, vectorizer.trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_preds = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/'\\\n",
    "    'edits_annotations/char_level_model_small.dev_preds', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_preds_inf = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/'\\\n",
    "    'edits_annotations/char_level_model_small.dev_preds.inf', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_log = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/'\\\n",
    "    'edits_annotations/char_level_model_small.dev_log.inf', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    updated_batch = sampler.update_batch(batch)\n",
    "\n",
    "#     print(updated_batch['trg_x'])\n",
    "#     print(updated_batch['trg_y'])\n",
    "#     print(updated_batch['src'])\n",
    "    src = sampler.get_src_sentence(0)\n",
    "    trg = sampler.get_trg_sentence(0)\n",
    "    pred = sampler.get_pred_sentence(0)\n",
    "    translated = sampler.translate_sentence(src)\n",
    "#     print(src)\n",
    "#     print(trg)\n",
    "#     print(pred)\n",
    "    \n",
    "    dev_log.write(f'src: ' + src)\n",
    "    dev_log.write('\\n')\n",
    "    dev_log.write(f'trg: ' + trg)\n",
    "    dev_log.write('\\n')\n",
    "    dev_log.write(f'pred: ' + pred)\n",
    "    dev_log.write('\\n')\n",
    "    dev_log.write(f'trans: ' + translated)\n",
    "    dev_log.write('\\n\\n')\n",
    "    dev_preds.write(pred)\n",
    "    dev_preds.write('\\n')\n",
    "    dev_preds_inf.write(translated)\n",
    "    dev_preds_inf.write('\\n')\n",
    "#     attention_scores = updated_batch['attention_scores'][0].cpu().detach().numpy()\n",
    "#     fig = plt.figure(figsize=(10,10))\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     cax = ax.matshow(attention_scores, cmap='bone')\n",
    "#     fig.colorbar(cax)\n",
    "\n",
    "#     # Set up axes\n",
    "#     ax.set_xticklabels(['','<s>'] + src.split(' ') +\n",
    "#                    ['</s>'], rotation=90)\n",
    "#     ax.set_yticklabels([''] + pred.split(' ') + ['</s>'])\n",
    "\n",
    "#     # Show label at every tick\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#     ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "#     break\n",
    "\n",
    "dev_log.close()\n",
    "dev_preds.close()\n",
    "dev_preds_inf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
