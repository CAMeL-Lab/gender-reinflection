{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample:\n",
    "    \"\"\"Simple object to encapsulate each data example\"\"\"\n",
    "    def __init__(self, src, trg, \n",
    "                 src_g, trg_g):    \n",
    "        self.src = src\n",
    "        self.trg = trg\n",
    "        self.src_g = src_g\n",
    "        self.trg_g = trg_g\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_str())\n",
    "    \n",
    "    def to_json_str(self):\n",
    "        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)\n",
    "    \n",
    "    def to_dict(self):\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataset:\n",
    "    \"\"\"Encapsulates the raw examples in InputExample objects\"\"\"\n",
    "    def __init__(self, data_dir):\n",
    "        self.train_examples = self.get_train_examples(data_dir)\n",
    "        self.dev_examples = self.get_dev_examples(data_dir)\n",
    "        self.test_examples = self.get_dev_examples(data_dir)\n",
    "        \n",
    "    def create_examples(self, src_path, trg_path):\n",
    "        \n",
    "        src_txt = self.get_txt_examples(src_path)\n",
    "        src_gender_labels = self.get_labels(src_path + '.label')\n",
    "        trg_txt = self.get_txt_examples(trg_path)\n",
    "        trg_gender_labels = self.get_labels(trg_path + '.label')\n",
    "        \n",
    "        examples = []\n",
    "        \n",
    "        for i in range(len(src_txt)):\n",
    "            src = src_txt[i].strip()\n",
    "            trg = trg_txt[i].strip()\n",
    "            src_g = src_gender_labels[i].strip()\n",
    "            trg_g = trg_gender_labels[i].strip()\n",
    "            input_example = InputExample(src, trg, src_g, trg_g)\n",
    "            examples.append(input_example)\n",
    "        \n",
    "        return examples\n",
    "    \n",
    "    def get_labels(self, data_dir):\n",
    "        with open(data_dir) as f:\n",
    "            return f.readlines()\n",
    "        \n",
    "    def get_txt_examples(self, data_dir):\n",
    "        with open(data_dir, encoding='utf8') as f:\n",
    "            return f.readlines()\n",
    "    \n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Reads the train examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-train.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-train.ar.M'))\n",
    "    \n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Reads the dev examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-dev.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-dev.ar.M'))\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Reads the test examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-test.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-test.ar.M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = RawDataset('/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus').train_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"src\": \"أريدها أن ترحل - فلنتخلص منها -\",\n",
       "  \"trg\": \"أريدها أن ترحل - فلنتخلص منها -\",\n",
       "  \"src_g\": \"B\",\n",
       "  \"trg_g\": \"B\"\n",
       "}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Base vocabulary class\"\"\"\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = dict()\n",
    "        \n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        return self.idx_to_token[index]\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self.token_to_idx}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "    \n",
    "class SeqVocabulary(Vocabulary):\n",
    "    \"\"\"Sequence vocabulary class\"\"\"\n",
    "    def __init__(self, token_to_idx=None, unk_token='<unk>',\n",
    "                 pad_token='<pad>', sos_token='<s>',\n",
    "                 eos_token='</s>'):\n",
    "        \n",
    "        super(SeqVocabulary, self).__init__(token_to_idx)\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        \n",
    "        self.pad_idx = self.add_token(self.pad_token)\n",
    "        self.unk_idx = self.add_token(self.unk_token)\n",
    "        self.sos_idx = self.add_token(self.sos_token)\n",
    "        self.eos_idx = self.add_token(self.eos_token)\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        contents = super(SeqVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self.unk_token,\n",
    "                         'pad_token': self.pad_token,\n",
    "                         'sos_token': self.sos_token, \n",
    "                         'eos_token': self.eos_token})\n",
    "        return contents\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx.get(token, self.unk_idx)\n",
    "    \n",
    "class GenderVocabulary(Vocabulary):\n",
    "    \"\"\"Sequence vocabulary class\"\"\"\n",
    "    def __init__(self, token_to_idx=None,\n",
    "                 pad_token='<pad>'):\n",
    "        \n",
    "        super(GenderVocabulary, self).__init__(token_to_idx)\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        \n",
    "        self.pad_idx = self.add_token(self.pad_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SeqVocabulary, self).to_serializable()\n",
    "        contents.update({'pad_token': self.pad_token})\n",
    "        return contents\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    \"\"\"Vectorizer Class\"\"\"\n",
    "    def __init__(self, src_vocab, trg_vocab, gender_vocab):\n",
    "        \"\"\"src_vocab and trg_vocab are on the char\n",
    "        level. gender_vocab is on the word level\"\"\"\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        self.gender_vocab = gender_vocab\n",
    "        \n",
    "    @classmethod\n",
    "    def create_vectorizer(cls, data_examples):\n",
    "        \"\"\"Class method which builds the vectorizer\n",
    "        vocab\"\"\"\n",
    "        \n",
    "        src_vocab = SeqVocabulary()\n",
    "        trg_vocab = SeqVocabulary()\n",
    "        gender_vocab = GenderVocabulary()\n",
    "        \n",
    "        for ex in data_examples:\n",
    "            src = ex.src\n",
    "            trg = ex.trg\n",
    "            src_gender = ex.src_g\n",
    "            trg_gender = ex.trg_g\n",
    "            \n",
    "            for t in src:\n",
    "                src_vocab.add_token(t)\n",
    "                \n",
    "            for t in trg:\n",
    "                trg_vocab.add_token(t)\n",
    "            \n",
    "            gender_vocab.add_token(src_gender)\n",
    "            gender_vocab.add_token(trg_gender)\n",
    "            \n",
    "        return cls(src_vocab, trg_vocab, gender_vocab)\n",
    "    \n",
    "    def get_src_indices(self, seq):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - seq (str): The src sequence\n",
    "        \n",
    "        Returns:\n",
    "          - indices (list): <s> + List of tokens to index mapping + </s>\n",
    "        \"\"\"\n",
    "        indices = [self.src_vocab.sos_idx] \n",
    "        indices.extend([self.src_vocab.lookup_token(t) for t in seq])\n",
    "        indices.append(self.src_vocab.eos_idx)\n",
    "        return indices\n",
    "    \n",
    "    def get_trg_indices(self, seq):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - seq (str): The trg sequence\n",
    "        \n",
    "        Returns:\n",
    "          - trg_x_indices (list): <s> + List of tokens to index mapping\n",
    "          - trg_y_indices (list): List of tokens to index mapping + </s>\n",
    "        \"\"\"\n",
    "        indices = [self.trg_vocab.lookup_token(t) for t in seq]\n",
    "        \n",
    "        trg_x_indices = [self.trg_vocab.sos_idx] + indices\n",
    "        trg_y_indices = indices + [self.trg_vocab.eos_idx]\n",
    "        return trg_x_indices, trg_y_indices\n",
    "    \n",
    "    def get_gender_indices(self, seq, gender):\n",
    "        \"\"\"\n",
    "         Args:\n",
    "          - seq (str): a sequence (src or trg)\n",
    "        \n",
    "        Returns:\n",
    "          - gender_indices (list): List of gender to index mapping\n",
    "        \"\"\"\n",
    "        indices = [self.gender_vocab.lookup_token(gender) for t in range(len(seq))]\n",
    "        return indices\n",
    "    \n",
    "    def vectorize(self, src, trg, src_gender, trg_gender):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - src (str): The src sequence\n",
    "          - src (str): The trg sequence\n",
    "        Returns:\n",
    "          - vectorized_src \n",
    "          - vectorized_trg_x \n",
    "          - vectorized_trg_y\n",
    "        \"\"\"\n",
    "        src = src\n",
    "        trg = trg\n",
    "        \n",
    "        vectorized_src = self.get_src_indices(src)\n",
    "        vectorized_trg_x, vectorized_trg_y = self.get_trg_indices(trg)\n",
    "        \n",
    "        vectorized_src_gender = self.get_gender_indices(vectorized_src, src_gender)\n",
    "        vectorized_trg_gender = self.get_gender_indices(vectorized_trg_x, trg_gender)\n",
    "        \n",
    "        return {'src': torch.tensor(vectorized_src, dtype=torch.long),\n",
    "                'trg_x': torch.tensor(vectorized_trg_x, dtype=torch.long),\n",
    "                'trg_y': torch.tensor(vectorized_trg_y, dtype=torch.long),\n",
    "                'src_gender': torch.tensor(vectorized_src_gender, dtype=torch.long),\n",
    "                'trg_gender': torch.tensor(vectorized_trg_gender, dtype=torch.long)\n",
    "               }\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'src_vocab': self.src_vocab.to_serializable(),\n",
    "                'trg_vocab': self.trg_vocab.to_serializable()\n",
    "               }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        src_vocab = SeqVocabulary.from_serializable(contents['src_vocab'])\n",
    "        trg_vocab = SeqVocabulary.from_serializable(contents['trg_vocab'])\n",
    "        return cls(src_vocab, trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MT_Dataset(Dataset):\n",
    "    \"\"\"MT Dataset as a PyTorch dataset\"\"\"\n",
    "    def __init__(self, raw_dataset, vectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.train_examples = raw_dataset.train_examples\n",
    "        self.dev_examples = raw_dataset.dev_examples\n",
    "        self.test_examples = raw_dataset.test_examples\n",
    "        self.lookup_split = {'train': self.train_examples,\n",
    "                             'dev': self.dev_examples,\n",
    "                             'test': self.test_examples}\n",
    "        self.set_split('train')\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "    \n",
    "    @classmethod\n",
    "    def load_data_and_create_vectorizer(cls, data_dir):\n",
    "        raw_dataset = RawDataset(data_dir)\n",
    "        # Note: we always create the vectorized based on the train examples\n",
    "        vectorizer = Vectorizer.create_vectorizer(raw_dataset.train_examples)\n",
    "        return cls(raw_dataset, vectorizer)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_data_and_load_vectorizer(cls, data_dir, vec_path):\n",
    "        raw_dataset = RawDataset(data_dir)\n",
    "        vectorizer = cls.load_vectorizer(vec_path)\n",
    "        return cls(raw_dataset, vectorizer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer(vec_path):\n",
    "        with open(vec_path) as f:\n",
    "            return Vectorizer.from_serializable(json.load(f))\n",
    "    \n",
    "    def save_vectorizer(self, vec_path):\n",
    "        with open(vec_path, 'w') as f:\n",
    "            return json.dump(self.vectorizer.to_serializable(), f)\n",
    "        \n",
    "    def set_split(self, split):\n",
    "        self.split = split\n",
    "        self.split_examples = self.lookup_split[self.split]\n",
    "        return self.split_examples\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        example = self.split_examples[index]\n",
    "        src, trg, src_gen, trg_gen = example.src, example.trg, \\\n",
    "                                        example.src_g, example.trg_g\n",
    "        vectorized = self.vectorizer.vectorize(src, trg, src_gen, trg_gen)\n",
    "        return vectorized\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.split_examples)\n",
    "    \n",
    "    \n",
    "class Collator:\n",
    "    def __init__(self, src_pad_idx, trg_pad_idx):\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # Sorting the batch by src seqs length in descending order\n",
    "        sorted_batch = sorted(batch, key=lambda x: x['src'].shape[0], reverse=True)\n",
    "        \n",
    "        src_seqs = [x['src'] for x in sorted_batch]\n",
    "        trg_x_seqs = [x['trg_x'] for x in sorted_batch]\n",
    "        trg_y_seqs = [x['trg_y'] for x in sorted_batch]\n",
    "        src_seqs_gender = [x['src_gender'] for x in sorted_batch]\n",
    "        trg_seqs_gender = [x['trg_gender'] for x in sorted_batch]\n",
    "        lengths = [len(seq) for seq in src_seqs]\n",
    "        \n",
    "        padded_src_seqs = pad_sequence(src_seqs, batch_first=True, padding_value=self.src_pad_idx)\n",
    "        padded_trg_x_seqs = pad_sequence(trg_x_seqs, batch_first=True, padding_value=self.trg_pad_idx)\n",
    "        padded_trg_y_seqs = pad_sequence(trg_y_seqs, batch_first=True, padding_value=self.trg_pad_idx)\n",
    "        src_seqs_gender = pad_sequence(src_seqs_gender, batch_first=True, padding_value=self.src_pad_idx)\n",
    "        trg_seqs_gender = pad_sequence(trg_seqs_gender, batch_first=True, padding_value=self.trg_pad_idx)\n",
    "        \n",
    "        lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "        \n",
    "        return {'src': padded_src_seqs,\n",
    "                'trg_x': padded_trg_x_seqs,\n",
    "                'trg_y': padded_trg_y_seqs,\n",
    "                'src_gender': src_seqs_gender,\n",
    "                'trg_gender': trg_seqs_gender,\n",
    "                'src_lengths': lengths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder biGRU\"\"\"\n",
    "    def __init__(self, src_input_dim, src_embed_dim,\n",
    "                 gender_input_dim, gender_embed_dim,\n",
    "                 hidd_dim, padding_idx=0):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_input_dim, src_embed_dim, padding_idx=padding_idx)\n",
    "        self.gender_embedding = nn.Embedding(gender_input_dim, gender_embed_dim, padding_idx=padding_idx)\n",
    "        self.rnn = nn.GRU(src_embed_dim + gender_embed_dim, hidd_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, src_seqs, src_lengths, src_seqs_gender):\n",
    "\n",
    "        embedded_seqs = self.src_embedding(src_seqs)\n",
    "        # embedded_seqs shape: [batch_size, src_seqs_len, src_embed_dim]\n",
    "        \n",
    "        embedded_gender = self.gender_embedding(src_seqs_gender)\n",
    "        # embedded_gender shape: [batch_size, src_seqs_len, gender_embed_dim]\n",
    "        \n",
    "        merged_embeddings = torch.cat((embedded_seqs, embedded_gender), dim=2)\n",
    "        # embedded_gender shape: [batch_size, src_seqs_len, src_embed_dim + gender_embed_dim]\n",
    "        \n",
    "        packed_seqs = pack_padded_sequence(merged_embeddings, src_lengths, batch_first=True)\n",
    "        \n",
    "        output, h_t = self.rnn(packed_seqs)\n",
    "        #output is a packed_sequence\n",
    "        #h_t shape: [num_layers * num_dirs, batch_size, hidd_dim]\n",
    "        \n",
    "        #reshaping h_t to [batch_size, num_layers * num_dirs * hidd_dim]\n",
    "    \n",
    "        h_t = h_t.permute(1, 0, 2) #[batch_size, num_layers * num_dirs, hidd_dim]\n",
    "        #Note: when we call permute, the contiguity of a tensor is lost,\n",
    "        #so we have to call contiguous before reshaping!\n",
    "        h_t = h_t.contiguous().view(h_t.shape[0], -1) \n",
    "        #h_t shape [batch_size, num_layers * num_dirs * hidd_dim]\n",
    "        \n",
    "        #unpacking output\n",
    "        unpacked_output, lengths = pad_packed_sequence(output, batch_first=True)\n",
    "        #output shape: [batch_size, src_seq_length, hidd_dim * num_dirs]\n",
    "        \n",
    "        return unpacked_output, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Decoder(nn.Module):\n",
    "#     \"\"\"Decoder GRU\n",
    "       \n",
    "#        Things to note:\n",
    "#            - The input to the decoder rnn at each time step is the \n",
    "#              concatenation of the embedded token and the context vector\n",
    "#            - The context vector will have a size of batch_size, hidd_dim\n",
    "#            - Note that the decoder hidd_dim == the encoder hidd_dim * 2\n",
    "#            - The prediction layer input is the concatenation of \n",
    "#              the context vector and the h_t of the decoder\n",
    "#     \"\"\"\n",
    "#     def __init__(self, trg_input_dim, trg_embed_dim,\n",
    "#                  gender_input_dim,\n",
    "#                  hidd_dim, output_dim,\n",
    "#                  padding_idx=0):\n",
    "        \n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.hidd_dim = hidd_dim\n",
    "#         self.trg_embedding = nn.Embedding(trg_input_dim, trg_embed_dim, padding_idx=padding_idx)\n",
    "#         self.gender_embedding = nn.Embedding(gender_input_dim, hidd_dim, padding_idx=padding_idx)\n",
    "#         # the input to the rnn is the context_vector + embedded token --> trg_input_dim + hidd_dim\n",
    "#         self.rnn = nn.GRU(trg_embed_dim + hidd_dim, hidd_dim * 2, batch_first=True)\n",
    "#         # the input to the classifier is h_t + context_vector --> hidd_dim * 2\n",
    "#         self.classification_layer = nn.Linear(hidd_dim + hidd_dim * 2, output_dim)\n",
    "        \n",
    "    \n",
    "#     def forward(self, trg_seqs, trg_seqs_gender, encoder_outputs, decoder_h_t, context_vectors):\n",
    "#         # trg_seqs shape: [batch_size]\n",
    "#         batch_size = trg_seqs.shape[0]\n",
    "\n",
    "#         # Step 1: embedding the target seqs\n",
    "#         embedded_seqs = self.trg_embedding(trg_seqs)\n",
    "#         # embedded_seqs shape: [batch_size, trg_embed_dim]\n",
    "\n",
    "#         # Step 2: embedding the target gender\n",
    "#         embedded_gender = self.gender_embedding(trg_seqs_gender)\n",
    "#         # embedded_gender shape: [batch_size, hidd_dim]\n",
    "        \n",
    "#         # concatenating the embedded trg sequence with the context_vectors\n",
    "#         rnn_input = torch.cat((embedded_seqs, context_vectors), dim=1)\n",
    "#         # rnn_input shape: [batch_size, embed_dim + hidd_dim]\n",
    "        \n",
    "#         # concatenating the embedded target gender with the decoder_h_t\n",
    "#         decoder_h_t = torch.cat((decoder_h_t, embedded_gender), dim=1)\n",
    "#         # decoder_h_t shape: [batch_size, hidd_dim * 2]\n",
    "        \n",
    "#         # the GRU expects an input of dimension [batch_size, sequence_len, embed_dim + hidd_dim]\n",
    "#         # since we have a single token, the sequence_len will be 1\n",
    "#         rnn_input = rnn_input.unsqueeze(1)\n",
    "#         # rnn_input shape: [batch_size, 1, embed_dim + hidd_dim]\n",
    "        \n",
    "#         # the GRU also expects a hidden state of dimension \n",
    "#         # [num_layers * num_dirs, batch_size, hidd_dim]\n",
    "#         # since we have a single layer and a forward GRU,\n",
    "#         # num_layers * num_dirs will be 1\n",
    "#         decoder_h_t = decoder_h_t.unsqueeze(0)\n",
    "#         # decoder_h_t shape: [1, batch_size, hidd_dim * 2]\n",
    "\n",
    "#         # Step 2: feeding the input to the rnn and updating the decoder_h_t\n",
    "#         output, decoder_h_t = self.rnn(rnn_input, decoder_h_t)\n",
    "        \n",
    "#         # output shape: [batch, 1, hidd_dim]\n",
    "#         # decoder_h_t shape: [1, batch_size, hidd_dim]\n",
    "        \n",
    "#         decoder_h_t = decoder_h_t.squeeze(0)\n",
    "#         # decoder_h_t shape: [batch_size, hidd_dim * 2]\n",
    "        \n",
    "#         # concatenating decoder_h_t with the context_vectors to create a \n",
    "#         # prediction vector\n",
    "#         predictions_vector = torch.cat((decoder_h_t, context_vectors), dim=1)\n",
    "#         # predictions_vector: [batch_size, hidd_dim + hidd_dim * 2]\n",
    "\n",
    "#         # Step 3: feeding the prediction vector to the fc layer\n",
    "#         # to a make a prediction\n",
    "#         prediction = self.classification_layer(predictions_vector)\n",
    "#         # prediction shape: [batch_size, output_dim]\n",
    "        \n",
    "# #         return prediction, decoder_h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder GRU\n",
    "       \n",
    "       Things to note:\n",
    "           - The input to the decoder rnn at each time step is the \n",
    "             concatenation of the embedded token and the context vector\n",
    "           - The context vector will have a size of batch_size, hidd_dim\n",
    "           - Note that the decoder hidd_dim == the encoder hidd_dim * 2\n",
    "           - The prediction layer input is the concatenation of \n",
    "             the context vector, the embedded gender, and the h_t of the decoder\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trg_input_dim, trg_embed_dim,\n",
    "                 gender_input_dim, gender_embed_dim,\n",
    "                 hidd_dim, attention,\n",
    "                 output_dim,\n",
    "                 padding_idx=0):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidd_dim = hidd_dim\n",
    "        self.attention = attention\n",
    "        self.trg_embedding = nn.Embedding(trg_input_dim, trg_embed_dim, padding_idx=padding_idx)\n",
    "        self.gender_embedding = nn.Embedding(gender_input_dim, gender_embed_dim, padding_idx=padding_idx)\n",
    "        # the input to the rnn is the context_vector + embedded token --> trg_input_dim + hidd_dim\n",
    "        self.rnn = nn.GRUCell(trg_embed_dim + hidd_dim, hidd_dim)\n",
    "        # the input to the classifier is h_t + context_vector + embedded gender --> hidd_dim * 2 + gender_embed_dim\n",
    "        self.classification_layer = nn.Linear(gender_embed_dim + (hidd_dim * 2), output_dim)\n",
    "        \n",
    "        # sampling temperature\n",
    "        self.sampling_temperature = 3\n",
    " \n",
    "    def forward(self, trg_seqs, trg_seqs_gender, encoder_outputs, encoder_h_t, mask, teacher_forcing_prob=0.3):\n",
    "        \n",
    "        # reshaping trg_seqs to: [trg_seq_len, batch_size]\n",
    "        trg_seqs = trg_seqs.permute(1, 0)\n",
    "        trg_seq_len, batch_size = trg_seqs.shape\n",
    "        trg_seqs_gender = trg_seqs_gender.permute(1, 0)\n",
    "        \n",
    "        # initializing the context_vectors to zeros\n",
    "        context_vectors = torch.zeros(batch_size, self.hidd_dim)\n",
    "        \n",
    "        # moving the context_vectors to the right device\n",
    "        context_vectors = context_vectors.to(encoder_h_t.device)\n",
    "\n",
    "        # initializing the hidden state to the encoder hidden state\n",
    "        h_t = encoder_h_t\n",
    "        \n",
    "        # initializing the first trg input to the <sos> token\n",
    "        y_t = trg_seqs[0, :] \n",
    "\n",
    "        outputs = []\n",
    "        self.attention_scores = []\n",
    "        \n",
    "        for i in range(trg_seq_len):\n",
    "            teacher_forcing = np.random.random() < teacher_forcing_prob\n",
    "            # if teacher_forcing, use ground truth target tokens\n",
    "            # as an input to the decoder\n",
    "            if teacher_forcing:\n",
    "                y_t = trg_seqs[i]\n",
    "                \n",
    "            # Step 1: Embed the current target token\n",
    "            embedded_trg = self.trg_embedding(y_t)\n",
    "            # embedded_trg shape: [batch_size, trg_embed_dim]\n",
    "            \n",
    "            # Step 2: Concat the embedded token with the context vectors\n",
    "            rnn_input = torch.cat((embedded_trg, context_vectors), dim=1)\n",
    "            # rnn_input shape: [batch_size, trg_embed_dim + hidd_dim]\n",
    "            \n",
    "            # Step 3: Do a single RNN step and update the decoder hidden state\n",
    "            h_t = self.rnn(rnn_input, h_t)\n",
    "            \n",
    "            # Step 4: Calculate attention and update context vectors\n",
    "            context_vectors, attention_probs = self.attention(encoder_outputs, h_t, mask)\n",
    "\n",
    "            # Step 5: Embed the gender information\n",
    "            embedded_gender = self.gender_embedding(trg_seqs_gender[i])\n",
    "            # embedded_gender shape: [batch_size, gender_embed_dim]\n",
    "            \n",
    "            # Step 6: Obtain a prediction vector\n",
    "            prediction_vector = torch.cat((h_t, context_vectors, embedded_gender), dim=1)\n",
    "            # prediction_vector shape: [batch_size, gender_embed_dim + (hidd_dim * 2)]\n",
    "            \n",
    "            # Step 7: Obtain the prediction output\n",
    "            pred_output = self.classification_layer(prediction_vector)\n",
    "            # pred_output shape: [batch_size, output_dim]\n",
    "            \n",
    "            # If not teacher force, use the maximum \n",
    "            # prediction as an input to the decoder in \n",
    "            # the next time step\n",
    "            if not teacher_forcing:\n",
    "                # we multiply the predictions with a sampling_temperature\n",
    "                # to make the propablities peakier, so we can be confident about the\n",
    "                # maximum prediction\n",
    "                pred_output_probs = F.softmax(pred_output * self.sampling_temperature, dim=1)\n",
    "                y_t = torch.argmax(pred_output_probs, dim=1)\n",
    "\n",
    "            outputs.append(pred_output)\n",
    "            self.attention_scores.append(attention_probs)\n",
    "            \n",
    "        outputs = torch.stack(outputs).permute(1, 0, 2)\n",
    "        # outputs shape: [batch_size, trg_seq_len, output_dim]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"Attention mechanism as a MLP \n",
    "    as used by Bahdanau et. al 2015\"\"\"\n",
    "\n",
    "    def __init__(self, encoder_hidd_dim, decoder_hidd_dim):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.atten = nn.Linear((encoder_hidd_dim * 2) + decoder_hidd_dim, decoder_hidd_dim)\n",
    "        self.v = nn.Linear(decoder_hidd_dim, 1)\n",
    "        \n",
    "    def forward(self, key_vectors, query_vector, mask):\n",
    "        \"\"\"key_vectors: encoder hidden states.\n",
    "           query_vector: decoder hidden state at time t\n",
    "           mask: the mask vector of zeros and ones\n",
    "        \"\"\"\n",
    "        \n",
    "        #key_vectors shape: [batch_size, src_seq_length, encoder_hidd_dim * 2]\n",
    "        #query_vector shape: [batch_size, decoder_hidd_dim]\n",
    "        #Note: encoder_hidd_dim * 2 == decoder_hidd_dim\n",
    "        \n",
    "        batch_size, src_seq_length, encoder_hidd_dim = key_vectors.shape\n",
    "        \n",
    "        #changing the shape of query_vector to [batch_size, src_seq_length, decoder_hidd_dim]\n",
    "        #we will repeat the query_vector src_seq_length times at dim 1\n",
    "        query_vector = query_vector.unsqueeze(1).repeat(1, src_seq_length, 1)\n",
    "        \n",
    "        # Step 1: Compute the attention scores through a MLP\n",
    "        \n",
    "        # concatenating the key_vectors and the query_vector\n",
    "        atten_input = torch.cat((key_vectors, query_vector), dim=2)\n",
    "        # atten_input shape: [batch_size, src_seq_length, (encoder_hidd_dim * 2) + decoder_hidd_dim]\n",
    "        \n",
    "        atten_scores = self.atten(atten_input)\n",
    "        # atten_scores shape: [batch_size, src_seq_length, decoder_hidd_dim]\n",
    "\n",
    "        atten_scores = torch.tanh(atten_scores)\n",
    "    \n",
    "        # mapping atten_scores from decoder_hidd_dim to 1\n",
    "        atten_scores = self.v(atten_scores)\n",
    "    \n",
    "        # atten_scores shape: [batch_size, src_seq_length, 1]\n",
    "        atten_scores = atten_scores.squeeze(dim=2)\n",
    "        # atten_scores shape: [batch_size, src_seq_length]\n",
    "        \n",
    "        # masking the atten_scores\n",
    "        atten_scores = atten_scores.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # Step 2: normalizing atten_scores through a softmax to get probs\n",
    "        atten_scores = F.softmax(atten_scores, dim=1)\n",
    "        \n",
    "        # Step 3: computing the new context vector\n",
    "        context_vectors = torch.matmul(key_vectors.permute(0, 2, 1), atten_scores.unsqueeze(2)).squeeze(dim=2)\n",
    "        \n",
    "        # context_vectors shape: [batch_size, encoder_hidd_dim * 2]\n",
    "        \n",
    "        return context_vectors, atten_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder_input_dim, encoder_embed_dim,\n",
    "                 src_gender_input_dim, src_gender_embed_dim,\n",
    "                 encoder_hidd_dim, decoder_input_dim, decoder_embed_dim,\n",
    "                 trg_gender_input_dim, trg_gender_embed_dim,\n",
    "                 decoder_output_dim, src_padding_idx=0, trg_padding_idx=0):\n",
    "    \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.src_padding_idx = src_padding_idx\n",
    "        self.encoder = Encoder(src_input_dim=encoder_input_dim,\n",
    "                               src_embed_dim=encoder_embed_dim,\n",
    "                               gender_input_dim=src_gender_input_dim,\n",
    "                               gender_embed_dim=src_gender_embed_dim,\n",
    "                               hidd_dim=encoder_hidd_dim,\n",
    "                               padding_idx=src_padding_idx)\n",
    "\n",
    "        decoder_hidd_dim = encoder_hidd_dim * 2 \n",
    "        \n",
    "        self.attention = AdditiveAttention(encoder_hidd_dim=encoder_hidd_dim,\n",
    "                                           decoder_hidd_dim=decoder_hidd_dim)\n",
    "        \n",
    "        self.decoder = Decoder(trg_input_dim=decoder_input_dim,\n",
    "                               trg_embed_dim=decoder_embed_dim,\n",
    "                               gender_input_dim=trg_gender_input_dim,\n",
    "                               gender_embed_dim=trg_gender_embed_dim,\n",
    "                               hidd_dim=decoder_hidd_dim,\n",
    "                               output_dim=decoder_output_dim,\n",
    "                               attention=self.attention,\n",
    "                               padding_idx=trg_padding_idx)\n",
    "    \n",
    "    def create_mask(self, src_seqs, src_padding_idx):\n",
    "        mask = (src_seqs != src_padding_idx)\n",
    "        # mask shape: [batch_size, src_seq_length]\n",
    "        return mask \n",
    "    \n",
    "    def forward(self, src_seqs, src_seqs_lengths, trg_seqs, src_gender, trg_gender, teacher_forcing_prob=0.3):\n",
    "        encoder_output, encoder_h_t = self.encoder(src_seqs, src_seqs_lengths, src_gender)\n",
    "        mask = self.create_mask(src_seqs, self.src_padding_idx)\n",
    "        decoder_output = self.decoder(trg_seqs, trg_gender, encoder_output, encoder_h_t, mask, \n",
    "                                      teacher_forcing_prob=teacher_forcing_prob)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, cuda):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(data_dir='/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus',\n",
    "                          vectorizer_path='/home/ba63/gender-bias/models/saved_models/char_level_vectorizer.json',\n",
    "                          reload_files=False,\n",
    "                          cache_files=False,\n",
    "                          num_epochs=50,\n",
    "                          embedding_dim=32,\n",
    "                          gender_embedding_dim=16,\n",
    "                          hidd_dim=64,\n",
    "                          learning_rate=5e-4,\n",
    "                          use_cuda=True,\n",
    "                          batch_size=64,\n",
    "                          seed=21,\n",
    "                          model_path='/home/ba63/gender-bias/models/saved_models/char_level_gender_model.pt'\n",
    "                          )\n",
    "\n",
    "device = torch.device('cuda' if args.use_cuda else 'cpu')\n",
    "set_seed(args.seed, args.use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.reload_files:\n",
    "    dataset = MT_Dataset.load_data_and_load_vectorizer(args.data_dir, args.vectorizer_path)\n",
    "else:\n",
    "    dataset = MT_Dataset.load_data_and_create_vectorizer(args.data_dir)\n",
    "\n",
    "if args.cache_files:\n",
    "    dataset.save_vectorizer(args.vectorizer_path)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "ENCODER_INPUT_DIM = len(vectorizer.src_vocab)\n",
    "GENDER_INPUT_DIM = len(vectorizer.gender_vocab)\n",
    "DECODER_INPUT_DIM = len(vectorizer.trg_vocab)\n",
    "DECODER_OUTPUT_DIM = len(vectorizer.trg_vocab)\n",
    "SRC_PAD_INDEX = vectorizer.src_vocab.pad_idx\n",
    "TRG_PAD_INDEX = vectorizer.trg_vocab.pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (src_embedding): Embedding(71, 32, padding_idx=0)\n",
       "    (gender_embedding): Embedding(4, 16, padding_idx=0)\n",
       "    (rnn): GRU(48, 64, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (attention): AdditiveAttention(\n",
       "    (atten): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (v): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): AdditiveAttention(\n",
       "      (atten): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (v): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "    (trg_embedding): Embedding(71, 32, padding_idx=0)\n",
       "    (gender_embedding): Embedding(4, 16, padding_idx=0)\n",
       "    (rnn): GRUCell(160, 128)\n",
       "    (classification_layer): Linear(in_features=272, out_features=71, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2Seq(encoder_input_dim=ENCODER_INPUT_DIM,\n",
    "                encoder_embed_dim=args.embedding_dim,\n",
    "                src_gender_input_dim=GENDER_INPUT_DIM,\n",
    "                src_gender_embed_dim=args.gender_embedding_dim,\n",
    "                encoder_hidd_dim=args.hidd_dim,\n",
    "                decoder_input_dim=DECODER_INPUT_DIM,\n",
    "                decoder_embed_dim=args.embedding_dim,\n",
    "                trg_gender_input_dim=GENDER_INPUT_DIM,\n",
    "                trg_gender_embed_dim=args.gender_embedding_dim,\n",
    "                decoder_output_dim=DECODER_OUTPUT_DIM,\n",
    "                src_padding_idx=SRC_PAD_INDEX,\n",
    "                trg_padding_idx=TRG_PAD_INDEX)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_INDEX)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                 patience=2, factor=0.5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device='cpu', teacher_forcing_prob=1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        src = batch['src']\n",
    "        trg_x = batch['trg_x']\n",
    "        trg_y = batch['trg_y']\n",
    "        src_gender = batch['src_gender']\n",
    "        trg_gender = batch['trg_gender']\n",
    "        src_lengths = batch['src_lengths']\n",
    "        \n",
    "        preds = model(src, src_lengths, trg_x, src_gender, trg_gender, teacher_forcing_prob=teacher_forcing_prob)\n",
    "        \n",
    "        # CrossEntropysLoss accepts matrices always! \n",
    "        # the preds must be of size (N, C) where C is the number \n",
    "        # of classes and N is the number of samples. \n",
    "        # The ground truth must be a Vector of size C!\n",
    "        preds = preds.contiguous().view(-1, preds.shape[-1])\n",
    "        trg_y = trg_y.view(-1)\n",
    "\n",
    "        loss = criterion(preds, trg_y)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device='cpu', teacher_forcing_prob=0):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            src = batch['src']\n",
    "            trg_x = batch['trg_x']\n",
    "            trg_y = batch['trg_y']\n",
    "            src_gender = batch['src_gender']\n",
    "            trg_gender = batch['trg_gender']\n",
    "            src_lengths = batch['src_lengths']\n",
    "\n",
    "            preds = model(src, src_lengths, trg_x, src_gender, trg_gender, teacher_forcing_prob=teacher_forcing_prob)\n",
    "            # CrossEntropyLoss accepts matrices always! \n",
    "            # the preds must be of size (N, C) where C is the number \n",
    "            # of classes and N is the number of samples. \n",
    "            # The ground truth must be a Vector of size C!\n",
    "            preds = preds.contiguous().view(-1, preds.shape[-1])\n",
    "            trg_y = trg_y.view(-1)\n",
    "            \n",
    "            loss = criterion(preds, trg_y)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Epoch: 1\n",
      "\tTrain Loss: 3.2225   |   Dev Loss: 3.0868\n",
      "Epoch: 2\n",
      "\tTrain Loss: 2.9426   |   Dev Loss: 2.7709\n",
      "Epoch: 3\n",
      "\tTrain Loss: 2.3820   |   Dev Loss: 2.0921\n",
      "Epoch: 4\n",
      "\tTrain Loss: 1.7550   |   Dev Loss: 1.5218\n",
      "Epoch: 5\n",
      "\tTrain Loss: 1.3590   |   Dev Loss: 1.2417\n",
      "Epoch: 6\n",
      "\tTrain Loss: 1.0618   |   Dev Loss: 1.1237\n",
      "Epoch: 7\n",
      "\tTrain Loss: 0.7951   |   Dev Loss: 0.8918\n",
      "Epoch: 8\n",
      "\tTrain Loss: 0.6224   |   Dev Loss: 0.6509\n",
      "Epoch: 9\n",
      "\tTrain Loss: 0.5320   |   Dev Loss: 0.5308\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.4567   |   Dev Loss: 0.4997\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.4483   |   Dev Loss: 0.4653\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.3960   |   Dev Loss: 0.4018\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.3003   |   Dev Loss: 0.3577\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.2757   |   Dev Loss: 0.3556\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.2501   |   Dev Loss: 0.3050\n",
      "Epoch: 16\n",
      "\tTrain Loss: 1.2469   |   Dev Loss: 2.5384\n",
      "Epoch: 17\n",
      "\tTrain Loss: 1.2401   |   Dev Loss: 0.7479\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.4662   |   Dev Loss: 0.4465\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.2638   |   Dev Loss: 0.3350\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.2157   |   Dev Loss: 0.3220\n",
      "Epoch: 21\n",
      "\tTrain Loss: 0.1919   |   Dev Loss: 0.2747\n",
      "Epoch: 22\n",
      "\tTrain Loss: 0.1653   |   Dev Loss: 0.4480\n",
      "Epoch: 23\n",
      "\tTrain Loss: 0.1563   |   Dev Loss: 0.2791\n",
      "Epoch: 24\n",
      "\tTrain Loss: 0.1592   |   Dev Loss: 0.6319\n",
      "Epoch: 25\n",
      "\tTrain Loss: 0.1378   |   Dev Loss: 0.2896\n",
      "Epoch: 26\n",
      "\tTrain Loss: 0.1198   |   Dev Loss: 0.2932\n",
      "Epoch: 27\n",
      "\tTrain Loss: 0.1177   |   Dev Loss: 0.2633\n",
      "Epoch: 28\n",
      "\tTrain Loss: 0.1250   |   Dev Loss: 0.2020\n",
      "Epoch: 29\n",
      "\tTrain Loss: 0.1942   |   Dev Loss: 0.2995\n",
      "Epoch: 30\n",
      "\tTrain Loss: 0.1043   |   Dev Loss: 0.2739\n",
      "Epoch: 31\n",
      "\tTrain Loss: 0.0999   |   Dev Loss: 0.2286\n",
      "Epoch: 32\n",
      "\tTrain Loss: 0.1013   |   Dev Loss: 0.2890\n",
      "Epoch: 33\n",
      "\tTrain Loss: 0.0992   |   Dev Loss: 0.2177\n",
      "Epoch: 34\n",
      "\tTrain Loss: 0.0892   |   Dev Loss: 0.2172\n",
      "Epoch: 35\n",
      "\tTrain Loss: 0.0847   |   Dev Loss: 0.2512\n",
      "Epoch: 36\n",
      "\tTrain Loss: 0.0790   |   Dev Loss: 0.1870\n",
      "Epoch: 37\n",
      "\tTrain Loss: 0.0994   |   Dev Loss: 0.2221\n",
      "Epoch: 38\n",
      "\tTrain Loss: 0.1096   |   Dev Loss: 0.2544\n",
      "Epoch: 39\n",
      "\tTrain Loss: 0.0834   |   Dev Loss: 0.2251\n",
      "Epoch: 40\n",
      "\tTrain Loss: 0.0783   |   Dev Loss: 0.1921\n",
      "Epoch: 41\n",
      "\tTrain Loss: 0.0796   |   Dev Loss: 0.2152\n",
      "Epoch: 42\n",
      "\tTrain Loss: 0.0736   |   Dev Loss: 0.2178\n",
      "Epoch: 43\n",
      "\tTrain Loss: 0.0753   |   Dev Loss: 0.1831\n",
      "Epoch: 44\n",
      "\tTrain Loss: 0.0805   |   Dev Loss: 0.1926\n",
      "Epoch: 45\n",
      "\tTrain Loss: 0.0819   |   Dev Loss: 0.2417\n",
      "Epoch: 46\n",
      "\tTrain Loss: 0.0812   |   Dev Loss: 0.1918\n",
      "Epoch: 47\n",
      "\tTrain Loss: 0.0749   |   Dev Loss: 0.2051\n",
      "Epoch: 48\n",
      "\tTrain Loss: 0.0751   |   Dev Loss: 0.2075\n",
      "Epoch: 49\n",
      "\tTrain Loss: 0.0745   |   Dev Loss: 0.2297\n",
      "Epoch: 50\n",
      "\tTrain Loss: 0.0799   |   Dev Loss: 0.1969\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "collator = Collator(SRC_PAD_INDEX, TRG_PAD_INDEX)\n",
    "best_loss = 1e10\n",
    "set_seed(args.seed, args.use_cuda)\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "print(f'Using {device}')\n",
    "for epoch in range(args.num_epochs):\n",
    "#     teacher_forcing_prob = (epoch + 5) / args.num_epochs\n",
    "    teacher_forcing_prob = 0.3\n",
    "    dataset.set_split('train')\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=args.batch_size, collate_fn=collator)\n",
    "\n",
    "    train_loss = train(model, dataloader, optimizer, criterion, device, teacher_forcing_prob=teacher_forcing_prob)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    dataset.set_split('dev')\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=args.batch_size, collate_fn=collator)\n",
    "    dev_loss = evaluate(model, dataloader, criterion, device, teacher_forcing_prob=0)\n",
    "    dev_losses.append(dev_loss)\n",
    "    \n",
    "    #save best model\n",
    "    if dev_loss < best_loss:\n",
    "        best_loss = dev_loss\n",
    "        torch.save(model.state_dict(), args.model_path)\n",
    "    \n",
    "    scheduler.step(dev_loss)\n",
    "    print(f'Epoch: {(epoch + 1)}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f}   |   Dev Loss: {dev_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, 1 + args.num_epochs), np.asarray(train_losses), 'b-', color='blue', label='Training')\n",
    "plt.plot(range(1, 1 + args.num_epochs), np.asarray(dev_losses), 'b-', color='orange', label='Evaluation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMT_Batch_Sampler:\n",
    "    def __init__(self, model, src_vocab, trg_vocab):\n",
    "        self.model = model\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "    \n",
    "    def update_batch(self, batch):\n",
    "        self.sample_batch = batch\n",
    "        \n",
    "        src = batch['src']\n",
    "        trg_x = batch['trg_x']\n",
    "        trg_y = batch['trg_y']\n",
    "        src_gender = batch['src_gender']\n",
    "        trg_gender = batch['trg_gender']\n",
    "        src_lengths = batch['src_lengths']\n",
    "\n",
    "        preds = self.model(src, src_lengths, trg_x, src_gender, trg_gender, teacher_forcing_prob=0)\n",
    "        # preds shape: [batch_size, trg_seq_len, output_dim]\n",
    "        \n",
    "        attention_scores = [score.cpu().detach() for score in self.model.decoder.attention_scores]\n",
    "        # len(attention_scores): trg_seq_len\n",
    "        # each vector in attention_scores has a shape: [batch_size, src_seq_len]\n",
    "        attention_scores = torch.stack(attention_scores)\n",
    "        attention_scores = attention_scores.permute(1, 0, 2)\n",
    "        # attention_score shape: [batch_size, trg_seq_len, src_seq_len]\n",
    "        \n",
    "        self.sample_batch['preds'] = preds\n",
    "        self.sample_batch['attention_scores'] = attention_scores\n",
    "        return self.sample_batch\n",
    "    \n",
    "    def get_pred_sentence(self, index):\n",
    "        preds = self.sample_batch['preds']\n",
    "       \n",
    "        max_preds = torch.argmax(preds, dim=2)\n",
    "        # max_preds shape: [batch_size, trg_seq_len]\n",
    "        max_pred_sentence = max_preds[index].cpu().detach().numpy()\n",
    "        return self.get_str_sentence(max_pred_sentence, self.trg_vocab)\n",
    "    \n",
    "    def get_trg_sentence(self, index):\n",
    "        trg_sentence = self.sample_batch['trg_y'][index].cpu().detach().numpy()\n",
    "        return self.get_str_sentence(trg_sentence, self.trg_vocab)\n",
    "    \n",
    "    def get_src_sentence(self, index):\n",
    "        src_sentence = self.sample_batch['src'][index].cpu().detach().numpy()\n",
    "        return self.get_str_sentence(src_sentence, self.src_vocab)\n",
    "    \n",
    "    def get_str_sentence(self, vectorized_sentence, vocab):\n",
    "        sentence = []\n",
    "        for i in vectorized_sentence:\n",
    "            if i == vocab.sos_idx:\n",
    "                continue\n",
    "            elif i == vocab.eos_idx:\n",
    "                break\n",
    "            else:\n",
    "                sentence.append(vocab.lookup_index(i))\n",
    "        return ''.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed, args.use_cuda)\n",
    "dataset.set_split('train')\n",
    "model.load_state_dict(torch.load(args.model_path))\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "collator = Collator(SRC_PAD_INDEX, TRG_PAD_INDEX)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collator)\n",
    "sampler = NMT_Batch_Sampler(model, vectorizer.src_vocab, vectorizer.trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_preds = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/edits_annotations/char_level_gender_model.train_preds', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    updated_batch = sampler.update_batch(batch)\n",
    "\n",
    "#     print(updated_batch['trg_x'])\n",
    "#     print(updated_batch['trg_y'])\n",
    "#     print(updated_batch['src'])\n",
    "    src = sampler.get_src_sentence(0)\n",
    "    trg = sampler.get_trg_sentence(0)\n",
    "    pred = sampler.get_pred_sentence(0)\n",
    "    \n",
    "    new_train_preds.write(pred)\n",
    "#     print(src)\n",
    "#     print(trg)\n",
    "#     print(pred)\n",
    "    new_train_preds.write('\\n')\n",
    "#     attention_scores = updated_batch['attention_scores'][0].cpu().detach().numpy()\n",
    "#     fig = plt.figure(figsize=(10,10))\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     cax = ax.matshow(attention_scores, cmap='bone')\n",
    "#     fig.colorbar(cax)\n",
    "\n",
    "#     # Set up axes\n",
    "#     ax.set_xticklabels(['','<s>'] + src.split(' ') +\n",
    "#                    ['</s>'], rotation=90)\n",
    "#     ax.set_yticklabels([''] + pred.split(' ') + ['</s>'])\n",
    "\n",
    "#     # Show label at every tick\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#     ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "new_train_preds.close()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed, args.use_cuda)\n",
    "dataset.set_split('dev')\n",
    "model.load_state_dict(torch.load(args.model_path))\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "collator = Collator(SRC_PAD_INDEX, TRG_PAD_INDEX)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collator)\n",
    "sampler = NMT_Batch_Sampler(model, vectorizer.src_vocab, vectorizer.trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dev_preds = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/edits_annotations/char_level_gender_model.dev_preds', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_log = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/edits_annotations/char_level_gender_model.dev_log', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    updated_batch = sampler.update_batch(batch)\n",
    "\n",
    "#     print(updated_batch['trg_x'])\n",
    "#     print(updated_batch['trg_y'])\n",
    "#     print(updated_batch['src'])\n",
    "    src = sampler.get_src_sentence(0)\n",
    "    trg = sampler.get_trg_sentence(0)\n",
    "    pred = sampler.get_pred_sentence(0)\n",
    "    \n",
    "    new_dev_preds.write(pred)\n",
    "#     print(src)\n",
    "#     print(trg)\n",
    "#     print(pred)\n",
    "    new_dev_preds.write('\\n')\n",
    "    \n",
    "    dev_log.write(f'src: ' + src)\n",
    "    dev_log.write('\\n')\n",
    "    dev_log.write(f'trg: ' + trg)\n",
    "    dev_log.write('\\n')\n",
    "    dev_log.write(f'pred: ' + pred)\n",
    "    dev_log.write('\\n\\n')\n",
    "#     attention_scores = updated_batch['attention_scores'][0].cpu().detach().numpy()\n",
    "#     fig = plt.figure(figsize=(10,10))\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     cax = ax.matshow(attention_scores, cmap='bone')\n",
    "#     fig.colorbar(cax)\n",
    "\n",
    "#     # Set up axes\n",
    "#     ax.set_xticklabels(['','<s>'] + src.split(' ') +\n",
    "#                    ['</s>'], rotation=90)\n",
    "#     ax.set_yticklabels([''] + pred.split(' ') + ['</s>'])\n",
    "\n",
    "#     # Show label at every tick\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#     ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "dev_log.close()\n",
    "new_dev_preds.close()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search\n",
    "eos_idx = 4\n",
    "def beam_search_decoder(data, k):\n",
    "    sequences = [[list(), 1.0]]\n",
    "    # walk over each step in sequence\n",
    "    for row in data:\n",
    "        all_candidates = list()\n",
    "        # expand each current candidate\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            for j in range(len(row)):\n",
    "                candidate = [seq + [j], score * -log(row[j])]\n",
    "                print(candidate)\n",
    "                all_candidates.append(candidate)\n",
    "            print('-------')\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "        # select k best\n",
    "        sequences = ordered[:k]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], 2.3025850929940455]\n",
      "[[1], 1.6094379124341003]\n",
      "[[2], 1.2039728043259361]\n",
      "[[3], 0.916290731874155]\n",
      "[[4], 0.6931471805599453]\n",
      "-------\n",
      "[[4, 0], 0.4804530139182014]\n",
      "[[4, 1], 0.6351243373717793]\n",
      "[[4, 2], 0.8345303547893733]\n",
      "[[4, 3], 1.1155773512899807]\n",
      "[[4, 4], 1.596030365208182]\n",
      "-------\n",
      "[[3, 0], 0.6351243373717793]\n",
      "[[3, 1], 0.8395887053184746]\n",
      "[[3, 2], 1.1031891220323908]\n",
      "[[3, 3], 1.474713042690254]\n",
      "[[3, 4], 2.109837380062033]\n",
      "-------\n",
      "[[2, 0], 0.8345303547893733]\n",
      "[[2, 1], 1.1031891220323908]\n",
      "[[2, 2], 1.4495505135564588]\n",
      "[[2, 3], 1.937719476821764]\n",
      "[[2, 4], 2.7722498316111372]\n",
      "-------\n",
      "[[4, 0, 0], 1.1062839477321111]\n",
      "[[4, 0, 1], 0.7732592957431818]\n",
      "[[4, 0, 2], 0.5784523625139449]\n",
      "[[4, 0, 3], 0.4402346437542523]\n",
      "[[4, 0, 4], 0.33302465198892944]\n",
      "-------\n",
      "[[4, 1, 0], 1.46242783142998]\n",
      "[[4, 1, 1], 1.0221931876757278]\n",
      "[[4, 1, 2], 0.7646724295611531]\n",
      "[[4, 1, 3], 0.5819585439214754]\n",
      "[[4, 1, 4], 0.4402346437542523]\n",
      "-------\n",
      "[[3, 0, 0], 1.46242783142998]\n",
      "[[3, 0, 1], 1.0221931876757278]\n",
      "[[3, 0, 2], 0.7646724295611531]\n",
      "[[3, 0, 3], 0.5819585439214754]\n",
      "[[3, 0, 4], 0.4402346437542523]\n",
      "-------\n",
      "[[4, 0, 4], 0.33302465198892944]\n",
      "[[4, 0, 3], 0.4402346437542523]\n",
      "[[4, 1, 4], 0.4402346437542523]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import log\n",
    "data = [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5]]\n",
    "\n",
    "data = np.asarray(data)\n",
    "# decode sequence\n",
    "result = beam_search_decoder(data, 3)\n",
    "# print result\n",
    "for seq in result:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
