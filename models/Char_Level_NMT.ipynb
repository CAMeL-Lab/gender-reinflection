{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample:\n",
    "    \"\"\"Simple object to encapsulate each data example\"\"\"\n",
    "    def __init__(self, src, trg, \n",
    "                 src_g, trg_g):    \n",
    "        self.src = src\n",
    "        self.trg = trg\n",
    "        self.src_g = src_g\n",
    "        self.trg_g = trg_g\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_str())\n",
    "    \n",
    "    def to_json_str(self):\n",
    "        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)\n",
    "    \n",
    "    def to_dict(self):\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataset:\n",
    "    \"\"\"Encapsulates the raw examples in InputExample objects\"\"\"\n",
    "    def __init__(self, data_dir):\n",
    "        self.train_examples = self.get_train_examples(data_dir)\n",
    "        self.dev_examples = self.get_dev_examples(data_dir)\n",
    "        self.test_examples = self.get_dev_examples(data_dir)\n",
    "        \n",
    "    def create_examples(self, src_path, trg_path):\n",
    "        \n",
    "        src_txt = self.get_txt_examples(src_path)\n",
    "        src_gender_labels = self.get_labels(src_path + '.label')\n",
    "        trg_txt = self.get_txt_examples(trg_path)\n",
    "        trg_gender_labels = self.get_labels(trg_path + '.label')\n",
    "        \n",
    "        examples = []\n",
    "        \n",
    "        for i in range(len(src_txt)):\n",
    "            src = src_txt[i].strip()\n",
    "            trg = trg_txt[i].strip()\n",
    "            src_g = src_gender_labels[i].strip()\n",
    "            trg_g = trg_gender_labels[i].strip()\n",
    "            input_example = InputExample(src, trg, src_g, trg_g)\n",
    "            examples.append(input_example)\n",
    "        \n",
    "        return examples\n",
    "    \n",
    "    def get_labels(self, data_dir):\n",
    "        with open(data_dir) as f:\n",
    "            return f.readlines()\n",
    "        \n",
    "    def get_txt_examples(self, data_dir):\n",
    "        with open(data_dir, encoding='utf8') as f:\n",
    "            return f.readlines()\n",
    "    \n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Reads the train examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-train.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-train.ar.M'))\n",
    "    \n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Reads the dev examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-dev.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-dev.ar.M'))\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Reads the test examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-test.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-test.ar.M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = RawDataset('/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/').train_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8566"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Base vocabulary class\"\"\"\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = dict()\n",
    "        \n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        return self.idx_to_token[index]\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self.token_to_idx}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "    \n",
    "class SeqVocabulary(Vocabulary):\n",
    "    \"\"\"Sequence vocabulary class\"\"\"\n",
    "    def __init__(self, token_to_idx=None, unk_token='<unk>',\n",
    "                 pad_token='<pad>', sos_token='<s>',\n",
    "                 eos_token='</s>'):\n",
    "        \n",
    "        super(SeqVocabulary, self).__init__(token_to_idx)\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        \n",
    "        self.pad_idx = self.add_token(self.pad_token)\n",
    "        self.unk_idx = self.add_token(self.unk_token)\n",
    "        self.sos_idx = self.add_token(self.sos_token)\n",
    "        self.eos_idx = self.add_token(self.eos_token)\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        contents = super(SeqVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self.unk_token,\n",
    "                         'pad_token': self.pad_token,\n",
    "                         'sos_token': self.sos_token, \n",
    "                         'eos_token': self.eos_token})\n",
    "        return contents\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx.get(token, self.unk_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    \"\"\"Vectorizer Class\"\"\"\n",
    "    def __init__(self, src_vocab, trg_vocab):\n",
    "        \"\"\"src_vocab and trg_vocab are on the char\n",
    "        level\"\"\"\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "    \n",
    "    @classmethod\n",
    "    def create_vectorizer(cls, data_examples):\n",
    "        \"\"\"Class method which builds the vectorizer\n",
    "        vocab\"\"\"\n",
    "        \n",
    "        src_vocab = SeqVocabulary()\n",
    "        trg_vocab = SeqVocabulary()\n",
    "        \n",
    "        for ex in data_examples:\n",
    "            src = ex.src\n",
    "            trg = ex.trg\n",
    "            \n",
    "            for t in src:\n",
    "                src_vocab.add_token(t)\n",
    "                \n",
    "            for t in trg:\n",
    "                trg_vocab.add_token(t)\n",
    "        \n",
    "        return cls(src_vocab, trg_vocab)\n",
    "    \n",
    "    def get_src_indices(self, seq):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - seq (str): The src sequence\n",
    "        \n",
    "        Returns:\n",
    "          - indices (list): <s> + List of tokens to index mapping + </s>\n",
    "        \"\"\"\n",
    "        indices = [self.src_vocab.sos_idx] \n",
    "        indices.extend([self.src_vocab.lookup_token(t) for t in seq])\n",
    "        indices.append(self.src_vocab.eos_idx)\n",
    "        return indices\n",
    "    \n",
    "    def get_trg_indices(self, seq):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - seq (str): The trg sequence\n",
    "        \n",
    "        Returns:\n",
    "          - trg_x_indices (list): <s> + List of tokens to index mapping\n",
    "          - trg_y_indices (list): List of tokens to index mapping + </s>\n",
    "        \"\"\"\n",
    "        indices = [self.trg_vocab.lookup_token(t) for t in seq]\n",
    "        \n",
    "        trg_x_indices = [self.trg_vocab.sos_idx] + indices\n",
    "        trg_y_indices = indices + [self.trg_vocab.eos_idx]\n",
    "        return trg_x_indices, trg_y_indices\n",
    "        \n",
    "    def vectorize(self, src, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - src (str): The src sequence\n",
    "          - src (str): The trg sequence\n",
    "        Returns:\n",
    "          - vectorized_src \n",
    "          - vectorized_trg_x \n",
    "          - vectorized_trg_y\n",
    "        \"\"\"\n",
    "        src = src\n",
    "        trg = trg\n",
    "        \n",
    "        vectorized_src = self.get_src_indices(src)\n",
    "        vectorized_trg_x, vectorized_trg_y = self.get_trg_indices(trg)\n",
    "        \n",
    "        return {'src': torch.tensor(vectorized_src, dtype=torch.long),\n",
    "                'trg_x': torch.tensor(vectorized_trg_x, dtype=torch.long),\n",
    "                'trg_y': torch.tensor(vectorized_trg_y, dtype=torch.long)\n",
    "               }\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'src_vocab': self.src_vocab.to_serializable(),\n",
    "                'trg_vocab': self.trg_vocab.to_serializable()\n",
    "               }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        src_vocab = SeqVocabulary.from_serializable(contents['src_vocab'])\n",
    "        trg_vocab = SeqVocabulary.from_serializable(contents['trg_vocab'])\n",
    "        return cls(src_vocab, trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MT_Dataset(Dataset):\n",
    "    \"\"\"MT Dataset as a PyTorch dataset\"\"\"\n",
    "    def __init__(self, raw_dataset, vectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.train_examples = raw_dataset.train_examples\n",
    "        self.dev_examples = raw_dataset.dev_examples\n",
    "        self.test_examples = raw_dataset.test_examples\n",
    "        self.lookup_split = {'train': self.train_examples,\n",
    "                             'dev': self.dev_examples,\n",
    "                             'test': self.test_examples}\n",
    "        self.set_split('train')\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "    \n",
    "    @classmethod\n",
    "    def load_data_and_create_vectorizer(cls, data_dir):\n",
    "        raw_dataset = RawDataset(data_dir)\n",
    "        # Note: we always create the vectorized based on the train examples\n",
    "        vectorizer = Vectorizer.create_vectorizer(raw_dataset.train_examples)\n",
    "        return cls(raw_dataset, vectorizer)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_data_and_load_vectorizer(cls, data_dir, vec_path):\n",
    "        raw_dataset = RawDataset(data_dir)\n",
    "        vectorizer = cls.load_vectorizer(vec_path)\n",
    "        return cls(raw_dataset, vectorizer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer(vec_path):\n",
    "        with open(vec_path) as f:\n",
    "            return Vectorizer.from_serializable(json.load(f))\n",
    "    \n",
    "    def save_vectorizer(self, vec_path):\n",
    "        with open(vec_path, 'w') as f:\n",
    "            return json.dump(self.vectorizer.to_serializable(), f)\n",
    "        \n",
    "    def set_split(self, split):\n",
    "        self.split = split\n",
    "        self.split_examples = self.lookup_split[self.split]\n",
    "        return self.split_examples\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        example = self.split_examples[index]\n",
    "        src, trg = example.src, example.trg\n",
    "        vectorized = self.vectorizer.vectorize(src, trg)\n",
    "        return vectorized\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.split_examples)\n",
    "    \n",
    "    \n",
    "class Collator:\n",
    "    def __init__(self, src_pad_idx, trg_pad_idx):\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # Sorting the batch by src seqs length in descending order\n",
    "        sorted_batch = sorted(batch, key=lambda x: x['src'].shape[0], reverse=True)\n",
    "        \n",
    "        src_seqs = [x['src'] for x in sorted_batch]\n",
    "        trg_x_seqs = [x['trg_x'] for x in sorted_batch]\n",
    "        trg_y_seqs = [x['trg_y'] for x in sorted_batch]\n",
    "        lengths = [len(seq) for seq in src_seqs]\n",
    "        \n",
    "        padded_src_seqs = pad_sequence(src_seqs, batch_first=True, padding_value=self.src_pad_idx)\n",
    "        padded_trg_x_seqs = pad_sequence(trg_x_seqs, batch_first=True, padding_value=self.trg_pad_idx)\n",
    "        padded_trg_y_seqs = pad_sequence(trg_y_seqs, batch_first=True, padding_value=self.trg_pad_idx)\n",
    "        lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "        \n",
    "        return {'src': padded_src_seqs,\n",
    "                'trg_x': padded_trg_x_seqs,\n",
    "                'trg_y': padded_trg_y_seqs,\n",
    "                'src_lengths': lengths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder bi-GRU\"\"\"\n",
    "    def __init__(self, input_dim, embed_dim,\n",
    "                 hidd_dim, padding_idx=0):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(input_dim, embed_dim, padding_idx=padding_idx)\n",
    "        self.rnn = nn.GRU(embed_dim, hidd_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, src_seqs, src_seqs_lengths):\n",
    "    \n",
    "        embedded_seqs = self.embedding_layer(src_seqs)\n",
    "        # packing the embedded_seqs\n",
    "        packed_embedded_seqs = pack_padded_sequence(embedded_seqs, src_seqs_lengths, batch_first=True)\n",
    "        \n",
    "        output, hidd = self.rnn(packed_embedded_seqs)\n",
    "        # hidd shape: [num_layers * num_dirs, batch_size, hidd_dim]\n",
    "        \n",
    "        # changing hidd shape to: [batch_size, num_layers * num_dirs, hidd_dim]\n",
    "        hidd = hidd.permute(1, 0 ,2)\n",
    "        \n",
    "        # changing hidd shape to: [batch_size, num_layers * num_dirs * hidd_dim]\n",
    "        hidd = hidd.contiguous().view(hidd.shape[0], -1)\n",
    "        \n",
    "        # unpacking the output\n",
    "        output, lengths = pad_packed_sequence(output, batch_first=True)\n",
    "        # output shape: [batch_size, src_seqs_length, num_dirs * hidd_dim]\n",
    "        return output, hidd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder GRU\n",
    "       \n",
    "       Things to note:\n",
    "           - The input to the decoder rnn at each time step is the \n",
    "             concatenation of the embedded token and the context vector\n",
    "           - The context vector will have a size of batch_size, hidd_dim\n",
    "           - Note that the decoder hidd_dim == the encoder hidd_dim * 2\n",
    "           - The prediction layer input is the concatenation of \n",
    "             the context vector and the h_t of the decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, embed_dim,\n",
    "                 hidd_dim, output_dim,\n",
    "                 padding_idx=0):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidd_dim = hidd_dim\n",
    "        self.embedding_layer = nn.Embedding(input_dim, embed_dim, padding_idx=padding_idx)\n",
    "        # the input to the rnn is the context_vector + embedded token --> embed_dim + hidd_dim\n",
    "        self.rnn = nn.GRUCell((embed_dim + hidd_dim), hidd_dim)\n",
    "        # the input to the classifier is h_t + context_vector --> hidd_dim * 2\n",
    "        self.classification_layer = nn.Linear(hidd_dim * 2, output_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, trg_seqs, encoder_outputs, decoder_h_t, context_vectors):\n",
    "        # trg_seqs shape: [batch_size]\n",
    "        batch_size = trg_seqs.shape[0]\n",
    "\n",
    "        # Step 1: embedding the target seqs\n",
    "        embedded_seqs = self.embedding_layer(trg_seqs)\n",
    "        # embedded_seqs shape: [batch_size, embed_dim]\n",
    "        \n",
    "        # concatenating the embedded trg sequence with the context_vectors\n",
    "        rnn_input = torch.cat((embedded_seqs, context_vectors), dim=1)\n",
    "        # rnn_input shape: [batch_size, embed_dim + hidd_dim]\n",
    "        \n",
    "        # Step 2: feeding the input to the rnn and updating the decoder_h_t\n",
    "        decoder_h_t = self.rnn(rnn_input, decoder_h_t)\n",
    "        # decoder_h_t shape: [batch_size, hidd_dim]\n",
    "\n",
    "        # concatenating decoder_h_t with the context_vectors to create a \n",
    "        # prediction vector\n",
    "        predictions_vector = torch.cat((decoder_h_t, context_vectors), dim=1)\n",
    "        # predictions_vector: [batch_size, hidd_dim * 2]\n",
    "        \n",
    "        # Step 3: feeding the prediction vector to the fc layer\n",
    "        # to a make a prediction\n",
    "        prediction = self.classification_layer(predictions_vector)\n",
    "        # prediction shape: [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, decoder_h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Decoder(nn.Module):\n",
    "#     \"\"\"Decoder GRU\n",
    "       \n",
    "#        Things to note:\n",
    "#            - The input to the decoder rnn at each time step is the \n",
    "#              concatenation of the embedded token and the context vector\n",
    "#            - The context vector will have a size of batch_size, hidd_dim\n",
    "#            - Note that the decoder hidd_dim == the encoder hidd_dim * 2\n",
    "#            - The prediction layer input is the concatenation of \n",
    "#              the context vector and the h_t of the decoder\n",
    "#     \"\"\"\n",
    "#     def __init__(self, input_dim, embed_dim,\n",
    "#                  hidd_dim, output_dim,\n",
    "#                  padding_idx=0):\n",
    "        \n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.hidd_dim = hidd_dim\n",
    "#         self.embedding_layer = nn.Embedding(input_dim, embed_dim, padding_idx=padding_idx)\n",
    "#         # the input to the rnn is the context_vector + embedded token --> embed_dim + hidd_dim\n",
    "#         self.rnn = nn.GRU((embed_dim + hidd_dim), hidd_dim, batch_first=True)\n",
    "#         # the input to the classifier is h_t + context_vector --> hidd_dim * 2\n",
    "#         self.classification_layer = nn.Linear(hidd_dim * 2, output_dim)\n",
    "        \n",
    "    \n",
    "#     def forward(self, trg_seqs, encoder_outputs, decoder_h_t, context_vectors):\n",
    "#         # trg_seqs shape: [batch_size]\n",
    "#         batch_size = trg_seqs.shape[0]\n",
    "\n",
    "#         # Step 1: embedding the target seqs\n",
    "#         embedded_seqs = self.embedding_layer(trg_seqs)\n",
    "#         # embedded_seqs shape: [batch_size, embed_dim]\n",
    "        \n",
    "#         # concatenating the embedded trg sequence with the context_vectors\n",
    "#         rnn_input = torch.cat((embedded_seqs, context_vectors), dim=1)\n",
    "#         # rnn_input shape: [batch_size, embed_dim + hidd_dim]\n",
    "        \n",
    "#         # the GRU expects an input of dimension [batch_size, sequence_len, embed_dim + hidd_dim]\n",
    "#         # since we have a single token, the sequence_len will be 1\n",
    "#         rnn_input = rnn_input.unsqueeze(1)\n",
    "#         # rnn_input shape: [batch_size, 1, embed_dim + hidd_dim]\n",
    "        \n",
    "#         # the GRU also expects a hidden state of dimension \n",
    "#         # [num_layers * num_dirs, batch_size, hidd_dim]\n",
    "#         # since we have a single layer and a forward GRU,\n",
    "#         # num_layers * num_dirs will be 1\n",
    "#         decoder_h_t = decoder_h_t.unsqueeze(0)\n",
    "#         # decoder_h_t shape: [1, batch_size, hidd_dim]\n",
    "        \n",
    "#         # Step 2: feeding the input to the rnn and updating the decoder_h_t\n",
    "#         output, decoder_h_t = self.rnn(rnn_input, decoder_h_t)\n",
    "#         # output shape: [batch, 1, hidd_dim]\n",
    "#         # decoder_h_t shape: [1, batch_size, hidd_dim]\n",
    "        \n",
    "#         decoder_h_t = decoder_h_t.squeeze(0)\n",
    "#         # decoder_h_t shape: [batch_size, hidd_dim]\n",
    "        \n",
    "#         # concatenating decoder_h_t with the context_vectors to create a \n",
    "#         # prediction vector\n",
    "#         predictions_vector = torch.cat((decoder_h_t, context_vectors), dim=1)\n",
    "#         # predictions_vector: [batch_size, hidd_dim * 2]\n",
    "        \n",
    "#         # Step 3: feeding the prediction vector to the fc layer\n",
    "#         # to a make a prediction\n",
    "#         prediction = self.classification_layer(predictions_vector)\n",
    "#         # prediction shape: [batch_size, output_dim]\n",
    "        \n",
    "#         return prediction, decoder_h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"Attention mechanism as a MLP \n",
    "    as used by Bahdanau et. al 2015\"\"\"\n",
    "\n",
    "    def __init__(self, encoder_hidd_dim, decoder_hidd_dim):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.atten = nn.Linear((encoder_hidd_dim * 2) + decoder_hidd_dim, decoder_hidd_dim)\n",
    "        self.v = nn.Linear(decoder_hidd_dim, 1)\n",
    "        \n",
    "    def forward(self, key_vectors, query_vector, mask):\n",
    "        \"\"\"key_vectors: encoder hidden states.\n",
    "           query_vector: decoder hidden state at time t\n",
    "           mask: the mask vector of zeros and ones\n",
    "        \"\"\"\n",
    "        \n",
    "        #key_vectors shape: [batch_size, src_seq_length, encoder_hidd_dim * 2]\n",
    "        #query_vector shape: [batch_size, decoder_hidd_dim]\n",
    "        #Note: encoder_hidd_dim * 2 == decoder_hidd_dim\n",
    "        \n",
    "        batch_size, src_seq_length, encoder_hidd_dim = key_vectors.shape\n",
    "        \n",
    "        #changing the shape of query_vector to [batch_size, src_seq_length, decoder_hidd_dim]\n",
    "        #we will repeat the query_vector src_seq_length times at dim 1\n",
    "        query_vector = query_vector.unsqueeze(1).repeat(1, src_seq_length, 1)\n",
    "        \n",
    "        # Step 1: Compute the attention scores through a MLP\n",
    "        \n",
    "        # concatenating the key_vectors and the query_vector\n",
    "        atten_input = torch.cat((key_vectors, query_vector), dim=2)\n",
    "        # atten_input shape: [batch_size, src_seq_length, (encoder_hidd_dim * 2) + decoder_hidd_dim]\n",
    "        \n",
    "        atten_scores = self.atten(atten_input)\n",
    "        # atten_scores shape: [batch_size, src_seq_length, decoder_hidd_dim]\n",
    "\n",
    "        atten_scores = torch.tanh(atten_scores)\n",
    "    \n",
    "        # mapping atten_scores from decoder_hidd_dim to 1\n",
    "        atten_scores = self.v(atten_scores)\n",
    "    \n",
    "        # atten_scores shape: [batch_size, src_seq_length, 1]\n",
    "        atten_scores = atten_scores.squeeze(dim=2)\n",
    "        # atten_scores shape: [batch_size, src_seq_length]\n",
    "        \n",
    "        # masking the atten_scores\n",
    "        atten_scores = atten_scores.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # Step 2: normalizing atten_scores through a softmax to get probs\n",
    "        atten_scores = F.softmax(atten_scores, dim=1)\n",
    "        \n",
    "        # Step 3: computing the new context vector\n",
    "        context_vectors = torch.matmul(key_vectors.permute(0, 2, 1), atten_scores.unsqueeze(2)).squeeze(dim=2)\n",
    "        \n",
    "        # context_vectors shape: [batch_size, encoder_hidd_dim * 2]\n",
    "        \n",
    "        return context_vectors, atten_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Seq2Seq model\"\"\"\n",
    "    def __init__(self, encoder_input_dim, encoder_embed_dim,\n",
    "                 encoder_hidd_dim, decoder_input_dim, \n",
    "                 decoder_embed_dim, decoder_output_dim, \n",
    "                 src_padding_idx=0, trg_padding_idx=0, trg_sos_idx=2):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(input_dim=encoder_input_dim,\n",
    "                               embed_dim=encoder_embed_dim,\n",
    "                               hidd_dim=encoder_hidd_dim,\n",
    "                               padding_idx=src_padding_idx)\n",
    "        \n",
    "        self.decoder_hidd_dim = encoder_hidd_dim * 2\n",
    "        \n",
    "        self.attention = AdditiveAttention(encoder_hidd_dim=encoder_hidd_dim,\n",
    "                                           decoder_hidd_dim=self.decoder_hidd_dim)\n",
    "        \n",
    "        self.decoder = Decoder(input_dim=decoder_input_dim,\n",
    "                               embed_dim=decoder_embed_dim,\n",
    "                               hidd_dim=self.decoder_hidd_dim,\n",
    "                               output_dim=decoder_input_dim,\n",
    "                               padding_idx=trg_padding_idx)\n",
    "        \n",
    "        self.src_padding_idx = src_padding_idx\n",
    "        self.trg_sos_idx = trg_sos_idx\n",
    "        self.sampling_temperature = 3\n",
    "        \n",
    "    def create_mask(self, src_seqs, src_padding_idx):\n",
    "        mask = (src_seqs != src_padding_idx)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src_seqs, src_seqs_lengths, trg_seqs, teacher_forcing_prob=0.3):\n",
    "        # trg_seqs shape: [batch_size, trg_seqs_length]\n",
    "        # reshaping to: [trg_seqs_length, batch_size]\n",
    "        trg_seqs = trg_seqs.permute(1, 0)\n",
    "        trg_seqs_length, batch_size = trg_seqs.shape\n",
    "        \n",
    "        # passing the src to the encoder\n",
    "        encoder_outputs, encoder_hidd = self.encoder(src_seqs, src_seqs_lengths)\n",
    "        \n",
    "        # creating attention masks\n",
    "        attention_mask = self.create_mask(src_seqs, self.src_padding_idx)\n",
    "\n",
    "        predictions = []\n",
    "        decoder_attention_scores = []\n",
    "        \n",
    "        # initializing the trg_seqs to <s> token\n",
    "        y_t = torch.ones(batch_size, dtype=torch.long) * self.trg_sos_idx\n",
    "        \n",
    "        # intializing the context_vectors to zero\n",
    "        context_vectors = torch.zeros(batch_size, self.decoder_hidd_dim)\n",
    "        \n",
    "        # initializing the hidden state of the decoder to the encoder hidden state\n",
    "        decoder_h_t = encoder_hidd\n",
    "        \n",
    "        # moving y_t and context_vectors to the right device\n",
    "        y_t = y_t.to(encoder_hidd.device)\n",
    "        context_vectors = context_vectors.to(encoder_hidd.device)\n",
    "        \n",
    "        for i in range(trg_seqs_length):\n",
    "            teacher_forcing = np.random.random() < teacher_forcing_prob\n",
    "            # if teacher_forcing, use ground truth target tokens\n",
    "            # as an input to the decoder\n",
    "            if teacher_forcing:\n",
    "                y_t = trg_seqs[i]\n",
    "            \n",
    "            # do a single decoder step\n",
    "            prediction, decoder_h_t = self.decoder(y_t, encoder_outputs, decoder_h_t,\n",
    "                                                   context_vectors)\n",
    "            \n",
    "            # updating the context_vectors \n",
    "            context_vectors, atten_scores = self.attention(key_vectors=encoder_outputs, query_vector=decoder_h_t,\n",
    "                                                           mask=attention_mask)\n",
    "            # If not teacher force, use the maximum \n",
    "            # prediction as an input to the decoder in \n",
    "            # the next time step\n",
    "            if not teacher_forcing:\n",
    "                # we multiply the predictions with a sampling_temperature\n",
    "                # to make the propablities peakier, so we can be confident about the\n",
    "                # maximum prediction\n",
    "                pred_output_probs = F.softmax(prediction * self.sampling_temperature, dim=1)\n",
    "                y_t = torch.argmax(pred_output_probs, dim=1)\n",
    "\n",
    "            predictions.append(prediction)\n",
    "            decoder_attention_scores.append(atten_scores)\n",
    "        \n",
    "        \n",
    "        predictions = torch.stack(predictions)\n",
    "        # predictions shape: [trg_seq_len, batch_size, output_dim]\n",
    "        predictions = predictions.permute(1, 0, 2)\n",
    "        # predictions shape: [batch_size, trg_seq_len, output_dim]\n",
    "    \n",
    "    \n",
    "        decoder_attention_scores = torch.stack(decoder_attention_scores)\n",
    "        # attention_scores_total shape: [trg_seq_len, batch_size, src_seq_len]\n",
    "        decoder_attention_scores = decoder_attention_scores.permute(1, 0, 2)\n",
    "        # attention_scores_total shape: [batch_size, trg_seq_len, src_seq_len]\n",
    "        \n",
    "        return predictions, decoder_attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, cuda):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(data_dir='/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus',\n",
    "                          vectorizer_path='/home/ba63/gender-bias/models/saved_models/char_level_vectorizer.json',\n",
    "                          reload_files=False,\n",
    "                          cache_files=False,\n",
    "                          num_epochs=50,\n",
    "                          embedding_dim=128,\n",
    "                          hidd_dim=256,\n",
    "                          learning_rate=5e-4,\n",
    "                          use_cuda=True,\n",
    "                          batch_size=64,\n",
    "                          seed=21,\n",
    "                          model_path='/home/ba63/gender-bias/models/saved_models/char_level_model_small.pt'\n",
    "                          )\n",
    "\n",
    "device = torch.device('cuda' if args.use_cuda else 'cpu')\n",
    "set_seed(args.seed, args.use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.reload_files:\n",
    "    dataset = MT_Dataset.load_data_and_load_vectorizer(args.data_dir, args.vectorizer_path)\n",
    "else:\n",
    "    dataset = MT_Dataset.load_data_and_create_vectorizer(args.data_dir)\n",
    "\n",
    "if args.cache_files:\n",
    "    dataset.save_vectorizer(args.vectorizer_path)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "ENCODER_INPUT_DIM = len(vectorizer.src_vocab)\n",
    "DECODER_INPUT_DIM = len(vectorizer.trg_vocab)\n",
    "DECODER_OUTPUT_DIM = len(vectorizer.trg_vocab)\n",
    "SRC_PAD_INDEX = vectorizer.src_vocab.pad_idx\n",
    "TRG_PAD_INDEX = vectorizer.trg_vocab.pad_idx\n",
    "TRG_SOS_INDEX = vectorizer.trg_vocab.sos_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding_layer): Embedding(71, 128, padding_idx=0)\n",
       "    (rnn): GRU(128, 256, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (attention): AdditiveAttention(\n",
       "    (atten): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (v): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding_layer): Embedding(71, 128, padding_idx=0)\n",
       "    (rnn): GRUCell(640, 512)\n",
       "    (classification_layer): Linear(in_features=1024, out_features=71, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2Seq(encoder_input_dim=ENCODER_INPUT_DIM,\n",
    "                encoder_embed_dim=args.embedding_dim,\n",
    "                encoder_hidd_dim=args.hidd_dim,\n",
    "                decoder_input_dim=DECODER_INPUT_DIM,\n",
    "                decoder_embed_dim=args.embedding_dim,\n",
    "                decoder_output_dim=DECODER_OUTPUT_DIM,\n",
    "                src_padding_idx=SRC_PAD_INDEX,\n",
    "                trg_padding_idx=TRG_PAD_INDEX,\n",
    "                trg_sos_idx=TRG_SOS_INDEX)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_INDEX)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                 patience=2, factor=0.5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device='cpu', teacher_forcing_prob=0.3):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        src = batch['src']\n",
    "        trg_x = batch['trg_x']\n",
    "        trg_y = batch['trg_y']\n",
    "        src_lengths = batch['src_lengths']\n",
    "\n",
    "        preds, attention_scores = model(src, src_lengths, trg_x, teacher_forcing_prob=teacher_forcing_prob)\n",
    "        \n",
    "        # CrossEntropyLoss accepts matrices always! \n",
    "        # the preds must be of size (N, C) where C is the number \n",
    "        # of classes and N is the number of samples. \n",
    "        # The ground truth must be a Vector of size C!\n",
    "        preds = preds.contiguous().view(-1, preds.shape[-1])\n",
    "        trg_y = trg_y.view(-1)\n",
    "\n",
    "        loss = criterion(preds, trg_y)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device='cpu', teacher_forcing_prob=0):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            src = batch['src']\n",
    "            trg_x = batch['trg_x']\n",
    "            trg_y = batch['trg_y']\n",
    "            src_lengths = batch['src_lengths']\n",
    "            \n",
    "            # we turn off teacher_forcing during evaluation\n",
    "            preds, attention_scores = model(src, src_lengths, trg_x, teacher_forcing_prob=teacher_forcing_prob)\n",
    "            # CrossEntropyLoss accepts matrices always! \n",
    "            # the preds must be of size (N, C) where C is the number \n",
    "            # of classes and N is the number of samples. \n",
    "            # The ground truth must be a Vector of size C!\n",
    "            preds = preds.contiguous().view(-1, preds.shape[-1])\n",
    "            trg_y = trg_y.view(-1)\n",
    "            \n",
    "            loss = criterion(preds, trg_y)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2064dfab41c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcollator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCollator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC_PAD_INDEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRG_PAD_INDEX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "collator = Collator(SRC_PAD_INDEX, TRG_PAD_INDEX)\n",
    "best_loss = 1e10\n",
    "set_seed(args.seed, args.use_cuda)\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "print(f'Using {device}')\n",
    "for epoch in range(args.num_epochs):\n",
    "#     teacher_forcing_prob = (epoch + 5) / args.num_epochs\n",
    "    teacher_forcing_prob = 0.3\n",
    "    dataset.set_split('train')\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=args.batch_size, collate_fn=collator, drop_last=True)\n",
    "    train_loss = train(model, dataloader, optimizer, criterion, device, teacher_forcing_prob=teacher_forcing_prob)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    dataset.set_split('dev')\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=args.batch_size, collate_fn=collator, drop_last=True)\n",
    "    dev_loss = evaluate(model, dataloader, criterion, device, teacher_forcing_prob=0)\n",
    "    dev_losses.append(dev_loss)\n",
    "    \n",
    "    #save best model\n",
    "    if dev_loss < best_loss:\n",
    "        best_loss = dev_loss\n",
    "        torch.save(model.state_dict(), args.model_path)\n",
    "    \n",
    "    # calling the scheduler\n",
    "    scheduler.step(dev_loss)\n",
    "\n",
    "    print(f'Epoch: {(epoch + 1)}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f}   |   Dev Loss: {dev_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, 1 + args.num_epochs), np.asarray(train_losses), 'b-', color='blue', label='Training')\n",
    "plt.plot(range(1, 1 + args.num_epochs), np.asarray(dev_losses), 'b-', color='orange', label='Evaluation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMT_Batch_Sampler:\n",
    "    def __init__(self, model, src_vocab, trg_vocab):\n",
    "        self.model = model\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "    \n",
    "    def update_batch(self, batch):\n",
    "        self.sample_batch = batch\n",
    "        \n",
    "        src = batch['src']\n",
    "        trg_x = batch['trg_x']\n",
    "        trg_y = batch['trg_y']\n",
    "        src_lengths = batch['src_lengths']\n",
    "        preds, attention_scores = self.model(src, src_lengths, trg_x, teacher_forcing_prob=0)\n",
    "        # preds shape: [batch_size, trg_seq_len, output_dim]\n",
    "        \n",
    "        self.sample_batch['preds'] = preds\n",
    "        self.sample_batch['attention_scores'] = attention_scores\n",
    "        return self.sample_batch\n",
    "    \n",
    "    def get_pred_sentence(self, index):\n",
    "        preds = self.sample_batch['preds']\n",
    "       \n",
    "        max_preds = torch.argmax(preds, dim=2)\n",
    "        # max_preds shape: [batch_size, trg_seq_len]\n",
    "        max_pred_sentence = max_preds[index].cpu().detach().numpy()\n",
    "        return self.get_str_sentence(max_pred_sentence, self.trg_vocab)\n",
    "    \n",
    "    def get_trg_sentence(self, index):\n",
    "        trg_sentence = self.sample_batch['trg_y'][index].cpu().detach().numpy()\n",
    "        return self.get_str_sentence(trg_sentence, self.trg_vocab)\n",
    "    \n",
    "    def get_src_sentence(self, index):\n",
    "        src_sentence = self.sample_batch['src'][index].cpu().detach().numpy()\n",
    "        return self.get_str_sentence(src_sentence, self.src_vocab)\n",
    "    \n",
    "    def get_str_sentence(self, vectorized_sentence, vocab):\n",
    "        sentence = []\n",
    "        for i in vectorized_sentence:\n",
    "            if i == vocab.sos_idx:\n",
    "                continue\n",
    "            elif i == vocab.eos_idx:\n",
    "                break\n",
    "            else:\n",
    "                sentence.append(vocab.lookup_index(i))\n",
    "        return ''.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed, args.use_cuda)\n",
    "dataset.set_split('train')\n",
    "model.load_state_dict(torch.load(args.model_path))\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "collator = Collator(SRC_PAD_INDEX, TRG_PAD_INDEX)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collator)\n",
    "sampler = NMT_Batch_Sampler(model, vectorizer.src_vocab, vectorizer.trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_preds = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/edits_annotations/char_level_model_small.train_preds', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    updated_batch = sampler.update_batch(batch)\n",
    "\n",
    "#     print(updated_batch['trg_x'])\n",
    "#     print(updated_batch['trg_y'])\n",
    "#     print(updated_batch['src'])\n",
    "    src = sampler.get_src_sentence(0)\n",
    "    trg = sampler.get_trg_sentence(0)\n",
    "    pred = sampler.get_pred_sentence(0)\n",
    "    \n",
    "    new_train_preds.write(pred)\n",
    "#     print(src)\n",
    "#     print(trg)\n",
    "#     print(pred)\n",
    "    new_train_preds.write('\\n')\n",
    "#     attention_scores = updated_batch['attention_scores'][0].cpu().detach().numpy()\n",
    "#     fig = plt.figure(figsize=(10,10))\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     cax = ax.matshow(attention_scores, cmap='bone')\n",
    "#     fig.colorbar(cax)\n",
    "\n",
    "#     # Set up axes\n",
    "#     ax.set_xticklabels(['','<s>'] + src.split(' ') +\n",
    "#                    ['</s>'], rotation=90)\n",
    "#     ax.set_yticklabels([''] + pred.split(' ') + ['</s>'])\n",
    "\n",
    "#     # Show label at every tick\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#     ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "new_train_preds.close()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed, args.use_cuda)\n",
    "dataset.set_split('dev')\n",
    "model.load_state_dict(torch.load(args.model_path))\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "collator = Collator(SRC_PAD_INDEX, TRG_PAD_INDEX)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collator)\n",
    "sampler = NMT_Batch_Sampler(model, vectorizer.src_vocab, vectorizer.trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dev_preds = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/edits_annotations/char_level_model_small.dev_preds', 'w')\n",
    "\n",
    "dev_log = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/edits_annotations/char_level_model_small.dev_log', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    updated_batch = sampler.update_batch(batch)\n",
    "\n",
    "#     print(updated_batch['trg_x'])\n",
    "#     print(updated_batch['trg_y'])\n",
    "#     print(updated_batch['src'])\n",
    "    src = sampler.get_src_sentence(0)\n",
    "    trg = sampler.get_trg_sentence(0)\n",
    "    pred = sampler.get_pred_sentence(0)\n",
    "    \n",
    "    new_dev_preds.write(pred)\n",
    "#     print(src)\n",
    "#     print(trg)\n",
    "#     print(pred)\n",
    "    new_dev_preds.write('\\n')\n",
    "    \n",
    "    dev_log.write(f'src: ' + src)\n",
    "    dev_log.write('\\n')\n",
    "    dev_log.write(f'trg: ' + trg)\n",
    "    dev_log.write('\\n')\n",
    "    dev_log.write(f'pred: ' + pred)\n",
    "    dev_log.write('\\n\\n')\n",
    "#     attention_scores = updated_batch['attention_scores'][0].cpu().detach().numpy()\n",
    "#     fig = plt.figure(figsize=(10,10))\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     cax = ax.matshow(attention_scores, cmap='bone')\n",
    "#     fig.colorbar(cax)\n",
    "\n",
    "#     # Set up axes\n",
    "#     ax.set_xticklabels(['','<s>'] + src.split(' ') +\n",
    "#                    ['</s>'], rotation=90)\n",
    "#     ax.set_yticklabels([''] + pred.split(' ') + ['</s>'])\n",
    "\n",
    "#     # Show label at every tick\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#     ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     plt.show()\n",
    "dev_log.close()\n",
    "new_dev_preds.close()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
