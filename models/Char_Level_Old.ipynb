{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample:\n",
    "    \"\"\"Simple object to encapsulate each data example\"\"\"\n",
    "    def __init__(self, src, trg, \n",
    "                 src_g, trg_g):    \n",
    "        self.src = src\n",
    "        self.trg = trg\n",
    "        self.src_g = src_g\n",
    "        self.trg_g = trg_g\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_str())\n",
    "    \n",
    "    def to_json_str(self):\n",
    "        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)\n",
    "    \n",
    "    def to_dict(self):\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataset:\n",
    "    \"\"\"Encapsulates the raw examples in InputExample objects\"\"\"\n",
    "    def __init__(self, data_dir):\n",
    "        self.train_examples = self.get_train_examples(data_dir)\n",
    "        self.dev_examples = self.get_dev_examples(data_dir)\n",
    "        self.test_examples = self.get_dev_examples(data_dir)\n",
    "        \n",
    "    def create_examples(self, src_path, trg_path):\n",
    "        \n",
    "        src_txt = self.get_txt_examples(src_path)\n",
    "        src_gender_labels = self.get_labels(src_path + '.label')\n",
    "        trg_txt = self.get_txt_examples(trg_path)\n",
    "        trg_gender_labels = self.get_labels(trg_path + '.label')\n",
    "        \n",
    "        examples = []\n",
    "        \n",
    "        for i in range(len(src_txt)):\n",
    "            src = src_txt[i].strip()\n",
    "            trg = trg_txt[i].strip()\n",
    "            src_g = src_gender_labels[i].strip()\n",
    "            trg_g = trg_gender_labels[i].strip()\n",
    "            input_example = InputExample(src, trg, src_g, trg_g)\n",
    "            examples.append(input_example)\n",
    "        \n",
    "        return examples\n",
    "    \n",
    "    def get_labels(self, data_dir):\n",
    "        with open(data_dir) as f:\n",
    "            return f.readlines()\n",
    "        \n",
    "    def get_txt_examples(self, data_dir):\n",
    "        with open(data_dir, encoding='utf8') as f:\n",
    "            return f.readlines()\n",
    "    \n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Reads the train examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-train.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-train.ar.M'))\n",
    "    \n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Reads the dev examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-dev.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-dev.ar.M'))\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Reads the test examples of the dataset\"\"\"\n",
    "        return self.create_examples(os.path.join(data_dir, 'D-set-test.arin'), \n",
    "                                    os.path.join(data_dir, 'D-set-test.ar.M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Base vocabulary class\"\"\"\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = dict()\n",
    "        \n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        return self.idx_to_token[index]\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self.token_to_idx}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "    \n",
    "class SeqVocabulary(Vocabulary):\n",
    "    \"\"\"Sequence vocabulary class\"\"\"\n",
    "    def __init__(self, token_to_idx=None, unk_token='<unk>',\n",
    "                 pad_token='<pad>', sos_token='<s>',\n",
    "                 eos_token='</s>'):\n",
    "        \n",
    "        super(SeqVocabulary, self).__init__(token_to_idx)\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        \n",
    "        self.pad_idx = self.add_token(self.pad_token)\n",
    "        self.unk_idx = self.add_token(self.unk_token)\n",
    "        self.sos_idx = self.add_token(self.sos_token)\n",
    "        self.eos_idx = self.add_token(self.eos_token)\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        contents = super(SeqVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self.unk_token,\n",
    "                         'pad_token': self.pad_token,\n",
    "                         'sos_token': self.sos_token, \n",
    "                         'eos_token': self.eos_token})\n",
    "        return contents\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx.get(token, self.unk_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    \"\"\"Vectorizer Class\"\"\"\n",
    "    def __init__(self, src_vocab, trg_vocab):\n",
    "        \"\"\"src_vocab and trg_vocab are on the char\n",
    "        level\"\"\"\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "    \n",
    "    @classmethod\n",
    "    def create_vectorizer(cls, data_examples):\n",
    "        \"\"\"Class method which builds the vectorizer\n",
    "        vocab\"\"\"\n",
    "        \n",
    "        src_vocab = SeqVocabulary()\n",
    "        trg_vocab = SeqVocabulary()\n",
    "        \n",
    "        for ex in data_examples:\n",
    "            src = ex.src\n",
    "            trg = ex.trg\n",
    "            \n",
    "            for t in src:\n",
    "                src_vocab.add_token(t)\n",
    "                \n",
    "            for t in trg:\n",
    "                trg_vocab.add_token(t)\n",
    "        \n",
    "        return cls(src_vocab, trg_vocab)\n",
    "    \n",
    "    def get_src_indices(self, seq):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - seq (str): The src sequence\n",
    "        \n",
    "        Returns:\n",
    "          - indices (list): <s> + List of tokens to index mapping + </s>\n",
    "        \"\"\"\n",
    "        indices = [self.src_vocab.sos_idx] \n",
    "        indices.extend([self.src_vocab.lookup_token(t) for t in seq])\n",
    "        indices.append(self.src_vocab.eos_idx)\n",
    "        return indices\n",
    "    \n",
    "    def get_trg_indices(self, seq):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - seq (str): The trg sequence\n",
    "        \n",
    "        Returns:\n",
    "          - trg_x_indices (list): <s> + List of tokens to index mapping\n",
    "          - trg_y_indices (list): List of tokens to index mapping + </s>\n",
    "        \"\"\"\n",
    "        indices = [self.trg_vocab.lookup_token(t) for t in seq]\n",
    "        \n",
    "        trg_x_indices = [self.trg_vocab.sos_idx] + indices\n",
    "        trg_y_indices = indices + [self.trg_vocab.eos_idx]\n",
    "        return trg_x_indices, trg_y_indices\n",
    "        \n",
    "    def vectorize(self, src, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - src (str): The src sequence\n",
    "          - src (str): The trg sequence\n",
    "        Returns:\n",
    "          - vectorized_src \n",
    "          - vectorized_trg_x \n",
    "          - vectorized_trg_y\n",
    "        \"\"\"\n",
    "        src = src\n",
    "        trg = trg\n",
    "        \n",
    "        vectorized_src = self.get_src_indices(src)\n",
    "        vectorized_trg_x, vectorized_trg_y = self.get_trg_indices(trg)\n",
    "        \n",
    "        return {'src': torch.tensor(vectorized_src, dtype=torch.long),\n",
    "                'trg_x': torch.tensor(vectorized_trg_x, dtype=torch.long),\n",
    "                'trg_y': torch.tensor(vectorized_trg_y, dtype=torch.long)\n",
    "               }\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'src_vocab': self.src_vocab.to_serializable(),\n",
    "                'trg_vocab': self.trg_vocab.to_serializable()\n",
    "               }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        src_vocab = SeqVocabulary.from_serializable(contents['src_vocab'])\n",
    "        trg_vocab = SeqVocabulary.from_serializable(contents['trg_vocab'])\n",
    "        return cls(src_vocab, trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MT_Dataset(Dataset):\n",
    "    \"\"\"MT Dataset as a PyTorch dataset\"\"\"\n",
    "    def __init__(self, raw_dataset, vectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.train_examples = raw_dataset.train_examples\n",
    "        self.dev_examples = raw_dataset.dev_examples\n",
    "        self.test_examples = raw_dataset.test_examples\n",
    "        self.lookup_split = {'train': self.train_examples,\n",
    "                             'dev': self.dev_examples,\n",
    "                             'test': self.test_examples}\n",
    "        self.set_split('train')\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "    \n",
    "    @classmethod\n",
    "    def load_data_and_create_vectorizer(cls, data_dir):\n",
    "        raw_dataset = RawDataset(data_dir)\n",
    "        # Note: we always create the vectorized based on the train examples\n",
    "        vectorizer = Vectorizer.create_vectorizer(raw_dataset.train_examples)\n",
    "        return cls(raw_dataset, vectorizer)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_data_and_load_vectorizer(cls, data_dir, vec_path):\n",
    "        raw_dataset = RawDataset(data_dir)\n",
    "        vectorizer = cls.load_vectorizer(vec_path)\n",
    "        return cls(raw_dataset, vectorizer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer(vec_path):\n",
    "        with open(vec_path) as f:\n",
    "            return Vectorizer.from_serializable(json.load(f))\n",
    "    \n",
    "    def save_vectorizer(self, vec_path):\n",
    "        with open(vec_path, 'w') as f:\n",
    "            return json.dump(self.vectorizer.to_serializable(), f)\n",
    "        \n",
    "    def set_split(self, split):\n",
    "        self.split = split\n",
    "        self.split_examples = self.lookup_split[self.split]\n",
    "        return self.split_examples\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        example = self.split_examples[index]\n",
    "        src, trg = example.src, example.trg\n",
    "        vectorized = self.vectorizer.vectorize(src, trg)\n",
    "        return vectorized\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.split_examples)\n",
    "    \n",
    "    \n",
    "class Collator:\n",
    "    def __init__(self, src_pad_idx, trg_pad_idx):\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # Sorting the batch by src seqs length in descending order\n",
    "        sorted_batch = sorted(batch, key=lambda x: x['src'].shape[0], reverse=True)\n",
    "        \n",
    "        src_seqs = [x['src'] for x in sorted_batch]\n",
    "        trg_x_seqs = [x['trg_x'] for x in sorted_batch]\n",
    "        trg_y_seqs = [x['trg_y'] for x in sorted_batch]\n",
    "        lengths = [len(seq) for seq in src_seqs]\n",
    "        \n",
    "        padded_src_seqs = pad_sequence(src_seqs, batch_first=True, padding_value=self.src_pad_idx)\n",
    "        padded_trg_x_seqs = pad_sequence(trg_x_seqs, batch_first=True, padding_value=self.trg_pad_idx)\n",
    "        padded_trg_y_seqs = pad_sequence(trg_y_seqs, batch_first=True, padding_value=self.trg_pad_idx)\n",
    "        lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "        \n",
    "        return {'src': padded_src_seqs,\n",
    "                'trg_x': padded_trg_x_seqs,\n",
    "                'trg_y': padded_trg_y_seqs,\n",
    "                'src_lengths': lengths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder biGRU\"\"\"\n",
    "    def __init__(self, input_dim, embed_dim, \n",
    "                 hidd_dim, padding_idx=0):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(input_dim, embed_dim, padding_idx=padding_idx)\n",
    "        self.rnn = nn.GRU(embed_dim, hidd_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, src_seqs, src_lengths):\n",
    "\n",
    "        embedded_seqs = self.embedding_layer(src_seqs)\n",
    "\n",
    "        packed_seqs = pack_padded_sequence(embedded_seqs, src_lengths, batch_first=True)\n",
    "        \n",
    "        output, h_t = self.rnn(packed_seqs)\n",
    "        \n",
    "        #output is a packed_sequence\n",
    "        #h_t shape: [num_layers * num_dirs, batch_size, hidd_dim]\n",
    "        \n",
    "        #reshaping h_t to [batch_size, num_layers * num_dirs * hidd_dim]\n",
    "    \n",
    "        h_t = h_t.permute(1, 0, 2) #[batch_size, num_layers * num_dirs, hidd_dim]\n",
    "        #Note: when we call permute, the contiguity of a tensor is lost,\n",
    "        #so we have to call contiguous before reshaping!\n",
    "        h_t = h_t.contiguous().view(h_t.shape[0], -1) #[batch_size, num_layers * num_dirs * hidd_dim]\n",
    "        \n",
    "        #unpacking output\n",
    "        unpacked_output, lengths = pad_packed_sequence(output, batch_first=True)\n",
    "        #output shape: [batch_size, src_seq_length, hidd_dim * num_dirs]\n",
    "        \n",
    "        return unpacked_output, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder GRU\n",
    "       \n",
    "       Things to note:\n",
    "           - The input to the decoder rnn at each time step is the \n",
    "             concatenation of the embedded token and the context vector\n",
    "           - The context vector will have a size of batch_size, hidd_dim\n",
    "           - Note that the decoder hidd_dim == the encoder hidd_dim * 2\n",
    "           - The prediction layer input is the concatenation of \n",
    "             the context vector and the h_t of the decoder\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, embed_dim,\n",
    "                 hidd_dim, output_dim,\n",
    "                 attention,\n",
    "                 padding_idx=0,\n",
    "                 sos_idx=2,\n",
    "                 eos_idx=3):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidd_dim = hidd_dim\n",
    "        self.sos_idx = sos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.attention = attention\n",
    "        self.embedding_layer = nn.Embedding(input_dim, embed_dim, padding_idx=padding_idx)\n",
    "        # the input to the rnn is the context_vector + embedded token --> embed_dim + hidd_dim\n",
    "        self.rnn = nn.GRUCell(embed_dim + hidd_dim, hidd_dim)\n",
    "        # the input to the classifier is h_t + context_vector --> hidd_dim * 2\n",
    "        self.classifier = nn.Linear(hidd_dim * 2, output_dim)\n",
    "        \n",
    "        # sampling temperature\n",
    "        self.sampling_temperature = 3\n",
    " \n",
    "    def forward(self, trg_seqs, encoder_outputs, encoder_h_t, mask, \n",
    "                teacher_forcing_prob=0.3, \n",
    "                inference=False,\n",
    "                max_len=200):\n",
    "        \n",
    "        # if we're doing inference\n",
    "        if trg_seqs is None:\n",
    "            teacher_forcing_prob = 0\n",
    "            trg_seq_len = max_len\n",
    "            batch_size = encoder_outputs.shape[0]\n",
    "            inference = True\n",
    "        else:\n",
    "            # reshaping trg_seqs to: [trg_seq_len, batch_size]\n",
    "            trg_seqs = trg_seqs.permute(1, 0)\n",
    "            trg_seq_len, batch_size = trg_seqs.shape\n",
    "        \n",
    "        # initializing the context_vectors to zeros\n",
    "        context_vectors = torch.zeros(batch_size, self.hidd_dim)\n",
    "        # moving the context_vectors to the right device\n",
    "        context_vectors = context_vectors.to(encoder_h_t.device)\n",
    "\n",
    "        # initializing the hidden state to the encoder hidden state\n",
    "        h_t = encoder_h_t\n",
    "        \n",
    "        # initializing the first trg input to the <sos> token\n",
    "        y_t = torch.ones(batch_size, dtype=torch.long) * self.sos_idx\n",
    "        # moving the y_t to the right device\n",
    "        y_t = y_t.to(encoder_h_t.device)\n",
    "        \n",
    "        outputs = []\n",
    "        self.attention_scores = []\n",
    "        \n",
    "        for i in range(trg_seq_len):\n",
    "            \n",
    "            teacher_forcing = np.random.random() < teacher_forcing_prob\n",
    "                \n",
    "            # Step 1: Concat the embedded token and the context_vectors\n",
    "            embedded = self.embedding_layer(y_t)\n",
    "\n",
    "            rnn_input = torch.cat((embedded, context_vectors), dim=1)\n",
    "            \n",
    "            # Step 2: Do a single RNN step and update the decoder hidden state\n",
    "            h_t = self.rnn(rnn_input, h_t)\n",
    "            \n",
    "            # Step 3: Calculate attention and update context vectors\n",
    "            context_vectors, attention_probs = self.attention(encoder_outputs, h_t, mask)\n",
    "            \n",
    "            # Step 4: Obtain the predicion vector\n",
    "            prediction_vector = torch.cat((h_t, context_vectors), dim=1)\n",
    "            \n",
    "            # Step 5: Obtain the prediction output\n",
    "            pred_output = self.classifier(prediction_vector)\n",
    "           \n",
    "            # if teacher_forcing, use ground truth target tokens\n",
    "            # as an input to the decoder in the next time_step\n",
    "            if teacher_forcing:\n",
    "                y_t = trg_seqs[i]\n",
    "                \n",
    "            # If not teacher_forcing force, use the maximum \n",
    "            # prediction as an input to the decoder in \n",
    "            # the next time step\n",
    "            if not teacher_forcing:\n",
    "                # we multiply the predictions with a sampling_temperature\n",
    "                # to make the propablities peakier, so we can be confident about the\n",
    "                # maximum prediction\n",
    "                pred_output_probs = F.softmax(pred_output * self.sampling_temperature, dim=1)\n",
    "                y_t = torch.argmax(pred_output_probs, dim=1)\n",
    "\n",
    "                # if we predicted the <eos> token stop decoding\n",
    "                if inference and (y_t == self.eos_idx).sum() == y_t.shape[0]:\n",
    "                    print('inference')\n",
    "                    break\n",
    "\n",
    "            outputs.append(pred_output)\n",
    "            self.attention_scores.append(attention_probs)\n",
    "            \n",
    "        outputs = torch.stack(outputs).permute(1, 0, 2)\n",
    "        # outputs shape: [batch_size, trg_seq_len, output_dim]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"Attention mechanism as a MLP \n",
    "    as used by Bahdanau et. al 2015\"\"\"\n",
    "\n",
    "    def __init__(self, encoder_hidd_dim, decoder_hidd_dim):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.atten = nn.Linear((encoder_hidd_dim * 2) + decoder_hidd_dim, decoder_hidd_dim)\n",
    "        self.v = nn.Linear(decoder_hidd_dim, 1)\n",
    "        \n",
    "    def forward(self, key_vectors, query_vector, mask):\n",
    "        \"\"\"key_vectors: encoder hidden states.\n",
    "           query_vector: decoder hidden state at time t\n",
    "           mask: the mask vector of zeros and ones\n",
    "        \"\"\"\n",
    "        \n",
    "        #key_vectors shape: [batch_size, src_seq_length, encoder_hidd_dim * 2]\n",
    "        #query_vector shape: [batch_size, decoder_hidd_dim]\n",
    "        #Note: encoder_hidd_dim * 2 == decoder_hidd_dim\n",
    "        \n",
    "        batch_size, src_seq_length, encoder_hidd_dim = key_vectors.shape\n",
    "        \n",
    "        #changing the shape of query_vector to [batch_size, src_seq_length, decoder_hidd_dim]\n",
    "        #we will repeat the query_vector src_seq_length times at dim 1\n",
    "        query_vector = query_vector.unsqueeze(1).repeat(1, src_seq_length, 1)\n",
    "        \n",
    "        # Step 1: Compute the attention scores through a MLP\n",
    "        \n",
    "        # concatenating the key_vectors and the query_vector\n",
    "        atten_input = torch.cat((key_vectors, query_vector), dim=2)\n",
    "        # atten_input shape: [batch_size, src_seq_length, (encoder_hidd_dim * 2) + decoder_hidd_dim]\n",
    "        \n",
    "        atten_scores = self.atten(atten_input)\n",
    "        # atten_scores shape: [batch_size, src_seq_length, decoder_hidd_dim]\n",
    "\n",
    "        atten_scores = torch.tanh(atten_scores)\n",
    "    \n",
    "        # mapping atten_scores from decoder_hidd_dim to 1\n",
    "        atten_scores = self.v(atten_scores)\n",
    "    \n",
    "        # atten_scores shape: [batch_size, src_seq_length, 1]\n",
    "        atten_scores = atten_scores.squeeze(dim=2)\n",
    "        # atten_scores shape: [batch_size, src_seq_length]\n",
    "        \n",
    "        # masking the atten_scores\n",
    "        atten_scores = atten_scores.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # Step 2: normalizing atten_scores through a softmax to get probs\n",
    "        atten_scores = F.softmax(atten_scores, dim=1)\n",
    "        \n",
    "        # Step 3: computing the new context vector\n",
    "        context_vectors = torch.matmul(key_vectors.permute(0, 2, 1), atten_scores.unsqueeze(2)).squeeze(dim=2)\n",
    "        \n",
    "        # context_vectors shape: [batch_size, encoder_hidd_dim * 2]\n",
    "        \n",
    "        return context_vectors, atten_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder_input_dim, encoder_embed_dim,\n",
    "                 encoder_hidd_dim, decoder_input_dim, decoder_embed_dim,\n",
    "                 decoder_output_dim, src_padding_idx=0, trg_padding_idx=0):\n",
    "    \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.src_padding_idx = src_padding_idx\n",
    "        self.encoder = Encoder(input_dim=encoder_input_dim,\n",
    "                               embed_dim=encoder_embed_dim,\n",
    "                               hidd_dim=encoder_hidd_dim,\n",
    "                               padding_idx=src_padding_idx)\n",
    "\n",
    "        decoder_hidd_dim = encoder_hidd_dim * 2 \n",
    "        \n",
    "        self.attention = AdditiveAttention(encoder_hidd_dim=encoder_hidd_dim,\n",
    "                                           decoder_hidd_dim=decoder_hidd_dim)\n",
    "        \n",
    "        self.decoder = Decoder(input_dim=decoder_input_dim,\n",
    "                               embed_dim=decoder_embed_dim,\n",
    "                               hidd_dim=decoder_hidd_dim,\n",
    "                               output_dim=decoder_output_dim,\n",
    "                               attention=self.attention,\n",
    "                               padding_idx=trg_padding_idx)\n",
    "    \n",
    "    def create_mask(self, src_seqs, src_padding_idx):\n",
    "        mask = (src_seqs != src_padding_idx)\n",
    "        # mask shape: [batch_size, src_seq_length]\n",
    "        return mask \n",
    "    \n",
    "    def forward(self, src_seqs, src_seqs_lengths, trg_seqs, teacher_forcing_prob=0.3):\n",
    "        encoder_output, encoder_h_t = self.encoder(src_seqs, src_seqs_lengths)\n",
    "        mask = self.create_mask(src_seqs, self.src_padding_idx)\n",
    "        decoder_output = self.decoder(trg_seqs, encoder_output, encoder_h_t, mask, \n",
    "                                      teacher_forcing_prob=teacher_forcing_prob)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, cuda):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(data_dir='/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus',\n",
    "                          vectorizer_path='/home/ba63/gender-bias/models/saved_models/char_level_vectorizer.json',\n",
    "                          reload_files=False,\n",
    "                          cache_files=False,\n",
    "                          num_epochs=50,\n",
    "                          embedding_dim=100,\n",
    "                          hidd_dim=128,\n",
    "                          learning_rate=5e-4,\n",
    "                          use_cuda=True,\n",
    "                          batch_size=64,\n",
    "                          seed=21,\n",
    "                          model_path='/home/ba63/gender-bias/models/saved_models/char_level_model_small_old.pt'\n",
    "                          )\n",
    "\n",
    "device = torch.device('cuda' if args.use_cuda else 'cpu')\n",
    "set_seed(args.seed, args.use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.reload_files:\n",
    "    dataset = MT_Dataset.load_data_and_load_vectorizer(args.data_dir, args.vectorizer_path)\n",
    "else:\n",
    "    dataset = MT_Dataset.load_data_and_create_vectorizer(args.data_dir)\n",
    "\n",
    "if args.cache_files:\n",
    "    dataset.save_vectorizer(args.vectorizer_path)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "ENCODER_INPUT_DIM = len(vectorizer.src_vocab)\n",
    "DECODER_INPUT_DIM = len(vectorizer.trg_vocab)\n",
    "DECODER_OUTPUT_DIM = len(vectorizer.trg_vocab)\n",
    "SRC_PAD_INDEX = vectorizer.src_vocab.pad_idx\n",
    "TRG_PAD_INDEX = vectorizer.trg_vocab.pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding_layer): Embedding(71, 100, padding_idx=0)\n",
       "    (rnn): GRU(100, 128, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (attention): AdditiveAttention(\n",
       "    (atten): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (v): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): AdditiveAttention(\n",
       "      (atten): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (v): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (embedding_layer): Embedding(71, 100, padding_idx=0)\n",
       "    (rnn): GRUCell(356, 256)\n",
       "    (classifier): Linear(in_features=512, out_features=71, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2Seq(encoder_input_dim=ENCODER_INPUT_DIM,\n",
    "                encoder_embed_dim=args.embedding_dim,\n",
    "                encoder_hidd_dim=args.hidd_dim,\n",
    "                decoder_input_dim=DECODER_INPUT_DIM,\n",
    "                decoder_embed_dim=args.embedding_dim,\n",
    "                decoder_output_dim=DECODER_OUTPUT_DIM,\n",
    "                src_padding_idx=SRC_PAD_INDEX,\n",
    "                trg_padding_idx=TRG_PAD_INDEX)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_INDEX)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                 patience=2, factor=0.5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device='cpu', teacher_forcing_prob=1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        src = batch['src']\n",
    "        trg_x = batch['trg_x']\n",
    "        trg_y = batch['trg_y']\n",
    "        src_lengths = batch['src_lengths']\n",
    "        \n",
    "        preds = model(src_seqs=src, \n",
    "                      src_seqs_lengths=src_lengths, \n",
    "                      trg_seqs=trg_x,\n",
    "                      teacher_forcing_prob=teacher_forcing_prob)\n",
    "        \n",
    "        # CrossEntropysLoss accepts matrices always! \n",
    "        # the preds must be of size (N, C) where C is the number \n",
    "        # of classes and N is the number of samples. \n",
    "        # The ground truth must be a Vector of size C!\n",
    "        preds = preds.contiguous().view(-1, preds.shape[-1])\n",
    "        trg_y = trg_y.view(-1)\n",
    "\n",
    "        loss = criterion(preds, trg_y)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device='cpu', teacher_forcing_prob=0):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            src = batch['src']\n",
    "            trg_x = batch['trg_x']\n",
    "            trg_y = batch['trg_y']\n",
    "            src_lengths = batch['src_lengths']\n",
    "\n",
    "            preds = model(src_seqs=src, \n",
    "                          src_seqs_lengths=src_lengths, \n",
    "                          trg_seqs=trg_x,\n",
    "                          teacher_forcing_prob=teacher_forcing_prob)\n",
    "            # CrossEntropyLoss accepts matrices always! \n",
    "            # the preds must be of size (N, C) where C is the number \n",
    "            # of classes and N is the number of samples. \n",
    "            # The ground truth must be a Vector of size C!\n",
    "            preds = preds.contiguous().view(-1, preds.shape[-1])\n",
    "            trg_y = trg_y.view(-1)\n",
    "            \n",
    "            loss = criterion(preds, trg_y)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Epoch: 1\n",
      "\tTrain Loss: 3.0398   |   Dev Loss: 2.6412\n",
      "Epoch: 2\n",
      "\tTrain Loss: 1.9745   |   Dev Loss: 1.6904\n",
      "Epoch: 3\n",
      "\tTrain Loss: 1.3267   |   Dev Loss: 1.2112\n",
      "Epoch: 4\n",
      "\tTrain Loss: 1.1447   |   Dev Loss: 1.1721\n",
      "Epoch: 5\n",
      "\tTrain Loss: 0.9730   |   Dev Loss: 1.0494\n",
      "Epoch: 6\n",
      "\tTrain Loss: 0.8180   |   Dev Loss: 2.3534\n",
      "Epoch: 7\n",
      "\tTrain Loss: 0.7962   |   Dev Loss: 0.6978\n",
      "Epoch: 8\n",
      "\tTrain Loss: 0.7453   |   Dev Loss: 0.8435\n",
      "Epoch: 9\n",
      "\tTrain Loss: 0.6522   |   Dev Loss: 0.6783\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.7087   |   Dev Loss: 0.7579\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.5550   |   Dev Loss: 0.6800\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.8544   |   Dev Loss: 0.7228\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.5584   |   Dev Loss: 0.5809\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.6446   |   Dev Loss: 1.3219\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.5957   |   Dev Loss: 0.7894\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.5527   |   Dev Loss: 0.6271\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.3561   |   Dev Loss: 0.8445\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.4121   |   Dev Loss: 1.7458\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.4015   |   Dev Loss: 2.0428\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.3865   |   Dev Loss: 0.5348\n",
      "Epoch: 21\n",
      "\tTrain Loss: 0.3024   |   Dev Loss: 1.0450\n",
      "Epoch: 22\n",
      "\tTrain Loss: 0.2986   |   Dev Loss: 0.6252\n",
      "Epoch: 23\n",
      "\tTrain Loss: 0.2570   |   Dev Loss: 0.4025\n",
      "Epoch: 24\n",
      "\tTrain Loss: 0.2887   |   Dev Loss: 0.5299\n",
      "Epoch: 25\n",
      "\tTrain Loss: 0.3039   |   Dev Loss: 0.4227\n",
      "Epoch: 26\n",
      "\tTrain Loss: 0.2396   |   Dev Loss: 0.4524\n",
      "Epoch: 27\n",
      "\tTrain Loss: 0.2749   |   Dev Loss: 0.4631\n",
      "Epoch: 28\n",
      "\tTrain Loss: 0.2340   |   Dev Loss: 0.4487\n",
      "Epoch: 29\n",
      "\tTrain Loss: 0.2279   |   Dev Loss: 0.4119\n",
      "Epoch: 30\n",
      "\tTrain Loss: 0.2042   |   Dev Loss: 0.4269\n",
      "Epoch: 31\n",
      "\tTrain Loss: 0.1984   |   Dev Loss: 0.4216\n",
      "Epoch: 32\n",
      "\tTrain Loss: 0.1949   |   Dev Loss: 0.4715\n",
      "Epoch: 33\n",
      "\tTrain Loss: 0.1938   |   Dev Loss: 0.4135\n",
      "Epoch: 34\n",
      "\tTrain Loss: 0.2035   |   Dev Loss: 0.4354\n",
      "Epoch: 35\n",
      "\tTrain Loss: 0.1904   |   Dev Loss: 0.4015\n",
      "Epoch: 36\n",
      "\tTrain Loss: 0.1873   |   Dev Loss: 0.3978\n",
      "Epoch: 37\n",
      "\tTrain Loss: 0.1978   |   Dev Loss: 0.4676\n",
      "Epoch: 38\n",
      "\tTrain Loss: 0.1946   |   Dev Loss: 0.3902\n",
      "Epoch: 39\n",
      "\tTrain Loss: 0.1912   |   Dev Loss: 0.3558\n",
      "Epoch: 40\n",
      "\tTrain Loss: 0.1862   |   Dev Loss: 0.4137\n",
      "Epoch: 41\n",
      "\tTrain Loss: 0.1824   |   Dev Loss: 0.4369\n",
      "Epoch: 42\n",
      "\tTrain Loss: 0.1960   |   Dev Loss: 0.4778\n",
      "Epoch: 43\n",
      "\tTrain Loss: 0.1863   |   Dev Loss: 0.3859\n",
      "Epoch: 44\n",
      "\tTrain Loss: 0.1824   |   Dev Loss: 0.4454\n",
      "Epoch: 45\n",
      "\tTrain Loss: 0.2014   |   Dev Loss: 0.4073\n",
      "Epoch: 46\n",
      "\tTrain Loss: 0.1929   |   Dev Loss: 0.4387\n",
      "Epoch: 47\n",
      "\tTrain Loss: 0.1839   |   Dev Loss: 0.4391\n",
      "Epoch: 48\n",
      "\tTrain Loss: 0.1853   |   Dev Loss: 0.4281\n",
      "Epoch: 49\n",
      "\tTrain Loss: 0.1789   |   Dev Loss: 0.4315\n",
      "Epoch: 50\n",
      "\tTrain Loss: 0.1801   |   Dev Loss: 0.4230\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "collator = Collator(SRC_PAD_INDEX, TRG_PAD_INDEX)\n",
    "best_loss = 1e10\n",
    "set_seed(args.seed, args.use_cuda)\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "print(f'Using {device}')\n",
    "for epoch in range(args.num_epochs):\n",
    "#     teacher_forcing_prob = (epoch + 5) / args.num_epochs\n",
    "    teacher_forcing_prob = 0.3\n",
    "    dataset.set_split('train')\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=args.batch_size, collate_fn=collator)\n",
    "\n",
    "    train_loss = train(model, dataloader, optimizer, criterion, device, teacher_forcing_prob=teacher_forcing_prob)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    dataset.set_split('dev')\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=args.batch_size, collate_fn=collator)\n",
    "    dev_loss = evaluate(model, dataloader, criterion, device, teacher_forcing_prob=0)\n",
    "    dev_losses.append(dev_loss)\n",
    "    \n",
    "    #save best model\n",
    "    if dev_loss < best_loss:\n",
    "        best_loss = dev_loss\n",
    "        torch.save(model.state_dict(), args.model_path)\n",
    "    \n",
    "    scheduler.step(dev_loss)\n",
    "    print(f'Epoch: {(epoch + 1)}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f}   |   Dev Loss: {dev_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, 1 + args.num_epochs), np.asarray(train_losses), 'b-', color='blue', label='Training')\n",
    "plt.plot(range(1, 1 + args.num_epochs), np.asarray(dev_losses), 'b-', color='orange', label='Evaluation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMT_Batch_Sampler:\n",
    "    def __init__(self, model, src_vocab, trg_vocab):\n",
    "        self.model = model\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "    \n",
    "    def update_batch(self, batch):\n",
    "        self.sample_batch = batch\n",
    "        \n",
    "        src = batch['src']\n",
    "\n",
    "        trg_x = batch['trg_x']\n",
    "        trg_y = batch['trg_y']\n",
    "        src_lengths = batch['src_lengths']\n",
    "\n",
    "        preds = self.model(src, src_lengths, trg_x, teacher_forcing_prob=0)\n",
    "        # preds shape: [batch_size, trg_seq_len, output_dim]\n",
    "        \n",
    "        attention_scores = [score.cpu().detach() for score in self.model.decoder.attention_scores]\n",
    "        # len(attention_scores): trg_seq_len\n",
    "        # each vector in attention_scores has a shape: [batch_size, src_seq_len]\n",
    "        attention_scores = torch.stack(attention_scores)\n",
    "        attention_scores = attention_scores.permute(1, 0, 2)\n",
    "        # attention_score shape: [batch_size, trg_seq_len, src_seq_len]\n",
    "        \n",
    "        self.sample_batch['preds'] = preds\n",
    "        self.sample_batch['attention_scores'] = attention_scores\n",
    "        return self.sample_batch\n",
    "    \n",
    "    def get_pred_sentence(self, index):\n",
    "        preds = self.sample_batch['preds']\n",
    "       \n",
    "        max_preds = torch.argmax(preds, dim=2)\n",
    "        # max_preds shape: [batch_size, trg_seq_len]\n",
    "        max_pred_sentence = max_preds[index].cpu().detach().numpy()\n",
    "        return self.get_str_sentence(max_pred_sentence, self.trg_vocab)\n",
    "    \n",
    "    def get_trg_sentence(self, index):\n",
    "        trg_sentence = self.sample_batch['trg_y'][index].cpu().detach().numpy()\n",
    "        return self.get_str_sentence(trg_sentence, self.trg_vocab)\n",
    "    \n",
    "    def get_src_sentence(self, index):\n",
    "        src_sentence = self.sample_batch['src'][index].cpu().detach().numpy()\n",
    "        return self.get_str_sentence(src_sentence, self.src_vocab)\n",
    "    \n",
    "    def get_str_sentence(self, vectorized_sentence, vocab):\n",
    "        sentence = []\n",
    "        for i in vectorized_sentence:\n",
    "            if i == vocab.sos_idx:\n",
    "                continue\n",
    "            elif i == vocab.eos_idx:\n",
    "                break\n",
    "            else:\n",
    "                sentence.append(vocab.lookup_index(i))\n",
    "        return ''.join(sentence)\n",
    "\n",
    "    def translate_sentence(self, sentence):\n",
    "        \"\"\"Args:\n",
    "            - sentence (string): sentence to be translated\n",
    "           Returns:\n",
    "            - translated sentence (string)\n",
    "        \"\"\"\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # vectorizing the sentence\n",
    "        vectorized_src_sentence = [self.src_vocab.sos_idx]\n",
    "        vectorized_src_sentence.extend([self.src_vocab.lookup_token(t) for t in sentence])\n",
    "        vectorized_src_sentence.append(self.src_vocab.eos_idx)\n",
    "\n",
    "        # getting sentence length\n",
    "        src_sentence_length = len(vectorized_src_sentence)\n",
    "\n",
    "        # converting the vectorized sentence and the length to tensors\n",
    "        vectorized_src_sentence = torch.tensor([vectorized_src_sentence], dtype=torch.long)\n",
    "        src_sentence_length = torch.tensor([src_sentence_length], dtype=torch.long)\n",
    "\n",
    "        vectorized_src_sentence = vectorized_src_sentence.to(device)\n",
    "        src_sentence_length = src_sentence_length.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            decoder_output = self.model(src_seqs=vectorized_src_sentence, \n",
    "                                        src_seqs_lengths=src_sentence_length, \n",
    "                                        trg_seqs=None, teacher_forcing_prob=0)\n",
    "\n",
    "        \n",
    "        max_preds = torch.argmax(decoder_output, dim=2).squeeze().cpu().detach().numpy()\n",
    "        str_sentence = self.get_str_sentence(max_preds, self.trg_vocab)\n",
    "        \n",
    "        return str_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed, args.use_cuda)\n",
    "dataset.set_split('train')\n",
    "model.load_state_dict(torch.load(args.model_path))\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "collator = Collator(SRC_PAD_INDEX, TRG_PAD_INDEX)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collator)\n",
    "sampler = NMT_Batch_Sampler(model, vectorizer.src_vocab, vectorizer.trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_preds = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/'\\\n",
    "    'edits_annotations/char_level_model_small_old.train_preds.inf', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/'\\\n",
    "    'edits_annotations/char_level_model_small_old.train_long.inf', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    updated_batch = sampler.update_batch(batch)\n",
    "\n",
    "#     print(updated_batch['trg_x'])\n",
    "#     print(updated_batch['trg_y'])\n",
    "#     print(updated_batch['src'])\n",
    "    src = sampler.get_src_sentence(0)\n",
    "    trg = sampler.get_trg_sentence(0)\n",
    "    pred = sampler.get_pred_sentence(0)\n",
    "    translated = sampler.translate_sentence(src)\n",
    "\n",
    "#     print(src)\n",
    "#     print(trg)\n",
    "#     print(pred)\n",
    "#     print(translated)\n",
    "#     print(len(translated))\n",
    "    \n",
    "    train_log.write(f'src: ' + src)\n",
    "    train_log.write(f'trg: ' + src)\n",
    "    train_log.write(f'pred: ' + src)\n",
    "    train_log.write(f'src: ' + src)\n",
    "    train_log.write('\\n')\n",
    "    new_train_preds.write(pred)\n",
    "    new_train_preds.write('\\n')\n",
    "#     attention_scores = updated_batch['attention_scores'][0].cpu().detach().numpy()\n",
    "#     fig = plt.figure(figsize=(10,10))\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     cax = ax.matshow(attention_scores, cmap='bone')\n",
    "#     fig.colorbar(cax)\n",
    "\n",
    "#     # Set up axes\n",
    "#     ax.set_xticklabels(['','<s>'] + src.split(' ') +\n",
    "#                    ['</s>'], rotation=90)\n",
    "#     ax.set_yticklabels([''] + pred.split(' ') + ['</s>'])\n",
    "\n",
    "#     # Show label at every tick\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#     ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "new_train_preds.close()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed, args.use_cuda)\n",
    "dataset.set_split('dev')\n",
    "model.load_state_dict(torch.load(args.model_path))\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "collator = Collator(SRC_PAD_INDEX, TRG_PAD_INDEX)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collator)\n",
    "sampler = NMT_Batch_Sampler(model, vectorizer.src_vocab, vectorizer.trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dev_preds = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/edits_annotations/char_level_model_small_old.dev_preds', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_log = open(\n",
    "    '/home/ba63/gender-bias/data/christine_2019/Arabic-parallel-gender-corpus/edits_annotations/char_level_model_small_old.dev_log', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    updated_batch = sampler.update_batch(batch)\n",
    "\n",
    "#     print(updated_batch['trg_x'])\n",
    "#     print(updated_batch['trg_y'])\n",
    "#     print(updated_batch['src'])\n",
    "    src = sampler.get_src_sentence(0)\n",
    "    trg = sampler.get_trg_sentence(0)\n",
    "    pred = sampler.get_pred_sentence(0)\n",
    "    \n",
    "    new_dev_preds.write(pred)\n",
    "#     print(src)\n",
    "#     print(trg)\n",
    "#     print(pred)\n",
    "    new_dev_preds.write('\\n')\n",
    "    \n",
    "    dev_log.write(f'src: ' + src)\n",
    "    dev_log.write('\\n')\n",
    "    dev_log.write(f'trg: ' + trg)\n",
    "    dev_log.write('\\n')\n",
    "    dev_log.write(f'pred: ' + pred)\n",
    "    dev_log.write('\\n\\n')\n",
    "#     attention_scores = updated_batch['attention_scores'][0].cpu().detach().numpy()\n",
    "#     fig = plt.figure(figsize=(10,10))\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     cax = ax.matshow(attention_scores, cmap='bone')\n",
    "#     fig.colorbar(cax)\n",
    "\n",
    "#     # Set up axes\n",
    "#     ax.set_xticklabels(['','<s>'] + src.split(' ') +\n",
    "#                    ['</s>'], rotation=90)\n",
    "#     ax.set_yticklabels([''] + pred.split(' ') + ['</s>'])\n",
    "\n",
    "#     # Show label at every tick\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#     ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     plt.show()\n",
    "dev_log.close()\n",
    "new_dev_preds.close()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
